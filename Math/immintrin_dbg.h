/*
Copyright (c) 2019, <alexander.komarov@intel.com>
All rights reserved.

Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are met:

* Redistributions of source code must retain the above copyright notice, this
  list of conditions and the following disclaimer.

* Redistributions in binary form must reproduce the above copyright notice,
  this list of conditions and the following disclaimer in the documentation
  and/or other materials provided with the distribution.

* Neither the name of the copyright holder nor the names of its
  contributors may be used to endorse or promote products derived from
  this software without specific prior written permission.

THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

   Intrinsics guide version: 3.5.4
   Parser version: 0.25
*/

#ifndef __IMMINTRIN_DBG_H_
#define __IMMINTRIN_DBG_H_
#ifdef __cplusplus
extern "C" {
#endif

#include <inttypes.h>
#include <limits.h>
#include <float.h>
#include "/usr/include/math.h"
#include <stdlib.h>

/* Include intrinsics library */
#if defined __GNUC__
#include <x86intrin.h>
#else
#include <immintrin.h>
#endif /* GCC */

#define SELECT2 _mm256_extractf32x4_ps_dbg
#define SELECT4 _mm_extract_epi32_dbg
static inline int POPCNT(uint64_t control)
{
return __builtin_popcountll(control);
}
static inline float Convert_FP64_To_FP32(double control)
{
return (float)control;
}
static inline double Convert_Int32_To_FP64(int32_t control)
{
return (double)control;

}
static inline float Convert_Int32_To_FP32(int32_t control)
{
return (float)control;
}
static inline float Convert_Int32_To_FP32_rounding(int32_t control, int CURRENT_ROUNDING)
{
return (float)control;
}
static inline int32_t Convert_FP64_To_Int32(double control)
{
//FIXME AK: default convert is rounding
return (int32_t)round(control);
}
static inline int32_t Convert_FP64_To_Int32_rounding(double control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (int32_t)round(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return (int32_t)floor(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (int32_t)ceil(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (int32_t)trunc(control);
 return (int32_t)control;
}
static inline int32_t Convert_FP32_To_Int32(float control)
{
//FIXME AK: default convert is rounding
return (int32_t)roundf(control);
}
static inline double Convert_FP32_To_FP64(float control)
{
return (double)control;
}
static inline uint32_t Convert_FP32_To_UnsignedInt32(float control)
{
return (uint32_t)control;
//FIXME AK
}
static inline int32_t Convert_FP64_To_Int32_Truncate(double control)
{
return (int32_t)control;
}
static inline double APPROXIMATE(double control)
{
return control;
}
static inline uint16_t Saturate_To_UnsignedInt16(int32_t control)
{
if (control > UINT16_MAX) return UINT16_MAX;
return control;
}
static inline uint8_t Saturate_To_UnsignedInt8(int16_t control)
{
if (control > UINT8_MAX) return UINT8_MAX;
return control;
}
static inline int16_t Saturate_To_Int16(int32_t control)
{
if (control > INT16_MAX) return INT16_MAX;
if (control < INT16_MIN) return INT16_MIN;
return control;
}
static inline int8_t Saturate_To_Int8(int16_t control)
{
if (control > INT8_MAX) return INT8_MAX;
if (control < INT8_MIN) return INT8_MIN;
return control;
}
static inline double ReduceArgumentPD(double src1, uint8_t imm8)
{
double tmp;
uint8_t m = (imm8 << 4 & 0xf0) >> 4;uint8_t rc = (imm8 << 1 & 0x3) >> 1;
uint8_t rc_src = (imm8 << 2 & 0x4) >> 2;
tmp = pow(2, -m) * round(pow(2, m));// * src1, 0, rc_src, rc);
tmp = src1 - tmp;
return tmp;
}

static inline double ReduceArgumentPS(float src1, uint8_t imm8)
{
double tmp;
uint8_t m = (imm8 << 4 & 0xf0) >> 4;uint8_t rc = (imm8 << 1 & 0x3) >> 1;
uint8_t rc_src = (imm8 << 2 & 0x4) >> 2;
tmp = pow(2, -m) * round(pow(2, m));// * src1, 0, rc_src, rc);
tmp = src1 - tmp;
return tmp;
}

static inline int64_t SignExtend(int64_t control)
{
return control;
}
static inline uint64_t ZeroExtend(uint64_t control)
{
return control;
}
static inline int64_t RIGHT_ROTATE_QWORDS(int64_t v, int8_t n)
{
n = n & 63U;
 if (n)
 v = (v >> n) | (v << (64-n));
 return v;
}
static inline int32_t RIGHT_ROTATE_DWORDS(int32_t v, int8_t n)
{
n = n & 31U;
 if (n)
 v = (v >> n) | (v << (32-n));
 return v;
}
static inline double ConvertExpFP64(double control)
{
return floor(log2(control));
}
static inline uint32_t Convert_FP64_To_UnsignedInt32(double control)
{
return (uint32_t)control;
}
static inline int64_t LEFT_ROTATE_QWORDS(int64_t v, int8_t n)
{
n = n & 63U;
 if (n)
 v = (v << n) | (v >> (64-n));
 return v;
}
static inline int32_t LEFT_ROTATE_DWORDS(int32_t v, int8_t n)
{
n = n & 31U;
 if (n)
 v = (v << n) | (v >> (32-n));
 return v;
}
static inline int8_t Truncate_Int16_To_Int8(int16_t control)
{
return control;
}
static inline int8_t Saturate_UnsignedInt16_To_Int8(uint16_t control)
{
if (control > UINT8_MAX) return UINT8_MAX;
return control;
}
static inline int16_t Saturate_UnsignedInt64_To_Int16(uint64_t control)
{
if (control > UINT16_MAX) return UINT16_MAX;
return control;
}
static inline int8_t Saturate_Int16_To_Int8(int16_t control)
{
if (control > INT8_MAX) return INT8_MAX;
if (control < INT8_MIN) return INT8_MIN;
return control;
}
static inline int8_t Saturate_UnsignedInt32_To_Int8(uint32_t control)
{
if (control > UINT8_MAX) return UINT8_MAX;
return control;
}
static inline int8_t Saturate_UnsignedInt64_To_Int8(uint64_t control)
{
if (control > UINT8_MAX) return UINT8_MAX;
return control;
}
static inline int32_t Convert_FP64_To_IntegerTruncate(double control)
{
return (int32_t)control;
}
static inline uint32_t Convert_FP64_To_UnsignedIntegerTruncate(double control)
{
return (uint32_t)control;
}
static inline int32_t Convert_FP32_To_IntegerTruncate(float control)
{
return (int32_t)control;
}
static inline uint32_t Convert_FP64_To_UnsignedInt32_Truncate(double control)
{
return (uint32_t)control;
}
static inline int16_t Saturate_UnsignedInt32_To_Int16(uint32_t control)
{
if (control > UINT16_MAX) return UINT16_MAX;
return control;
}
static inline int32_t Saturate_UnsignedInt64_To_Int32(uint64_t control)
{
if (control > UINT32_MAX) return UINT32_MAX;
return control;
}
static inline double ConvertUnsignedIntegerTo_FP64(uint32_t control)
{
return (double)control;
}
static inline uint64_t Convert_FP32_To_UnsignedInt64_Truncate(float control)
{
return (uint64_t)control;
}
static inline int64_t Convert_FP32_To_Int64_Truncate(float control)
{
return (uint64_t)control;
}
static inline double ConvertUnsignedInt64_To_FP64(uint64_t control)
{
return (double)control;
}
static inline float ConvertUnsignedInt64_To_FP32(uint64_t control)
{
return (float)control;
}
static inline float ConvertExpFP32(float control)
{
return floorf(log2(control));
}
static inline int32_t Convert_FP32_To_Int32_Truncate(float control)
{
return (int32_t)control;
}
static inline uint32_t Convert_FP32_To_UnsignedIntegerTruncate(float control)
{
return (uint32_t)control;
}
static inline float ConvertUnsignedInt32_To_FP32(uint32_t control)
{
return (float)control;
}
static inline uint32_t Convert_FP32_To_UnsignedInt32_Truncate(float control)
{
return (uint32_t)control;
}
static inline int8_t Truncate_Int32_To_Int8(int32_t control)
{
return control;
}
static inline int16_t Truncate_Int32_To_Int16(int32_t control)
{
return control;
}
static inline int8_t Truncate_Int64_To_Int8(int64_t control)
{
return control;
}
static inline int32_t Truncate_Int64_To_Int32(int64_t control)
{
return control;
}
static inline int16_t Truncate_Int64_To_Int16(int64_t control)
{
return control;
}
static inline int8_t Saturate_Int32_To_Int8(int32_t control)
{
if (control > INT8_MAX) return INT8_MAX;
if (control < INT8_MIN) return INT8_MIN;
return control;
}
static inline int16_t Saturate_Int32_To_Int16(int32_t control)
{
if (control > INT16_MAX) return INT16_MAX;
if (control < INT16_MIN) return INT16_MIN;
return control;
}
static inline int8_t Saturate_Int64_To_Int8(int64_t control)
{
if (control > INT8_MAX) return INT8_MAX;
if (control < INT8_MIN) return INT8_MIN;
return control;
}
static inline int32_t Saturate_Int64_To_Int32(int64_t control)
{
if (control > INT32_MAX) return INT32_MAX;
if (control < INT32_MIN) return INT32_MIN;
return control;
}
static inline int16_t Saturate_Int64_To_Int16(int64_t control)
{
if (control > INT16_MAX) return INT16_MAX;
if (control < INT16_MIN) return INT16_MIN;
return control;
}
static inline uint64_t Convert_FP64_To_UnsignedInt64(double control)
{
//FIXME AK: default convert is rounding
return (uint64_t)round(control);
}
static inline uint64_t Convert_FP32_To_UnsignedInt64(float control)
{
//FIXME AK: default convert is rounding
return (uint64_t)roundf(control);
}
static inline double Convert_Int64_To_FP64(int64_t control)
{
return (double)control;
}
static inline float Convert_Int64_To_FP32(int64_t control)
{
return (float)control;
}
static inline int64_t Convert_FP64_To_Int64_Truncate(double control)
{
return (int64_t)control;
}
static inline uint64_t Convert_FP64_To_UnsignedInt64_Truncate(double control)
{
return (uint64_t)control;
}
static inline int64_t Convert_FP64_To_Int64(double control)
{
//FIXME AK: default convert is rounding
return (int64_t)round(control);
}
static inline int64_t Convert_FP32_To_Int64(float control)
{
return (uint64_t)control;
}
#define MIN(X, Y) (((X) < (Y)) ? (X) : (Y))
#define MAX(X, Y) (((X) > (Y)) ? (X) : (Y))
#define NEG(X) (-(X))
static inline double CEIL(double control)
{
return ceil(control);
}
static inline double ROUND(double control, int CURRENT_ROUNDING)
{
return round(control);
}
static inline double FLOOR(double control)
{
return floor(control);
}
static inline float Convert_FP64_To_FP32_rounding(double control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return roundf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return floorf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (int32_t)ceilf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return truncf(control);
 return (float)control;

}
static inline uint32_t Convert_FP64_To_UnsignedInt32_rounding(double control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (uint32_t)round(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return floor(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (int32_t)ceil(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (uint32_t)trunc(control);
 return (uint32_t)control;

}
static inline int32_t Convert_FP32_To_Int32_rounding(float control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (int32_t)roundf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return (int32_t)floorf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (int32_t)ceilf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (int32_t)truncf(control);
 return (int32_t)control;
}
static inline uint32_t Convert_FP32_To_UnsignedInt32_rounding(float control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (uint32_t)roundf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return (uint32_t)floorf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (uint32_t)ceilf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (uint32_t)truncf(control);
 return (int32_t)control;
}
static inline int64_t Convert_FP64_To_Int64_rounding(double control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (int64_t)round(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return (int64_t)floor(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (int64_t)ceil(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (int64_t)trunc(control);
 return (int64_t)control;
}
static inline double Convert_Int64_To_FP64_rounding(int64_t control, int CURRENT_ROUNDING)
{
return (double)control;
}
static inline float Convert_Int64_To_FP32_rounding(int64_t control, int CURRENT_ROUNDING)
{
return (float)control;
}
static inline double Convert_FP32_To_FP64_rounding(float control, int CURRENT_ROUNDING)
{
return (double)control;
}
static inline int64_t Convert_FP32_To_Int64_rounding(float control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (int64_t)roundf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return (int64_t)floorf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (int64_t)ceilf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (int64_t)truncf(control);
 return (int64_t)control;
}
static inline double ConvertUnsignedInt64_To_FP64_rounding(uint64_t control, int CURRENT_ROUNDING)
{
return (double)control;
}
static inline uint64_t Convert_FP32_To_UnsignedInt64_rounding(float control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (uint64_t)roundf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return (uint64_t)floorf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (uint64_t)ceilf(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (uint64_t)truncf(control);
 return (uint64_t)control;
}
static inline float ConvertUnsignedInt32_To_FP32_rounding(uint32_t control, int CURRENT_ROUNDING)
{
return (float)control;
}
static inline int64_t Convert_FP32_To_Int64_Truncate_rounding(float control, int CURRENT_ROUNDING)
{
return (uint64_t)control;
}
static inline int32_t Convert_FP64_To_Int32_Truncate_rounding(double control, int CURRENT_ROUNDING)
{
return (int32_t)control;
}
static inline uint64_t Convert_FP64_To_UnsignedInt64_rounding(double control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return (uint64_t)round(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return (uint64_t)floor(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return (uint64_t)ceil(control);
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return (uint64_t)trunc(control);
 return (uint64_t)control;
}
static inline float ConvertUnsignedInt64_To_FP32_rounding(uint64_t control, int CURRENT_ROUNDING)
{
return (float)control;
}
static inline double ConvertExpFP64_rounding(double control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return round(log2(control));
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return floor(log2(control));
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return ceil(log2(control));
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return trunc(log2(control));
 return floor(log2(control));
}
static inline float ConvertExpFP32_rounding(float control, int CURRENT_ROUNDING)
{
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEAREST_INT)
 return roundf(log2f(control));
 if (CURRENT_ROUNDING & _MM_FROUND_TO_NEG_INF)
 return floorf(log2f(control));
 if (CURRENT_ROUNDING & _MM_FROUND_TO_POS_INF)
 return ceilf(log2f(control));
 if (CURRENT_ROUNDING & _MM_FROUND_TO_ZERO)
 return truncf(log2f(control));
 return floorf(log2f(control));
}
static inline int32_t Convert_FP32_To_Int32_Truncate_rounding(float control, int CURRENT_ROUNDING)
{
return (int32_t)control;
}
static inline int64_t Convert_FP64_To_Int64_Truncate_rounding(double control, int CURRENT_ROUNDING)
{
return (int64_t)control;
}

static inline double RANGE(double src1, double src2, int opCtl, int signSelCtl)
{
  double tmp;
  double dst;
  uint64_t *ptmp = (uint64_t*)&tmp, *psrc1 = (uint64_t*)&src1, *psrc2 = (uint64_t*)&src2, *pdst = (uint64_t*)&dst;
  switch (opCtl) {
    case 0:
      tmp = (src1 <= src2) ? src1 : src2; break;
    case 1:
      tmp = (src1 <= src2) ? src2 : src1; break;
    case 2:
      tmp = (abs(src1) <= abs(src2)) ? src1 : src2; break;
    case 3:
      tmp = (abs(src1) <= abs(src2)) ? src2 : src1; break;
  }

  switch (signSelCtl) {
    case 0: *pdst = (*psrc1 & 0x8000000000000000ULL) | (*ptmp & 0x7FFFFFFFFFFFFFFFULL); break;
    case 1: *pdst = *ptmp; break;
    case 2: *pdst = (*ptmp & 0x7FFFFFFFFFFFFFFFULL); break;
    case 3: *pdst = (1ULL << 63) | (*ptmp & 0x7FFFFFFFFFFFFFFFULL); break;
  }

  return dst;
}

static inline double RANGE32(float src1, float src2, int opCtl, int signSelCtl)
{
  float tmp;
  float dst;
  uint32_t *ptmp = (uint32_t*)&tmp, *psrc1 = (uint32_t*)&src1, *psrc2 = (uint32_t*)&src2, *pdst = (uint32_t*)&dst;
  switch (opCtl) {
    case 0: 
      tmp = (src1 <= src2) ? src1 : src2; break;
    case 1: 
      tmp = (src1 <= src2) ? src2 : src1; break;
	  case 2: 
      tmp = (abs(src1) <= abs(src2)) ? src1 : src2; break;
	  case 3: 
      tmp = (abs(src1) <= abs(src2)) ? src2 : src1; break;
  }
	
  switch (signSelCtl) {
    case 0: *pdst = (*psrc1 & 0x80000000UL) | (*ptmp & 0x7FFFFFFFUL); break;
    case 1: *pdst = *ptmp; break;
    case 2: *pdst = (*ptmp & 0x7FFFFFFFUL); break;
    case 3: *pdst = (1 << 31) | (*ptmp & 0x7FFFFFFFUL); break;
  }
	
	return dst;
}

/*
 Store 128-bits (composed of 8 packed 16-bit integers) from "a" into memory.
                "mem_addr" does not need to be aligned on any particular boundary.
*/
static inline void _mm_storeu_epi16_dbg(void *mem_addr, __m128i A)
{
  _mm_storeu_si128((__m128i*)mem_addr, A);
}
#undef _mm_storeu_epi16
#define _mm_storeu_epi16 _mm_storeu_epi16_dbg 

/*
 Load 512-bits (composed of 32 packed 16-bit integers) from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m512i _mm512_loadu_epi16_dbg(void const* mem_addr)
{
  union simd
  {
  __m512i dst512;
  __m128i dst128[4];
  } dst;
  for (int i = 0; i < 4; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst512;
}
#undef _mm512_loadu_epi16
#define _mm512_loadu_epi16 _mm512_loadu_epi16_dbg

/*
 Load 512-bits (composed of 8 packed 64-bit doubles) from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m512d _mm512_loadu_pd_dbg(void const* mem_addr)
{
  union simd
  {
  __m512d dst512;
  __m128i dst128[4];
  } dst;
  for (int i = 0; i < 4; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst512;
}
#undef _mm512_loadu_pd
#define _mm512_loadu_pd _mm512_loadu_pd_dbg

/*
 Load 512-bits (composed of 8 packed 64-bit doubles) from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m512 _mm512_loadu_ps_dbg(void const* mem_addr)
{
  union simd
  {
  __m512 dst512;
  __m128i dst128[4];
  } dst;
  for (int i = 0; i < 4; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst512;
}
#undef _mm512_loadu_ps
#define _mm512_loadu_ps _mm512_loadu_ps_dbg

/*
 Load 128-bits (composed of 2 packed 64-bit doubles) from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m128i _mm_load_epi64_dbg(void const* mem_addr)
{
  return _mm_load_si128((__m128i*)mem_addr);
}
#undef _mm_load_epi64
#define _mm_load_epi64 _mm_load_epi64_dbg

/*
 Load 512-bits (composed of 8 packed 64-bit doubles) from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m512i _mm512_loadu_si512_dbg(void const* mem_addr)
{
  union simd
  {
  __m512i dst512;
  __m128i dst128[4];
  } dst;
  for (int i = 0; i < 4; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst512;
}
#undef _mm512_loadu_si512
#define _mm512_loadu_si512 _mm512_loadu_si512_dbg

/*
 Load 256-bits of integer data from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m256i _mm256_loadu_si256_dbg(void const* mem_addr)
{
  union simd
  {
  __m256i dst256;
  __m128i dst128[2];
  } dst;
  for (int i = 0; i < 2; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst256;
}
#undef _mm256_loadu_si256
#define _mm256_loadu_si256 _mm256_loadu_si256_dbg

/*
 Load 256-bits of integer data from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m256i _mm256_loadu_epi16_dbg(void const* mem_addr)
{
  union simd
  {
  __m256i dst256;
  __m128i dst128[2];
  } dst;
  for (int i = 0; i < 2; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst256;
}
#undef _mm256_loadu_epi16
#define _mm256_loadu_epi16 _mm256_loadu_epi16_dbg

/*
 Load 256-bits of integer data from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m256 _mm256_loadu_ps_dbg(void const* mem_addr)
{
  union simd
  {
  __m256 dst256;
  __m128i dst128[2];
  } dst;
  for (int i = 0; i < 2; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst256;
}
#undef _mm256_loadu_ps
#define _mm256_loadu_ps _mm256_loadu_ps_dbg

/*
 Load 256-bits of integer data from memory into dst. mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m256d _mm256_loadu_pd_dbg(void const* mem_addr)
{
  union simd
  {
  __m256d dst256;
  __m128i dst128[2];
  } dst;
  for (int i = 0; i < 2; i++)
	  dst.dst128[i] = _mm_loadu_si128((__m128i*)mem_addr + i);
  return dst.dst256;
}
#undef _mm256_loadu_pd
#define _mm256_loadu_pd _mm256_loadu_pd_dbg

/*
 Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from a into memory. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm512_storeu_pd_dbg(void *mem_addr, __m512d A)
{
  union simd
  {
  __m512d dst512;
  __m128i dst128[4];
  } dst;
  dst.dst512 = A;
  for (int i = 0; i < 4; i++)
	  _mm_storeu_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm512_storeu_pd
#define _mm512_storeu_pd _mm512_storeu_pd_dbg

/*
 Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from a into memory. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm512_store_si512_dbg(void *mem_addr, __m512i A)
{
  union simd
  {
  __m512i dst512;
  __m128i dst128[4];
  } dst;
  dst.dst512 = A;
  for (int i = 0; i < 4; i++)
	  _mm_store_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm512_store_si512
#define _mm512_store_si512 _mm512_store_si512_dbg

/*
 Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from a into memory. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm512_storeu_ps_dbg(void *mem_addr, __m512 A)
{
  union simd
  {
  __m512 dst512;
  __m128i dst128[4];
  } dst;
  dst.dst512 = A;
  for (int i = 0; i < 4; i++)
	  _mm_storeu_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm512_storeu_ps
#define _mm512_storeu_ps _mm512_storeu_ps_dbg

/*
 Store 512-bits (composed of 32 packed 16-bit integers) from "a" into memory.
  "mem_addr" does not need to be aligned on any particular boundary..
*/
static inline void _mm512_storeu_epi16_dbg(void *mem_addr, __m512i A)
{
  _mm512_storeu_ps(mem_addr, (__m512)A);
}
#undef _mm512_storeu_epi16
#define _mm512_storeu_epi16 _mm512_storeu_epi16_dbg

/*
 Store 512-bits (composed of 8 packed double-precision (64-bit) floating-point elements) from a into memory. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm512_storeu_si512_dbg(void *mem_addr, __m512i A)
{
  union simd
  {
  __m512i dst512;
  __m128i dst128[4];
  } dst;
  dst.dst512 = A;
  for (int i = 0; i < 4; i++)
	  _mm_storeu_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm512_storeu_si512
#define _mm512_storeu_si512 _mm512_storeu_si512_dbg

/*
 Load packed 32-bit integers from memory into dst using writemask k (elements are copied from src when the corresponding mask bit is not set). mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m512i _mm512_mask_loadu_epi32_dbg(__m512i src, __mmask16 k, void const* mem_addr)
{
  int32_t *dst_vec = (int32_t*)mem_addr;
  int32_t src_vec[16];
  union simd
  {
  __m512i dst512;
  int32_t dst32[16];
  } dst;
  dst.dst512 = src;
  for (int i = 0; i < 15; i++)
      if (k & ((1 << i) & 0xffff)) {
        dst.dst32[i] = dst_vec[i];
      }
  return dst.dst512;
}
#undef _mm512_mask_loadu_epi32
#define _mm512_mask_loadu_epi32 _mm512_mask_loadu_epi32_dbg

/*
 Store packed 32-bit integers from a into memory using writemask k. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm512_mask_storeu_epi32_dbg (void* mem_addr, __mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t *dst_vec = (int32_t*)mem_addr;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    }
  }
}
#undef _mm512_mask_storeu_epi32
#define _mm512_mask_storeu_epi32 _mm512_mask_storeu_epi32_dbg


/*
 Store 256-bits of integer data from a into memory. mem_addr does not need to be aligned on any particular boundary.

*/
static inline void _mm256_storeu_si256_dbg(__m256i *mem_addr, __m256i A)
{
  union simd
  {
  __m256i dst256;
  __m128i dst128[4];
  } dst;
  dst.dst256 = A;
  for (int i = 0; i < 2; i++)
	  _mm_storeu_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm256_storeu_si256
#define _mm256_storeu_si256 _mm256_storeu_si256_dbg 

/*
 Store 256-bits (composed of 16 packed 16-bit integers) from "a" into memory.
                "mem_addr" does not need to be aligned on any particular boundary.
*/
static inline void _mm256_storeu_epi16_dbg(void *mem_addr, __m256i A)
{
  _mm256_storeu_si256((__m256i*)mem_addr, A);
}
#undef _mm256_storeu_epi16
#define _mm256_storeu_epi16 _mm256_storeu_epi16_dbg 

/*
 Store packed 32-bit integers from a into memory using writemask k. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm256_mask_storeu_epi32_dbg (void* mem_addr, __mmask16 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t *dst_vec = (int32_t*)mem_addr;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    }
  }
}
#undef _mm256_mask_storeu_epi32
#define _mm256_mask_storeu_epi32 _mm256_mask_storeu_epi32_dbg

/*
 Store packed 32-bit integers from a into memory using writemask k. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm_mask_storeu_epi32_dbg (void* mem_addr, __mmask16 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t *dst_vec = (int32_t*)mem_addr;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    }
  }
}
#undef _mm_mask_storeu_epi32
#define _mm_mask_storeu_epi32 _mm_mask_storeu_epi32_dbg

/*
 Store packed 8-bit integers from a into memory using writemask k. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm_mask_storeu_epi8_dbg (void* mem_addr, __mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t *dst_vec = (int8_t*)mem_addr;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    }
  }
}
#undef _mm_mask_storeu_epi8
#define _mm_mask_storeu_epi8 _mm_mask_storeu_epi8_dbg

/*
 Store packed 16-bit integers from a into memory using writemask k. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm256_mask_storeu_epi16_dbg (void* mem_addr, __mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t *dst_vec = (int16_t*)mem_addr;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    }
  }
}
#undef _mm256_mask_storeu_epi16
#define _mm256_mask_storeu_epi16 _mm256_mask_storeu_epi16_dbg

/*
 Store 256-bits of integer data from a into memory. mem_addr does not need to be aligned on any particular boundary.

*/
static inline void _mm256_storeu_pd_dbg(double *mem_addr, __m256d A)
{
  union simd
  {
  __m256d dst256;
  __m128i dst128[4];
  } dst;
  dst.dst256 = A;
  for (int i = 0; i < 2; i++)
	  _mm_storeu_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm256_storeu_pd
#define _mm256_storeu_pd _mm256_storeu_pd_dbg 

/*
 Store 256-bits of integer data from a into memory. mem_addr does not need to be aligned on any particular boundary.

*/
static inline void _mm256_storeu_ps_dbg(float *mem_addr, __m256 A)
{
  union simd
  {
  __m256 dst256;
  __m128i dst128[4];
  } dst;
  dst.dst256 = A;
  for (int i = 0; i < 2; i++)
	  _mm_storeu_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm256_storeu_ps
#define _mm256_storeu_ps _mm256_storeu_ps_dbg 

/*
 Store 128-bits of integer data from a into memory. mem_addr does not need to be aligned on any particular boundary.

*/
static inline void _mm_storeu_epi64_dbg(void *mem_addr, __m128i A)
{
  _mm_storeu_si128((__m128i*)mem_addr, A);
}
#undef _mm_storeu_epi64
#undef _mm_store_epi64
#define _mm_store_epi64 _mm_storeu_epi64_dbg 
#define _mm_storeu_epi64 _mm_storeu_epi64_dbg 

/*
 Store 512-bits (composed of 8 packed 64-bit integers) from a into memory. mem_addr must be aligned on a 64-byte boundary or a general-protection exception may be generated.
*/
static inline void _mm512_store_epi64_dbg(void * mem_addr, __m512i A)
{
  union simd
  {
  __m512i dst512;
  __m128i dst128[4];
  } dst;
  dst.dst512 = A;
  for (int i = 0; i < 4; i++)
	  _mm_store_si128((__m128i*)mem_addr + i, dst.dst128[i]);
}
#undef _mm512_store_epi64
#define _mm512_store_epi64 _mm512_store_epi64_dbg

/*
 Store packed 32-bit integers from a into memory using writemask k. mem_addr does not need to be aligned on any particular boundary.
*/
static inline void _mm_mask_store_epi32_dbg (void* mem_addr, __mmask16 k, __m128i a)
{
  int32_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t *dst_vec = (int32_t*)mem_addr;
  for (int j = 0; j < 4; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    }
  }
}
#undef _mm_mask_store_epi32
#define _mm_mask_store_epi32 _mm_mask_store_epi32_dbg

/*
 Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m512i _mm512_maskz_loadu_epi32_dbg (__mmask16 k, void const* mem_addr)
{
  int32_t *a_vec = (int32_t*)mem_addr;
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_epi16((void*)dst_vec);
}
#undef _mm512_maskz_loadu_epi32
#define _mm512_maskz_loadu_epi32 _mm512_maskz_loadu_epi32_dbg

/*
 Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m256i _mm256_maskz_loadu_epi32_dbg (__mmask16 k, void const* mem_addr)
{
  int32_t *a_vec = (int32_t*)mem_addr;
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_epi16((void*)dst_vec);
}
#undef _mm256_maskz_loadu_epi32
#define _mm256_maskz_loadu_epi32 _mm256_maskz_loadu_epi32_dbg

/*
 Load packed 32-bit floats from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m256 _mm256_maskz_loadu_ps_dbg (__mmask16 k, void const* mem_addr)
{
  float *a_vec = (float*)mem_addr;
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}
#undef _mm256_maskz_loadu_ps
#define _mm256_maskz_loadu_ps _mm256_maskz_loadu_ps_dbg

/*
 Load packed 32-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m128i _mm_maskz_loadu_epi32_dbg (__mmask16 k, void const* mem_addr)
{
  int32_t *a_vec = (int32_t*)mem_addr;
  int32_t dst_vec[4];
  for (int j = 0; j < 4; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}
#undef _mm_maskz_loadu_epi32
#define _mm_maskz_loadu_epi32 _mm_maskz_loadu_epi32_dbg

/*
 Load packed 16-bit integers from memory into dst using zeromask k (elements are zeroed out when the corresponding mask bit is not set). mem_addr does not need to be aligned on any particular boundary.
*/
static inline __m256i _mm256_maskz_loadu_epi16_dbg (__mmask16 k, void const* mem_addr)
{
  int16_t *a_vec = (int16_t*)mem_addr;
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_epi16((void*)dst_vec);
}
#undef _mm256_maskz_loadu_epi16
#define _mm256_maskz_loadu_epi16 _mm256_maskz_loadu_epi16_dbg

static inline __m128i _mm_loadu_epi16_dbg(void const* mem_addr)
{
  return _mm_loadu_si128((__m128i*)mem_addr);
}
#undef _mm_loadu_epi16
#define _mm_loadu_epi16 _mm_loadu_epi16_dbg

/*
 Broadcast 32-bit integer "a" to all elements of "dst".
*/
static inline __m512i _mm512_set1_epi32_dbg(int a)
{
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}
#undef _mm512_set1_epi32
#define _mm512_set1_epi32 _mm512_set1_epi32_dbg

/*
 Copy "a" to "tmp", then insert a single-precision (32-bit) floating-point element from "b" into "tmp" using the control in "imm8". Store "tmp" to "dst" using the mask in "imm8" (elements are zeroed out when the corresponding bit is set). 
*/
static inline __m128 _mm_insert_ps_dbg(__m128 a, __m128 b, const int imm8)
{
  float tmp1;
  float tmp2_vec[4];
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  _mm_storeu_ps((float*)&tmp2_vec[0], a);
  switch ((imm8 & 0xc0) >> 6) {
    case 0:
    tmp1 = b_vec[0];
break;
    case 1:
    tmp1 = b_vec[1];
break;
    case 2:
    tmp1 = b_vec[2];
break;
    case 3:
    tmp1 = b_vec[3];
break;
  }
  switch ((imm8 & 0x30) >> 4) {
    case 0:
    tmp2_vec[0] = tmp1;
break;
    case 1:
    tmp2_vec[1] = tmp1;
break;
    case 2:
    tmp2_vec[2] = tmp1;
break;
    case 3:
    tmp2_vec[3] = tmp1;
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (imm8 & ((1 << j) & 0xff)) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = tmp2_vec[j];
    }
  }
return _mm_loadu_ps((float*)dst_vec);
}
#undef _mm_insert_ps
#define _mm_insert_ps _mm_insert_ps_dbg

/*
 Return vector of type __m512i with all elements set to zero.
*/
static inline __m512i _mm512_setzero_epi32_dbg()
{
  return _mm512_set1_epi32(0);
}
#undef _mm512_setzero_epi32
#define _mm512_setzero_epi32 _mm512_setzero_epi32_dbg

/*
 Return vector of type __m512i with all elements set to zero.
*/
static inline __m512i _mm512_setzero_si512_dbg()
{
  return _mm512_set1_epi32(0);
}
#undef _mm512_setzero_si512
#define _mm512_setzero_si512 _mm512_setzero_si512_dbg

/*
 Horizontally compute the minimum amongst the packed unsigned 16-bit integers in "a", store the minimum and index in "dst", and zero the remaining bits in "dst".
*/
static inline __m128i _mm_minpos_epu16_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  uint16_t dst_vec[8];
  int index = 0;
  uint16_t min = a_vec[0];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] < min) {
      index = j;
      min = a_vec[j];
    }
  }
  dst_vec[0] = min;
  dst_vec[1] = index;
  dst_vec[2] = dst_vec[3] = dst_vec[4] = dst_vec[5] = dst_vec[6] = dst_vec[7] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}
#undef _mm_minpos_epu16 
#define _mm_minpos_epu16 _mm_minpos_epu16_dbg

/*
 Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit in "imm8" is written to the corresponding bit in "dst".
	
*/
static inline __m512i _mm512_ternarylogic_epi32_dbg(__m512i a, __m512i b, __m512i c, int imm8)
{
  uint32_t a_vec[16];
  _mm512_store_epi64((void*)a_vec, a);
  uint32_t b_vec[16];
  _mm512_store_epi64((void*)b_vec, b);
  uint32_t c_vec[16];
  _mm512_store_epi64((void*)c_vec, c);
  uint32_t dst_vec[16];
  _mm512_store_epi64((void*)dst_vec, _mm512_setzero_epi32_dbg());

  for (int j = 0; j <= 15; j++) {
    for (int h = 0; h < 32; h++) {
      int index = ((((a_vec[j] & (1UL << h)) >> h) << 2) | (((b_vec[j] & (1UL << h)) >> h) << 1) | ((c_vec[j] & (1UL << h)) >> h)) & 0x7;
      dst_vec[j] = (dst_vec[j] & ~(1UL << h)) | ((((imm8 & (1UL << index)) >> index)) << h);
    }
  }
  return _mm512_loadu_epi16((void*)dst_vec);
}
#undef _mm512_ternarylogic_epi32 
#define _mm512_ternarylogic_epi32 _mm512_ternarylogic_epi32_dbg

/*
 Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the corresponding bit from "src", "a", and "b" are used to form a 3 bit index into "imm8", and the value at that bit in "imm8" is written to the corresponding bit in "dst" using writemask "k" at 32-bit granularity (32-bit elements are copied from "src" when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_mask_ternarylogic_epi32_dbg(__m512i src, __mmask16 k, __m512i a, __m512i b, int imm8)
{
  uint32_t src_vec[32];
  _mm512_store_epi32((void*)src_vec, src);
  uint32_t a_vec[32];
  _mm512_store_epi32((void*)a_vec, a);
  uint32_t b_vec[32];
  _mm512_store_epi32((void*)b_vec, b);
  uint32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & (1UL << j)) {
      for (int h = 0; h < 32; h++) {
        int index = ((((src_vec[j] & (1UL << h)) >> h) << 2) | (((a_vec[j] & (1UL << h)) >> h) << 1) | ((b_vec[j] & (1UL << h)) >> h)) & 0x7;
        dst_vec[j] = (dst_vec[j] & ~(1UL << h)) | ((((imm8 & (1UL << index)) >> index)) << h);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_epi16((void*)dst_vec);
}
#undef _mm512_mask_ternarylogic_epi32
#define _mm512_mask_ternarylogic_epi32 _mm512_mask_ternarylogic_epi32_dbg

/*
 Bitwise ternary logic that provides the capability to implement any three-operand binary function; the specific binary function is specified by value in "imm8". For each bit in each packed 32-bit integer, the corresponding bit from "a", "b", and "c" are used to form a 3 bit index into "imm8", and the value at that bit in "imm8" is written to the corresponding bit in "dst" using zeromask "k" at 32-bit granularity (32-bit elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_ternarylogic_epi32_dbg(__mmask16 k, __m512i a, __m512i b, __m512i c, int imm8)
{
  uint32_t a_vec[32];
  _mm512_store_epi64((void*)a_vec, a);
  uint32_t b_vec[32];
  _mm512_store_epi64((void*)b_vec, b);
  uint32_t c_vec[32];
  _mm512_store_epi64((void*)c_vec, c);
  uint32_t dst_vec[64];
  for (int j = 0; j <= 15; j++) {
    if (k & (1UL << j)) {
      for (int h = 0; h < 32; h++) {
        int index = ((((a_vec[j] & (1UL << h)) >> h) << 2) | (((b_vec[j] & (1UL << h)) >> h) << 1) | ((c_vec[j] & (1UL << h)) >> h)) & 0x7;
        dst_vec[j] = (dst_vec[j] & ~(1UL << h)) | ((((imm8 & (1UL << index)) >> index)) << h);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_epi16((void*)dst_vec);
}
#undef _mm512_maskz_ternarylogic_epi32
#define _mm512_maskz_ternarylogic_epi32 _mm512_maskz_ternarylogic_epi32_dbg

/*
 Shuffle packed 8-bit integers in a according to shuffle control mask in the corresponding 8-bit element of b, and store the results in dst.
	
*/
static inline __m512i _mm512_shuffle_epi8_dbg(__m512i a, __m512i b)
{
  uint8_t a_vec[64];
  _mm512_storeu_pd((void*)a_vec, (__m512d)a);
  uint8_t b_vec[64];
  _mm512_storeu_pd((void*)b_vec, (__m512d)b);
  uint8_t dst_vec[64];
  uint8_t index;
  for (int j = 0; j <= 63; j++) {
    if (b_vec[j] & 0x80) {
      dst_vec[j] = 0;
    } else {
      index = ((b_vec[j] & 0xf) + (j & 0x30)) & 0x3f;
      dst_vec[j] = (uint8_t)a_vec[index];
    }
  }
  return (__m512i)_mm512_loadu_pd((void*)dst_vec);
}
#undef _mm512_shuffle_epi8
#define _mm512_shuffle_epi8 _mm512_shuffle_epi8_dbg

/*
 Extract a 32-bit integer from a, selected with imm8, and store the result in dst.
*/
static inline int _mm_extract_epi32_dbg(__m128i a, const int imm8)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, (__m128)a);
  switch (imm8 & 0x3) {
     case 0:
       return a_vec[0];
     case 1:
       return a_vec[1];
     case 2:
       return a_vec[2];
     case 3:
       return a_vec[3];
  }
}
#undef _mm_extract_epi32
#define _mm_extract_epi32 _mm_extract_epi32_dbg

/*
		Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m256i _mm256_xor_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] ^ b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_xor_epi64
#define _mm256_xor_epi64 _mm256_xor_epi64_dbg


/*
 
		Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst".
	
*/
static inline __m256i _mm256_xor_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] ^ b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_xor_epi32
#define _mm256_xor_epi32 _mm256_xor_epi32_dbg


/*
 
		Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst".
	
*/
static inline __m128i _mm_xor_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = a_vec[j] ^ b_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_xor_epi64
#define _mm_xor_epi64 _mm_xor_epi64_dbg


/*
 
		Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst".
	
*/
static inline __m128i _mm_xor_epi32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] ^ b_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_xor_epi32
#define _mm_xor_epi32 _mm_xor_epi32_dbg


/*
 
		Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst".
	
*/
static inline __m256i _mm256_or_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] | b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_or_epi64
#define _mm256_or_epi64 _mm256_or_epi64_dbg


/*
 
		Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst".
	
*/
static inline __m256i _mm256_or_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] | b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_or_epi32
#define _mm256_or_epi32 _mm256_or_epi32_dbg


/*
 
		Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst".
	
*/
static inline __m128i _mm_or_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = a_vec[j] | b_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_or_epi64
#define _mm_or_epi64 _mm_or_epi64_dbg


/*
 
		Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst".
	
*/
static inline __m128i _mm_or_epi32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] | b_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_or_epi32
#define _mm_or_epi32 _mm_or_epi32_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_popcnt_epi64_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_popcnt_epi64
#define _mm256_maskz_popcnt_epi64 _mm256_maskz_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_popcnt_epi64_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_popcnt_epi64
#define _mm256_mask_popcnt_epi64 _mm256_mask_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_popcnt_epi64_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_popcnt_epi64
#define _mm256_popcnt_epi64 _mm256_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_popcnt_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_popcnt_epi64
#define _mm_maskz_popcnt_epi64 _mm_maskz_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_popcnt_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_popcnt_epi64
#define _mm_mask_popcnt_epi64 _mm_mask_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst".
*/
static inline __m128i _mm_popcnt_epi64_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_popcnt_epi64
#define _mm_popcnt_epi64 _mm_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_popcnt_epi32_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_popcnt_epi32
#define _mm256_popcnt_epi32 _mm256_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_popcnt_epi32_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_popcnt_epi32
#define _mm256_mask_popcnt_epi32 _mm256_mask_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_popcnt_epi32_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_popcnt_epi32
#define _mm256_maskz_popcnt_epi32 _mm256_maskz_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst".
*/
static inline __m128i _mm_popcnt_epi32_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_popcnt_epi32
#define _mm_popcnt_epi32 _mm_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_popcnt_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_popcnt_epi32
#define _mm_mask_popcnt_epi32 _mm_mask_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_popcnt_epi32_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_popcnt_epi32
#define _mm_maskz_popcnt_epi32 _mm_maskz_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst".
*/
static inline __m512i _mm512_popcnt_epi16_dbg(__m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_popcnt_epi16
#define _mm512_popcnt_epi16 _mm512_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_popcnt_epi16_dbg(__m512i src, __mmask32 k, __m512i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_popcnt_epi16
#define _mm512_mask_popcnt_epi16 _mm512_mask_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_popcnt_epi16_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_popcnt_epi16
#define _mm512_maskz_popcnt_epi16 _mm512_maskz_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_popcnt_epi16_dbg(__m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_popcnt_epi16
#define _mm256_popcnt_epi16 _mm256_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_popcnt_epi16_dbg(__m256i src, __mmask16 k, __m256i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_popcnt_epi16
#define _mm256_mask_popcnt_epi16 _mm256_mask_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_popcnt_epi16_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_popcnt_epi16
#define _mm256_maskz_popcnt_epi16 _mm256_maskz_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst".
*/
static inline __m128i _mm_popcnt_epi16_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_popcnt_epi16
#define _mm_popcnt_epi16 _mm_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_popcnt_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_popcnt_epi16
#define _mm_mask_popcnt_epi16 _mm_mask_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 16-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_popcnt_epi16_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_popcnt_epi16
#define _mm_maskz_popcnt_epi16 _mm_maskz_popcnt_epi16_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst".
*/
static inline __m512i _mm512_popcnt_epi8_dbg(__m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_popcnt_epi8
#define _mm512_popcnt_epi8 _mm512_popcnt_epi8_dbg

/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_popcnt_epi8_dbg(__m512i src, __mmask64 k, __m512i a)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_popcnt_epi8
#define _mm512_mask_popcnt_epi8 _mm512_mask_popcnt_epi8_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_popcnt_epi8_dbg(__mmask64 k, __m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_popcnt_epi8
#define _mm512_maskz_popcnt_epi8 _mm512_maskz_popcnt_epi8_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_popcnt_epi8_dbg(__m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_popcnt_epi8
#define _mm256_popcnt_epi8 _mm256_popcnt_epi8_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_popcnt_epi8_dbg(__m256i src, __mmask32 k, __m256i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_popcnt_epi8
#define _mm256_mask_popcnt_epi8 _mm256_mask_popcnt_epi8_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_popcnt_epi8_dbg(__mmask32 k, __m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_popcnt_epi8
#define _mm256_maskz_popcnt_epi8 _mm256_maskz_popcnt_epi8_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst".
*/
static inline __m128i _mm_popcnt_epi8_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_popcnt_epi8
#define _mm_popcnt_epi8 _mm_popcnt_epi8_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_popcnt_epi8_dbg(__m128i src, __mmask16 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_popcnt_epi8
#define _mm_mask_popcnt_epi8 _mm_mask_popcnt_epi8_dbg


/*
 Count the number of logical 1 bits in packed 8-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_popcnt_epi8_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_popcnt_epi8
#define _mm_maskz_popcnt_epi8 _mm_maskz_popcnt_epi8_dbg


/*
 Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_expand_epi16_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_expand_epi16
#define _mm512_maskz_expand_epi16 _mm512_maskz_expand_epi16_dbg


/*
 Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_expand_epi16_dbg(__m512i src, __mmask32 k, __m512i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_expand_epi16
#define _mm512_mask_expand_epi16 _mm512_mask_expand_epi16_dbg


/*
 Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_expand_epi16_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_expand_epi16
#define _mm256_maskz_expand_epi16 _mm256_maskz_expand_epi16_dbg


/*
 Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_expand_epi16_dbg(__m256i src, __mmask16 k, __m256i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_expand_epi16
#define _mm256_mask_expand_epi16 _mm256_mask_expand_epi16_dbg


/*
 Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_expand_epi16_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_expand_epi16
#define _mm_maskz_expand_epi16 _mm_maskz_expand_epi16_dbg


/*
 Load contiguous active 16-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_expand_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_expand_epi16
#define _mm_mask_expand_epi16 _mm_mask_expand_epi16_dbg


/*
 Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_expand_epi8_dbg(__mmask64 k, __m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  int m = 0;
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_expand_epi8
#define _mm512_maskz_expand_epi8 _mm512_maskz_expand_epi8_dbg


/*
 Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_expand_epi8_dbg(__m512i src, __mmask64 k, __m512i a)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  int m = 0;
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_expand_epi8
#define _mm512_mask_expand_epi8 _mm512_mask_expand_epi8_dbg


/*
 Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_expand_epi8_dbg(__mmask32 k, __m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_expand_epi8
#define _mm256_maskz_expand_epi8 _mm256_maskz_expand_epi8_dbg


/*
 Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_expand_epi8_dbg(__m256i src, __mmask32 k, __m256i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_expand_epi8
#define _mm256_mask_expand_epi8 _mm256_mask_expand_epi8_dbg


/*
 Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_expand_epi8_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_expand_epi8
#define _mm_maskz_expand_epi8 _mm_maskz_expand_epi8_dbg


/*
 Load contiguous active 8-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_expand_epi8_dbg(__m128i src, __mmask16 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_expand_epi8
#define _mm_mask_expand_epi8 _mm_mask_expand_epi8_dbg

/*
 Compute the bitwise NOT of 16-bit masks "a" and then AND with "b", and store the result in "k".
*/
static inline __mmask16 _mm512_kandn_dbg(__mmask16 a, __mmask16 b)
{
  __mmask16 k;
  k = (~ a) & b;
  return k;
}

#undef _mm512_kandn
#define _mm512_kandn _mm512_kandn_dbg

/*
 Compute the bitwise AND of 16-bit masks "a" and "b", and store the result in "k".
*/
static inline __mmask16 _mm512_kand_dbg(__mmask16 a, __mmask16 b)
{
  __mmask16 k;
  k = a & b;
  return k;
}

#undef _mm512_kand
#define _mm512_kand _mm512_kand_dbg


/*
 Copy 16-bit mask "a" to "k".
*/
static inline __mmask16 _mm512_kmov_dbg(__mmask16 a)
{
  __mmask16 k;
  k = a;
  return k;
}

#undef _mm512_kmov
#define _mm512_kmov _mm512_kmov_dbg


/*
 Compute the bitwise NOT of 16-bit mask "a", and store the result in "k".
*/
static inline __mmask16 _mm512_knot_dbg(__mmask16 a)
{
  __mmask16 k;
  k = ~ a;
  return k;
}

#undef _mm512_knot
#define _mm512_knot _mm512_knot_dbg


/*
 Compute the bitwise OR of 16-bit masks "a" and "b", and store the result in "k".
*/
static inline __mmask16 _mm512_kor_dbg(__mmask16 a, __mmask16 b)
{
  __mmask16 k;
  k = a | b;
  return k;
}

#undef _mm512_kor
#define _mm512_kor _mm512_kor_dbg


/*
 Unpack and interleave 8 bits from masks "a" and "b", and store the 16-bit result in "k".
*/
static inline __mmask16 _mm512_kunpackb_dbg(__mmask16 a, __mmask16 b)
{
  __mmask16 k;
  k = (b & 0xff);
  k |= (a & 0xff) << 8;
  return k;
}

#undef _mm512_kunpackb
#define _mm512_kunpackb _mm512_kunpackb_dbg


/*
 Compute the bitwise XNOR of 16-bit masks "a" and "b", and store the result in "k".
*/
static inline __mmask16 _mm512_kxnor_dbg(__mmask16 a, __mmask16 b)
{
  __mmask16 k;
  k = ~ (a ^ b);
  return k;
}

#undef _mm512_kxnor
#define _mm512_kxnor _mm512_kxnor_dbg


/*
 Compute the bitwise XOR of 16-bit masks "a" and "b", and store the result in "k".
*/
static inline __mmask16 _mm512_kxor_dbg(__mmask16 a, __mmask16 b)
{
  __mmask16 k;
  k = a ^ b;
  return k;
}

#undef _mm512_kxor
#define _mm512_kxor _mm512_kxor_dbg


/*
 Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_add_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_add_pd
#define _mm512_maskz_add_pd _mm512_maskz_add_pd_dbg


/*
 Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_maskz_add_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_add_round_pd
#define _mm512_maskz_add_round_pd _mm512_maskz_add_round_pd_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_add_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_add_ps
#define _mm512_maskz_add_ps _mm512_maskz_add_ps_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_maskz_add_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_add_round_ps
#define _mm512_maskz_add_round_ps _mm512_maskz_add_round_ps_dbg


/*
 Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_add_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = a_vec[0] + b_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_add_round_sd
#define _mm_add_round_sd _mm_add_round_sd_dbg


/*
 Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_mask_add_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_add_round_sd
#define _mm_mask_add_round_sd _mm_mask_add_round_sd_dbg


/*
 Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". 
*/
static inline __m128d _mm_mask_add_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_add_sd
#define _mm_mask_add_sd _mm_mask_add_sd_dbg


/*
 Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_maskz_add_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_add_round_sd
#define _mm_maskz_add_round_sd _mm_maskz_add_round_sd_dbg


/*
 Add the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_add_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_add_sd
#define _mm_maskz_add_sd _mm_maskz_add_sd_dbg

/*
 Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_add_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = a_vec[0] + b_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_add_round_ss
#define _mm_add_round_ss _mm_add_round_ss_dbg


/*
 Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_mask_add_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_add_round_ss
#define _mm_mask_add_round_ss _mm_mask_add_round_ss_dbg


/*
 Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
*/
static inline __m128 _mm_mask_add_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_add_ss
#define _mm_mask_add_ss _mm_mask_add_ss_dbg


/*
 Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_maskz_add_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_add_round_ss
#define _mm_maskz_add_round_ss _mm_maskz_add_round_ss_dbg


/*
 Add the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_add_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] + b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_add_ss
#define _mm_maskz_add_ss _mm_maskz_add_ss_dbg


/*
 Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m512 _mm512_broadcast_f32x4_dbg(__m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 4);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_broadcast_f32x4
#define _mm512_broadcast_f32x4 _mm512_broadcast_f32x4_dbg


/*
 Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_broadcast_f32x4_dbg(__m512 src, __mmask16 k, __m128 a)
{
  int32_t src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_broadcast_f32x4
#define _mm512_mask_broadcast_f32x4 _mm512_mask_broadcast_f32x4_dbg


/*
 Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_broadcast_f32x4_dbg(__mmask16 k, __m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_f32x4
#define _mm512_maskz_broadcast_f32x4 _mm512_maskz_broadcast_f32x4_dbg


/*
 Broadcast the 4 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m512d _mm512_broadcast_f64x4_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_broadcast_f64x4
#define _mm512_broadcast_f64x4 _mm512_broadcast_f64x4_dbg


/*
 Broadcast the 4 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_broadcast_f64x4_dbg(__m512d src, __mmask8 k, __m256d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_broadcast_f64x4
#define _mm512_mask_broadcast_f64x4 _mm512_mask_broadcast_f64x4_dbg


/*
 Broadcast the 4 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_broadcast_f64x4_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_f64x4
#define _mm512_maskz_broadcast_f64x4 _mm512_maskz_broadcast_f64x4_dbg


/*
 Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst".
*/
static inline __m512i _mm512_broadcast_i32x4_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 4);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcast_i32x4
#define _mm512_broadcast_i32x4 _mm512_broadcast_i32x4_dbg


/*
 Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcast_i32x4_dbg(__m512i src, __mmask16 k, __m128i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcast_i32x4
#define _mm512_mask_broadcast_i32x4 _mm512_mask_broadcast_i32x4_dbg


/*
 Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_broadcast_i32x4_dbg(__mmask16 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_i32x4
#define _mm512_maskz_broadcast_i32x4 _mm512_maskz_broadcast_i32x4_dbg


/*
 Broadcast the 4 packed 64-bit integers from "a" to all elements of "dst".
*/
static inline __m512i _mm512_broadcast_i64x4_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcast_i64x4
#define _mm512_broadcast_i64x4 _mm512_broadcast_i64x4_dbg


/*
 Broadcast the 4 packed 64-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcast_i64x4_dbg(__m512i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcast_i64x4
#define _mm512_mask_broadcast_i64x4 _mm512_mask_broadcast_i64x4_dbg


/*
 Broadcast the 4 packed 64-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_broadcast_i64x4_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_i64x4
#define _mm512_maskz_broadcast_i64x4 _mm512_maskz_broadcast_i64x4_dbg


/*
 Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst".
*/
static inline __m512d _mm512_broadcastsd_pd_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_broadcastsd_pd
#define _mm512_broadcastsd_pd _mm512_broadcastsd_pd_dbg


/*
 Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_broadcastsd_pd_dbg(__m512d src, __mmask8 k, __m128d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_broadcastsd_pd
#define _mm512_mask_broadcastsd_pd _mm512_mask_broadcastsd_pd_dbg


/*
 Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_broadcastsd_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_broadcastsd_pd
#define _mm512_maskz_broadcastsd_pd _mm512_maskz_broadcastsd_pd_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst".
*/
static inline __m512 _mm512_broadcastss_ps_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_broadcastss_ps
#define _mm512_broadcastss_ps _mm512_broadcastss_ps_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_broadcastss_ps_dbg(__m512 src, __mmask16 k, __m128 a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_broadcastss_ps
#define _mm512_mask_broadcastss_ps _mm512_mask_broadcastss_ps_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_broadcastss_ps_dbg(__mmask16 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_broadcastss_ps
#define _mm512_maskz_broadcastss_ps _mm512_maskz_broadcastss_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m512d _mm512_cvtepi32_pd_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvtepi32_pd
#define _mm512_cvtepi32_pd _mm512_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_cvtepi32_pd_dbg(__m512d src, __mmask8 k, __m256i a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvtepi32_pd
#define _mm512_mask_cvtepi32_pd _mm512_mask_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_cvtepi32_pd_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi32_pd
#define _mm512_maskz_cvtepi32_pd _mm512_maskz_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_cvt_roundepi32_ps_dbg(__m512i a, int rounding)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_Int32_To_FP32_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvt_roundepi32_ps
#define _mm512_cvt_roundepi32_ps _mm512_cvt_roundepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m512 _mm512_cvtepi32_ps_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvtepi32_ps
#define _mm512_cvtepi32_ps _mm512_cvtepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_mask_cvt_roundepi32_ps_dbg(__m512 src, __mmask16 k, __m512i a, int rounding)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_Int32_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundepi32_ps
#define _mm512_mask_cvt_roundepi32_ps _mm512_mask_cvt_roundepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_cvtepi32_ps_dbg(__m512 src, __mmask16 k, __m512i a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvtepi32_ps
#define _mm512_mask_cvtepi32_ps _mm512_mask_cvtepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_maskz_cvt_roundepi32_ps_dbg(__mmask16 k, __m512i a, int rounding)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_Int32_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundepi32_ps
#define _mm512_maskz_cvt_roundepi32_ps _mm512_maskz_cvt_roundepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_cvtepi32_ps_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi32_ps
#define _mm512_maskz_cvtepi32_ps _mm512_maskz_cvtepi32_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256i _mm512_cvt_roundpd_epi32_dbg(__m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_Int32_rounding(a_vec[j], rounding);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvt_roundpd_epi32
#define _mm512_cvt_roundpd_epi32 _mm512_cvt_roundpd_epi32_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm512_cvtpd_epi32_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtpd_epi32
#define _mm512_cvtpd_epi32 _mm512_cvtpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256i _mm512_mask_cvt_roundpd_epi32_dbg(__m256i src, __mmask8 k, __m512d a, int rounding)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundpd_epi32
#define _mm512_mask_cvt_roundpd_epi32 _mm512_mask_cvt_roundpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtpd_epi32_dbg(__m256i src, __mmask8 k, __m512d a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtpd_epi32
#define _mm512_mask_cvtpd_epi32 _mm512_mask_cvtpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256i _mm512_maskz_cvt_roundpd_epi32_dbg(__mmask8 k, __m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundpd_epi32
#define _mm512_maskz_cvt_roundpd_epi32 _mm512_maskz_cvt_roundpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm512_maskz_cvtpd_epi32_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtpd_epi32
#define _mm512_maskz_cvtpd_epi32 _mm512_maskz_cvtpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_cvt_roundpd_ps_dbg(__m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_FP32_rounding(a_vec[j], rounding);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvt_roundpd_ps
#define _mm512_cvt_roundpd_ps _mm512_cvt_roundpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256 _mm512_cvtpd_ps_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvtpd_ps
#define _mm512_cvtpd_ps _mm512_cvtpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_mask_cvt_roundpd_ps_dbg(__m256 src, __mmask8 k, __m512d a, int rounding)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundpd_ps
#define _mm512_mask_cvt_roundpd_ps _mm512_mask_cvt_roundpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm512_mask_cvtpd_ps_dbg(__m256 src, __mmask8 k, __m512d a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvtpd_ps
#define _mm512_mask_cvtpd_ps _mm512_mask_cvtpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_maskz_cvt_roundpd_ps_dbg(__mmask8 k, __m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundpd_ps
#define _mm512_maskz_cvt_roundpd_ps _mm512_maskz_cvt_roundpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm512_maskz_cvtpd_ps_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvtpd_ps
#define _mm512_maskz_cvtpd_ps _mm512_maskz_cvtpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256i _mm512_cvt_roundpd_epu32_dbg(__m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt32_rounding(a_vec[j], rounding);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvt_roundpd_epu32
#define _mm512_cvt_roundpd_epu32 _mm512_cvt_roundpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm512_cvtpd_epu32_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtpd_epu32
#define _mm512_cvtpd_epu32 _mm512_cvtpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256i _mm512_mask_cvt_roundpd_epu32_dbg(__m256i src, __mmask8 k, __m512d a, int rounding)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundpd_epu32
#define _mm512_mask_cvt_roundpd_epu32 _mm512_mask_cvt_roundpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtpd_epu32_dbg(__m256i src, __mmask8 k, __m512d a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtpd_epu32
#define _mm512_mask_cvtpd_epu32 _mm512_mask_cvtpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256i _mm512_maskz_cvt_roundpd_epu32_dbg(__mmask8 k, __m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundpd_epu32
#define _mm512_maskz_cvt_roundpd_epu32 _mm512_maskz_cvt_roundpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm512_maskz_cvtpd_epu32_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtpd_epu32
#define _mm512_maskz_cvtpd_epu32 _mm512_maskz_cvtpd_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_cvt_roundps_epi32_dbg(__m512 a, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_Int32_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvt_roundps_epi32
#define _mm512_cvt_roundps_epi32 _mm512_cvt_roundps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtps_epi32_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtps_epi32
#define _mm512_cvtps_epi32 _mm512_cvtps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_mask_cvt_roundps_epi32_dbg(__m512i src, __mmask16 k, __m512 a, int rounding)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_Int32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundps_epi32
#define _mm512_mask_cvt_roundps_epi32 _mm512_mask_cvt_roundps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtps_epi32_dbg(__m512i src, __mmask16 k, __m512 a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtps_epi32
#define _mm512_mask_cvtps_epi32 _mm512_mask_cvtps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_maskz_cvt_roundps_epi32_dbg(__mmask16 k, __m512 a, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_Int32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundps_epi32
#define _mm512_maskz_cvt_roundps_epi32 _mm512_maskz_cvt_roundps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtps_epi32_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtps_epi32
#define _mm512_maskz_cvtps_epi32 _mm512_maskz_cvtps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512d _mm512_cvt_roundps_pd_dbg(__m256 a, int sae)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_FP64(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvt_roundps_pd
#define _mm512_cvt_roundps_pd _mm512_cvt_roundps_pd_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m512d _mm512_cvtps_pd_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_FP64(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvtps_pd
#define _mm512_cvtps_pd _mm512_cvtps_pd_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512d _mm512_mask_cvt_roundps_pd_dbg(__m512d src, __mmask8 k, __m256 a, int sae)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundps_pd
#define _mm512_mask_cvt_roundps_pd _mm512_mask_cvt_roundps_pd_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_cvtps_pd_dbg(__m512d src, __mmask8 k, __m256 a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvtps_pd
#define _mm512_mask_cvtps_pd _mm512_mask_cvtps_pd_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512d _mm512_maskz_cvt_roundps_pd_dbg(__mmask8 k, __m256 a, int sae)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundps_pd
#define _mm512_maskz_cvt_roundps_pd _mm512_maskz_cvt_roundps_pd_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_cvtps_pd_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvtps_pd
#define _mm512_maskz_cvtps_pd _mm512_maskz_cvtps_pd_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_cvt_roundps_epu32_dbg(__m512 a, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt32_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvt_roundps_epu32
#define _mm512_cvt_roundps_epu32 _mm512_cvt_roundps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtps_epu32_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtps_epu32
#define _mm512_cvtps_epu32 _mm512_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_mask_cvt_roundps_epu32_dbg(__m512i src, __mmask16 k, __m512 a, int rounding)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundps_epu32
#define _mm512_mask_cvt_roundps_epu32 _mm512_mask_cvt_roundps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_cvtps_epu32_dbg(__m512i src, __mmask16 k, __m512 a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtps_epu32
#define _mm512_mask_cvtps_epu32 _mm512_mask_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_maskz_cvt_roundps_epu32_dbg(__mmask16 k, __m512 a, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundps_epu32
#define _mm512_maskz_cvt_roundps_epu32 _mm512_maskz_cvt_roundps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtps_epu32_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtps_epu32
#define _mm512_maskz_cvtps_epu32 _mm512_maskz_cvtps_epu32_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvt_roundsd_i32_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int dst;
  dst = Convert_FP64_To_Int32_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundsd_i32
#define _mm_cvt_roundsd_i32 _mm_cvt_roundsd_i32_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvt_roundsd_i64_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst;
  dst = Convert_FP64_To_Int64_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundsd_i64
#define _mm_cvt_roundsd_i64 _mm_cvt_roundsd_i64_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvt_roundsd_si32_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int dst;
  dst = Convert_FP64_To_Int32_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundsd_si32
#define _mm_cvt_roundsd_si32 _mm_cvt_roundsd_si32_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvt_roundsd_si64_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst;
  dst = Convert_FP64_To_Int64_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundsd_si64
#define _mm_cvt_roundsd_si64 _mm_cvt_roundsd_si64_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer, and store the result in "dst".
*/
static inline int _mm_cvtsd_i32_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int dst;
  dst = Convert_FP64_To_Int32(a_vec[0]);
return dst;
}

#undef _mm_cvtsd_i32
#define _mm_cvtsd_i32 _mm_cvtsd_i32_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer, and store the result in "dst".
*/
static inline int64_t _mm_cvtsd_i64_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst;
  dst = Convert_FP64_To_Int64(a_vec[0]);
return dst;
}

#undef _mm_cvtsd_i64
#define _mm_cvtsd_i64 _mm_cvtsd_i64_dbg

/*
 Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_cvt_roundsd_ss_dbg(__m128 a, __m128d b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = Convert_FP64_To_FP32_rounding(b_vec[0], rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvt_roundsd_ss
#define _mm_cvt_roundsd_ss _mm_cvt_roundsd_ss_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_mask_cvt_roundsd_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128d b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP64_To_FP32_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_cvt_roundsd_ss
#define _mm_mask_cvt_roundsd_ss _mm_mask_cvt_roundsd_ss_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	
*/
static inline __m128 _mm_mask_cvtsd_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128d b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP64_To_FP32(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_cvtsd_ss
#define _mm_mask_cvtsd_ss _mm_mask_cvtsd_ss_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_maskz_cvt_roundsd_ss_dbg(__mmask8 k, __m128 a, __m128d b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP64_To_FP32_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_cvt_roundsd_ss
#define _mm_maskz_cvt_roundsd_ss _mm_maskz_cvt_roundsd_ss_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	
*/
static inline __m128 _mm_maskz_cvtsd_ss_dbg(__mmask8 k, __m128 a, __m128d b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP64_To_FP32(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_cvtsd_ss
#define _mm_maskz_cvtsd_ss _mm_maskz_cvtsd_ss_dbg

/*
 Convert the 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_cvt_roundi64_sd_dbg(__m128d a, int64_t b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  dst_vec[0] = Convert_Int64_To_FP64_rounding(b, rounding);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvt_roundi64_sd
#define _mm_cvt_roundi64_sd _mm_cvt_roundi64_sd_dbg


/*
 Convert the 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_cvt_roundsi64_sd_dbg(__m128d a, int64_t b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  dst_vec[0] = Convert_Int64_To_FP64_rounding(b, rounding);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvt_roundsi64_sd
#define _mm_cvt_roundsi64_sd _mm_cvt_roundsi64_sd_dbg


/*
 Convert the 32-bit integer "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". 
*/
static inline __m128d _mm_cvti32_sd_dbg(__m128d a, int b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  dst_vec[0] = Convert_Int32_To_FP64(b);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvti32_sd
#define _mm_cvti32_sd _mm_cvti32_sd_dbg


/*
 Convert the 64-bit integer "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". 
*/
static inline __m128d _mm_cvti64_sd_dbg(__m128d a, int64_t b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  dst_vec[0] = Convert_Int64_To_FP64(b);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvti64_sd
#define _mm_cvti64_sd _mm_cvti64_sd_dbg


/*
 Convert the 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_cvt_roundi32_ss_dbg(__m128 a, int b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  dst_vec[0] = Convert_Int32_To_FP32_rounding(b, rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvt_roundi32_ss
#define _mm_cvt_roundi32_ss _mm_cvt_roundi32_ss_dbg


/*
 Convert the 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_cvt_roundi64_ss_dbg(__m128 a, int64_t b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  dst_vec[0] = Convert_Int64_To_FP32_rounding(b, rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvt_roundi64_ss
#define _mm_cvt_roundi64_ss _mm_cvt_roundi64_ss_dbg


/*
 Convert the 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_cvt_roundsi32_ss_dbg(__m128 a, int b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  dst_vec[0] = Convert_Int32_To_FP32_rounding(b, rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvt_roundsi32_ss
#define _mm_cvt_roundsi32_ss _mm_cvt_roundsi32_ss_dbg


/*
 Convert the 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_cvt_roundsi64_ss_dbg(__m128 a, int64_t b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  dst_vec[0] = Convert_Int64_To_FP32_rounding(b, rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvt_roundsi64_ss
#define _mm_cvt_roundsi64_ss _mm_cvt_roundsi64_ss_dbg


/*
 Convert the 32-bit integer "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_cvti32_ss_dbg(__m128 a, int b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  dst_vec[0] = Convert_Int32_To_FP32(b);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvti32_ss
#define _mm_cvti32_ss _mm_cvti32_ss_dbg


/*
 Convert the 64-bit integer "b" to a single-precision (32-bit) floating-point element, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_cvti64_ss_dbg(__m128 a, int64_t b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  dst_vec[0] = Convert_Int64_To_FP32(b);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvti64_ss
#define _mm_cvti64_ss _mm_cvti64_ss_dbg

/*
 Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_cvt_roundss_sd_dbg(__m128d a, __m128 b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = Convert_FP32_To_FP64_rounding(b_vec[0], rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvt_roundss_sd
#define _mm_cvt_roundss_sd _mm_cvt_roundss_sd_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_mask_cvt_roundss_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128 b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP32_To_FP64_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_cvt_roundss_sd
#define _mm_mask_cvt_roundss_sd _mm_mask_cvt_roundss_sd_dbg

/*
 Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	
*/
static inline __m128d _mm_mask_cvtss_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128 b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP32_To_FP64(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_cvtss_sd
#define _mm_mask_cvtss_sd _mm_mask_cvtss_sd_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_maskz_cvt_roundss_sd_dbg(__mmask8 k, __m128d a, __m128 b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP32_To_FP64_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_cvt_roundss_sd
#define _mm_maskz_cvt_roundss_sd _mm_maskz_cvt_roundss_sd_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "b" to a double-precision (64-bit) floating-point element, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_cvtss_sd_dbg(__mmask8 k, __m128d a, __m128 b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = Convert_FP32_To_FP64(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_cvtss_sd
#define _mm_maskz_cvtss_sd _mm_maskz_cvtss_sd_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvt_roundss_i32_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int dst;
  dst = Convert_FP32_To_Int32_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundss_i32
#define _mm_cvt_roundss_i32 _mm_cvt_roundss_i32_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvt_roundss_i64_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst;
  dst = Convert_FP32_To_Int64_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundss_i64
#define _mm_cvt_roundss_i64 _mm_cvt_roundss_i64_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvt_roundss_si32_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int dst;
  dst = Convert_FP32_To_Int32_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundss_si32
#define _mm_cvt_roundss_si32 _mm_cvt_roundss_si32_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvt_roundss_si64_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst;
  dst = Convert_FP32_To_Int64_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvt_roundss_si64
#define _mm_cvt_roundss_si64 _mm_cvt_roundss_si64_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer, and store the result in "dst".
*/
static inline int _mm_cvtss_i32_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int dst;
  dst = Convert_FP32_To_Int32(a_vec[0]);
return dst;
}

#undef _mm_cvtss_i32
#define _mm_cvtss_i32 _mm_cvtss_i32_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer, and store the result in "dst".
*/
static inline int64_t _mm_cvtss_i64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst;
  dst = Convert_FP32_To_Int64(a_vec[0]);
return dst;
}

#undef _mm_cvtss_i64
#define _mm_cvtss_i64 _mm_cvtss_i64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m256i _mm512_cvtt_roundpd_epi32_dbg(__m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_IntegerTruncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtt_roundpd_epi32
#define _mm512_cvtt_roundpd_epi32 _mm512_cvtt_roundpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm512_cvttpd_epi32_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvttpd_epi32
#define _mm512_cvttpd_epi32 _mm512_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m256i _mm512_mask_cvtt_roundpd_epi32_dbg(__m256i src, __mmask8 k, __m512d a, int sae)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_IntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundpd_epi32
#define _mm512_mask_cvtt_roundpd_epi32 _mm512_mask_cvtt_roundpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvttpd_epi32_dbg(__m256i src, __mmask8 k, __m512d a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvttpd_epi32
#define _mm512_mask_cvttpd_epi32 _mm512_mask_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m256i _mm512_maskz_cvtt_roundpd_epi32_dbg(__mmask8 k, __m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_IntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundpd_epi32
#define _mm512_maskz_cvtt_roundpd_epi32 _mm512_maskz_cvtt_roundpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm512_maskz_cvttpd_epi32_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvttpd_epi32
#define _mm512_maskz_cvttpd_epi32 _mm512_maskz_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m256i _mm512_cvtt_roundpd_epu32_dbg(__m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedIntegerTruncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtt_roundpd_epu32
#define _mm512_cvtt_roundpd_epu32 _mm512_cvtt_roundpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm512_cvttpd_epu32_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvttpd_epu32
#define _mm512_cvttpd_epu32 _mm512_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m256i _mm512_mask_cvtt_roundpd_epu32_dbg(__m256i src, __mmask8 k, __m512d a, int sae)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedIntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundpd_epu32
#define _mm512_mask_cvtt_roundpd_epu32 _mm512_mask_cvtt_roundpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvttpd_epu32_dbg(__m256i src, __mmask8 k, __m512d a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvttpd_epu32
#define _mm512_mask_cvttpd_epu32 _mm512_mask_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m256i _mm512_maskz_cvtt_roundpd_epu32_dbg(__mmask8 k, __m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedIntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundpd_epu32
#define _mm512_maskz_cvtt_roundpd_epu32 _mm512_maskz_cvtt_roundpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm512_maskz_cvttpd_epu32_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvttpd_epu32
#define _mm512_maskz_cvttpd_epu32 _mm512_maskz_cvttpd_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_cvtt_roundps_epi32_dbg(__m512 a, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_IntegerTruncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtt_roundps_epi32
#define _mm512_cvtt_roundps_epi32 _mm512_cvtt_roundps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m512i _mm512_cvttps_epi32_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_Int32_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvttps_epi32
#define _mm512_cvttps_epi32 _mm512_cvttps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions. 
*/
static inline __m512i _mm512_mask_cvtt_roundps_epi32_dbg(__m512i src, __mmask16 k, __m512 a, int sae)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_IntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundps_epi32
#define _mm512_mask_cvtt_roundps_epi32 _mm512_mask_cvtt_roundps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvttps_epi32_dbg(__m512i src, __mmask16 k, __m512 a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvttps_epi32
#define _mm512_mask_cvttps_epi32 _mm512_mask_cvttps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_maskz_cvtt_roundps_epi32_dbg(__mmask16 k, __m512 a, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_IntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundps_epi32
#define _mm512_maskz_cvtt_roundps_epi32 _mm512_maskz_cvtt_roundps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvttps_epi32_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvttps_epi32
#define _mm512_maskz_cvttps_epi32 _mm512_maskz_cvttps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_cvtt_roundps_epu32_dbg(__m512 a, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  uint32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedIntegerTruncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtt_roundps_epu32
#define _mm512_cvtt_roundps_epu32 _mm512_cvtt_roundps_epu32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m512i _mm512_cvttps_epu32_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  uint32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt32_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvttps_epu32
#define _mm512_cvttps_epu32 _mm512_cvttps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_mask_cvtt_roundps_epu32_dbg(__m512i src, __mmask16 k, __m512 a, int sae)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  uint32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedIntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundps_epu32
#define _mm512_mask_cvtt_roundps_epu32 _mm512_mask_cvtt_roundps_epu32_dbg

/*
 Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvttps_epu32_dbg(__m512i src, __mmask16 k, __m512 a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  uint32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvttps_epu32
#define _mm512_mask_cvttps_epu32 _mm512_mask_cvttps_epu32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_maskz_cvtt_roundps_epu32_dbg(__mmask16 k, __m512 a, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  uint32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedIntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundps_epu32
#define _mm512_maskz_cvtt_roundps_epu32 _mm512_maskz_cvtt_roundps_epu32_dbg

/*
 Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvttps_epu32_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  uint32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvttps_epu32
#define _mm512_maskz_cvttps_epu32 _mm512_maskz_cvttps_epu32_dbg

/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvtt_roundsd_i32_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int dst;
  dst = Convert_FP64_To_Int32_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundsd_i32
#define _mm_cvtt_roundsd_i32 _mm_cvtt_roundsd_i32_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvtt_roundsd_i64_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst;
  dst = Convert_FP64_To_Int64_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundsd_i64
#define _mm_cvtt_roundsd_i64 _mm_cvtt_roundsd_i64_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvtt_roundsd_si32_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int dst;
  dst = Convert_FP64_To_Int32_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundsd_si32
#define _mm_cvtt_roundsd_si32 _mm_cvtt_roundsd_si32_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvtt_roundsd_si64_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst;
  dst = Convert_FP64_To_Int64_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundsd_si64
#define _mm_cvtt_roundsd_si64 _mm_cvtt_roundsd_si64_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 32-bit integer with truncation, and store the result in "dst".
*/
static inline int _mm_cvttsd_i32_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int dst;
  dst = Convert_FP64_To_Int32_Truncate(a_vec[0]);
return dst;
}

#undef _mm_cvttsd_i32
#define _mm_cvttsd_i32 _mm_cvttsd_i32_dbg


/*
 Convert the lower double-precision (64-bit) floating-point element in "a" to a 64-bit integer with truncation, and store the result in "dst".
*/
static inline int64_t _mm_cvttsd_i64_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst;
  dst = Convert_FP64_To_Int64_Truncate(a_vec[0]);
return dst;
}

#undef _mm_cvttsd_i64
#define _mm_cvttsd_i64 _mm_cvttsd_i64_dbg

/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvtt_roundss_i32_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int dst;
  dst = Convert_FP32_To_Int32_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundss_i32
#define _mm_cvtt_roundss_i32 _mm_cvtt_roundss_i32_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvtt_roundss_i64_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst;
  dst = Convert_FP32_To_Int64_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundss_i64
#define _mm_cvtt_roundss_i64 _mm_cvtt_roundss_i64_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int _mm_cvtt_roundss_si32_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int dst;
  dst = Convert_FP32_To_Int32_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundss_si32
#define _mm_cvtt_roundss_si32 _mm_cvtt_roundss_si32_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer with truncation, and store the result in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline int64_t _mm_cvtt_roundss_si64_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst;
  dst = Convert_FP32_To_Int64_Truncate_rounding(a_vec[0], rounding);
return dst;
}

#undef _mm_cvtt_roundss_si64
#define _mm_cvtt_roundss_si64 _mm_cvtt_roundss_si64_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 32-bit integer with truncation, and store the result in "dst".
*/
static inline int _mm_cvttss_i32_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int dst;
  dst = Convert_FP32_To_Int32_Truncate(a_vec[0]);
return dst;
}

#undef _mm_cvttss_i32
#define _mm_cvttss_i32 _mm_cvttss_i32_dbg


/*
 Convert the lower single-precision (32-bit) floating-point element in "a" to a 64-bit integer with truncation, and store the result in "dst".
*/
static inline int64_t _mm_cvttss_i64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst;
  dst = Convert_FP32_To_Int64_Truncate(a_vec[0]);
return dst;
}

#undef _mm_cvttss_i64
#define _mm_cvttss_i64 _mm_cvttss_i64_dbg






/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m512d _mm512_cvtepu32_pd_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvtepu32_pd
#define _mm512_cvtepu32_pd _mm512_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m512d _mm512_mask_cvtepu32_pd_dbg(__m512d src, __mmask8 k, __m256i a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvtepu32_pd
#define _mm512_mask_cvtepu32_pd _mm512_mask_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_cvtepu32_pd_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu32_pd
#define _mm512_maskz_cvtepu32_pd _mm512_maskz_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_cvt_roundepu32_ps_dbg(__m512i a, int rounding)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ConvertUnsignedInt32_To_FP32_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvt_roundepu32_ps
#define _mm512_cvt_roundepu32_ps _mm512_cvt_roundepu32_ps_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m512 _mm512_cvtepu32_ps_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ConvertUnsignedInt32_To_FP32(a_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvtepu32_ps
#define _mm512_cvtepu32_ps _mm512_cvtepu32_ps_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_mask_cvt_roundepu32_ps_dbg(__m512 src, __mmask16 k, __m512i a, int rounding)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ConvertUnsignedInt32_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundepu32_ps
#define _mm512_mask_cvt_roundepu32_ps _mm512_mask_cvt_roundepu32_ps_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m512 _mm512_mask_cvtepu32_ps_dbg(__m512 src, __mmask16 k, __m512i a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ConvertUnsignedInt32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvtepu32_ps
#define _mm512_mask_cvtepu32_ps _mm512_mask_cvtepu32_ps_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_maskz_cvt_roundepu32_ps_dbg(__mmask16 k, __m512i a, int rounding)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ConvertUnsignedInt32_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundepu32_ps
#define _mm512_maskz_cvt_roundepu32_ps _mm512_maskz_cvt_roundepu32_ps_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512 _mm512_maskz_cvtepu32_ps_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ConvertUnsignedInt32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu32_ps
#define _mm512_maskz_cvtepu32_ps _mm512_maskz_cvtepu32_ps_dbg









/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst".
*/
static inline __m512d _mm512_div_pd_dbg(__m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] / b_vec[j];
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_div_pd
#define _mm512_div_pd _mm512_div_pd_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", =and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_div_round_pd_dbg(__m512d a, __m512d b, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] / b_vec[j];
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_div_round_pd
#define _mm512_div_round_pd _mm512_div_round_pd_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_div_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_div_pd
#define _mm512_mask_div_pd _mm512_mask_div_pd_dbg


/*
 
	Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_mask_div_round_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b, int rounding)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_div_round_pd
#define _mm512_mask_div_round_pd _mm512_mask_div_round_pd_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_div_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_div_pd
#define _mm512_maskz_div_pd _mm512_maskz_div_pd_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_maskz_div_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_div_round_pd
#define _mm512_maskz_div_round_pd _mm512_maskz_div_round_pd_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst".
*/
static inline __m512 _mm512_div_ps_dbg(__m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] / b_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_div_ps
#define _mm512_div_ps _mm512_div_ps_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_div_round_ps_dbg(__m512 a, __m512 b, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] / b_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_div_round_ps
#define _mm512_div_round_ps _mm512_div_round_ps_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_div_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_div_ps
#define _mm512_mask_div_ps _mm512_mask_div_ps_dbg


/*
 
	Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_mask_div_round_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b, int rounding)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_div_round_ps
#define _mm512_mask_div_round_ps _mm512_mask_div_round_ps_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_div_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_div_ps
#define _mm512_maskz_div_ps _mm512_maskz_div_ps_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_maskz_div_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_div_round_ps
#define _mm512_maskz_div_round_ps _mm512_maskz_div_round_ps_dbg


/*
 Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_div_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = a_vec[0] / b_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_div_round_sd
#define _mm_div_round_sd _mm_div_round_sd_dbg


/*
 Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". 
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_mask_div_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_div_round_sd
#define _mm_mask_div_round_sd _mm_mask_div_round_sd_dbg


/*
 Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". 
*/
static inline __m128d _mm_mask_div_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_div_sd
#define _mm_mask_div_sd _mm_mask_div_sd_dbg


/*
 Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_maskz_div_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_div_round_sd
#define _mm_maskz_div_round_sd _mm_maskz_div_round_sd_dbg


/*
 Divide the lower double-precision (64-bit) floating-point element in "a" by the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_div_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_div_sd
#define _mm_maskz_div_sd _mm_maskz_div_sd_dbg


/*
 Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_div_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = a_vec[0] / b_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_div_round_ss
#define _mm_div_round_ss _mm_div_round_ss_dbg


/*
 Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_mask_div_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_div_round_ss
#define _mm_mask_div_round_ss _mm_mask_div_round_ss_dbg


/*
 Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
*/
static inline __m128 _mm_mask_div_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_div_ss
#define _mm_mask_div_ss _mm_mask_div_ss_dbg


/*
 Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_maskz_div_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_div_round_ss
#define _mm_maskz_div_round_ss _mm_maskz_div_round_ss_dbg


/*
 Divide the lower single-precision (32-bit) floating-point element in "a" by the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_div_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] / b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_div_ss
#define _mm_maskz_div_ss _mm_maskz_div_ss_dbg


/*
 Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_mask_expand_pd_dbg(__m512d src, __mmask8 k, __m512d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_expand_pd
#define _mm512_mask_expand_pd _mm512_mask_expand_pd_dbg

/*
 Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_expand_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_expand_pd
#define _mm512_maskz_expand_pd _mm512_maskz_expand_pd_dbg


/*
 Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_mask_expand_ps_dbg(__m512 src, __mmask16 k, __m512 a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_expand_ps
#define _mm512_mask_expand_ps _mm512_mask_expand_ps_dbg


/*
 Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_expand_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_expand_ps
#define _mm512_maskz_expand_ps _mm512_maskz_expand_ps_dbg


/*
 Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m128 _mm512_extractf32x4_ps_dbg(__m512 a, int imm8)
{
  __m128 a_vec[4];
  _mm512_storeu_ps((void*)a_vec, a);
  __m128 dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
    case 2:
    dst_vec[0] = a_vec[2];
break;
    case 3:
    dst_vec[0] = a_vec[3];
break;
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm512_extractf32x4_ps
#define _mm512_extractf32x4_ps _mm512_extractf32x4_ps_dbg


/*
 Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm512_mask_extractf32x4_ps_dbg(__m128 src, __mmask8 k, __m512 a, int imm8)
{
  float tmp_vec[16];
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  __m128 a_vec[4];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[0]);
break;
    case 1:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[1]);
break;
    case 2:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[2]);
break;
    case 3:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[3]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm512_mask_extractf32x4_ps
#define _mm512_mask_extractf32x4_ps _mm512_mask_extractf32x4_ps_dbg


/*
 Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128 _mm512_maskz_extractf32x4_ps_dbg(__mmask8 k, __m512 a, int imm8)
{
  float tmp_vec[16];
  __m128 a_vec[4];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[0]);
break;
    case 1:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[1]);
break;
    case 2:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[2]);
break;
    case 3:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[3]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm512_maskz_extractf32x4_ps
#define _mm512_maskz_extractf32x4_ps _mm512_maskz_extractf32x4_ps_dbg


/*
 Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m256d _mm512_extractf64x4_pd_dbg(__m512d a, int imm8)
{
  __m256d a_vec[2];
  _mm512_storeu_pd((void*)a_vec, a);
  __m256d dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm512_extractf64x4_pd
#define _mm512_extractf64x4_pd _mm512_extractf64x4_pd_dbg


/*
 Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm512_mask_extractf64x4_pd_dbg(__m256d src, __mmask8 k, __m512d a, int imm8)
{
  int64_t tmp_vec[4];
  int64_t src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  __m256d a_vec[2];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_pd((double*)tmp_vec, (__m256d)a_vec[0]);
break;
    case 1:
    _mm256_storeu_pd((double*)tmp_vec, (__m256d)a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_extractf64x4_pd
#define _mm512_mask_extractf64x4_pd _mm512_mask_extractf64x4_pd_dbg

/*
 Extract 256 bits (composed of 4 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256d _mm512_maskz_extractf64x4_pd_dbg(__mmask8 k, __m512d a, int imm8)
{
  int64_t tmp_vec[8];
  __m256d a_vec[2];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_pd((double*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm256_storeu_pd((double*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_extractf64x4_pd
#define _mm512_maskz_extractf64x4_pd _mm512_maskz_extractf64x4_pd_dbg


/*
 Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m128i _mm512_extracti32x4_epi32_dbg(__m512i a, int imm8)
{
  __m128i a_vec[4];
  _mm512_storeu_si512((void*)a_vec, a);
  __m128i dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
    case 2:
    dst_vec[0] = a_vec[2];
break;
    case 3:
    dst_vec[0] = a_vec[3];
break;
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_extracti32x4_epi32
#define _mm512_extracti32x4_epi32 _mm512_extracti32x4_epi32_dbg


/*
 Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_extracti32x4_epi32_dbg(__m128i src, __mmask8 k, __m512i a, int imm8)
{
  int32_t tmp_vec[4];
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  __m128i a_vec[4];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[0]);
break;
    case 1:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[1]);
break;
    case 2:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[2]);
break;
    case 3:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[3]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_extracti32x4_epi32
#define _mm512_mask_extracti32x4_epi32 _mm512_mask_extracti32x4_epi32_dbg


/*
 Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_extracti32x4_epi32_dbg(__mmask8 k, __m512i a, int imm8)
{
  int32_t tmp_vec[16];
  __m128i a_vec[4];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[0]);
break;
    case 1:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[1]);
break;
    case 2:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[2]);
break;
    case 3:
    _mm_storeu_ps((float*)tmp_vec, (__m128)a_vec[3]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_extracti32x4_epi32
#define _mm512_maskz_extracti32x4_epi32 _mm512_maskz_extracti32x4_epi32_dbg


/*
 Extract 256 bits (composed of 4 packed 64-bit integers) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m256i _mm512_extracti64x4_epi64_dbg(__m512i a, int imm8)
{
  __m256i a_vec[2];
  _mm512_storeu_si512((void*)a_vec, a);
  __m256i dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_extracti64x4_epi64
#define _mm512_extracti64x4_epi64 _mm512_extracti64x4_epi64_dbg


/*
 Extract 256 bits (composed of 4 packed 64-bit integers) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_extracti64x4_epi64_dbg(__m256i src, __mmask8 k, __m512i a, int imm8)
{
  int64_t tmp_vec[4];
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  __m256d a_vec[2];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_pd((double*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm256_storeu_pd((double*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_extracti64x4_epi64
#define _mm512_mask_extracti64x4_epi64 _mm512_mask_extracti64x4_epi64_dbg


/*
 Extract 256 bits (composed of 4 packed 64-bit integers) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_extracti64x4_epi64_dbg(__mmask8 k, __m512i a, int imm8)
{
  int64_t tmp_vec[4];
  __m256d a_vec[2];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_pd((double*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm256_storeu_pd((double*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_extracti64x4_epi64
#define _mm512_maskz_extracti64x4_epi64 _mm512_maskz_extracti64x4_epi64_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512d _mm512_maskz_fmadd_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmadd_pd
#define _mm512_maskz_fmadd_pd _mm512_maskz_fmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_fmadd_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmadd_round_pd
#define _mm512_maskz_fmadd_round_pd _mm512_maskz_fmadd_round_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512 _mm512_maskz_fmadd_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmadd_ps
#define _mm512_maskz_fmadd_ps _mm512_maskz_fmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "a" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_fmadd_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmadd_round_ps
#define _mm512_maskz_fmadd_round_ps _mm512_maskz_fmadd_round_ps_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_fmadd_round_sd_dbg(__m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_fmadd_round_sd
#define _mm_fmadd_round_sd _mm_fmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask3_fmadd_round_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmadd_round_sd
#define _mm_mask3_fmadd_round_sd _mm_mask3_fmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
	
*/
static inline __m128d _mm_mask3_fmadd_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmadd_sd
#define _mm_mask3_fmadd_sd _mm_mask3_fmadd_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask_fmadd_round_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmadd_round_sd
#define _mm_mask_fmadd_round_sd _mm_mask_fmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	
*/
static inline __m128d _mm_mask_fmadd_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmadd_sd
#define _mm_mask_fmadd_sd _mm_mask_fmadd_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_maskz_fmadd_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmadd_round_sd
#define _mm_maskz_fmadd_round_sd _mm_maskz_fmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	
*/
static inline __m128d _mm_maskz_fmadd_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmadd_sd
#define _mm_maskz_fmadd_sd _mm_maskz_fmadd_sd_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask3_fmadd_round_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmadd_round_ss
#define _mm_mask3_fmadd_round_ss _mm_mask3_fmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to the upper elements of "dst".
	
*/
static inline __m128 _mm_mask3_fmadd_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmadd_ss
#define _mm_mask3_fmadd_ss _mm_mask3_fmadd_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_fmadd_round_ss_dbg(__m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_fmadd_round_ss
#define _mm_fmadd_round_ss _mm_fmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask_fmadd_round_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmadd_round_ss
#define _mm_mask_fmadd_round_ss _mm_mask_fmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	
*/
static inline __m128 _mm_mask_fmadd_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmadd_ss
#define _mm_mask_fmadd_ss _mm_mask_fmadd_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_maskz_fmadd_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmadd_round_ss
#define _mm_maskz_fmadd_round_ss _mm_maskz_fmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	
*/
static inline __m128 _mm_maskz_fmadd_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmadd_ss
#define _mm_maskz_fmadd_ss _mm_maskz_fmadd_ss_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", al/ernatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst". 
	
*/
static inline __m512d _mm512_fmaddsub_pd_dbg(__m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_fmaddsub_pd
#define _mm512_fmaddsub_pd _mm512_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_fmaddsub_round_pd_dbg(__m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_fmaddsub_round_pd
#define _mm512_fmaddsub_round_pd _mm512_fmaddsub_round_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m512d _mm512_mask3_fmaddsub_pd_dbg(__m512d a, __m512d b, __m512d c, __mmask8 k)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask3_fmaddsub_pd
#define _mm512_mask3_fmaddsub_pd _mm512_mask3_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_mask3_fmaddsub_round_pd_dbg(__m512d a, __m512d b, __m512d c, __mmask8 k, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask3_fmaddsub_round_pd
#define _mm512_mask3_fmaddsub_round_pd _mm512_mask3_fmaddsub_round_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_fmaddsub_pd_dbg(__m512d a, __mmask8 k, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_fmaddsub_pd
#define _mm512_mask_fmaddsub_pd _mm512_mask_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_mask_fmaddsub_round_pd_dbg(__m512d a, __mmask8 k, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_fmaddsub_round_pd
#define _mm512_mask_fmaddsub_round_pd _mm512_mask_fmaddsub_round_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512d _mm512_maskz_fmaddsub_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmaddsub_pd
#define _mm512_maskz_fmaddsub_pd _mm512_maskz_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_fmaddsub_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmaddsub_round_pd
#define _mm512_maskz_fmaddsub_round_pd _mm512_maskz_fmaddsub_round_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst". 
	
*/
static inline __m512 _mm512_fmaddsub_ps_dbg(__m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_fmaddsub_ps
#define _mm512_fmaddsub_ps _mm512_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_fmaddsub_round_ps_dbg(__m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_fmaddsub_round_ps
#define _mm512_fmaddsub_round_ps _mm512_fmaddsub_round_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m512 _mm512_mask3_fmaddsub_ps_dbg(__m512 a, __m512 b, __m512 c, __mmask16 k)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask3_fmaddsub_ps
#define _mm512_mask3_fmaddsub_ps _mm512_mask3_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask3_fmaddsub_round_ps_dbg(__m512 a, __m512 b, __m512 c, __mmask16 k, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask3_fmaddsub_round_ps
#define _mm512_mask3_fmaddsub_round_ps _mm512_mask3_fmaddsub_round_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_fmaddsub_ps_dbg(__m512 a, __mmask16 k, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_fmaddsub_ps
#define _mm512_mask_fmaddsub_ps _mm512_mask_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask_fmaddsub_round_ps_dbg(__m512 a, __mmask16 k, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_fmaddsub_round_ps
#define _mm512_mask_fmaddsub_round_ps _mm512_mask_fmaddsub_round_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512 _mm512_maskz_fmaddsub_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmaddsub_ps
#define _mm512_maskz_fmaddsub_ps _mm512_maskz_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_fmaddsub_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmaddsub_round_ps
#define _mm512_maskz_fmaddsub_round_ps _mm512_maskz_fmaddsub_round_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512d _mm512_maskz_fmsub_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmsub_pd
#define _mm512_maskz_fmsub_pd _mm512_maskz_fmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_fmsub_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmsub_round_pd
#define _mm512_maskz_fmsub_round_pd _mm512_maskz_fmsub_round_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512 _mm512_maskz_fmsub_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmsub_ps
#define _mm512_maskz_fmsub_ps _mm512_maskz_fmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_fmsub_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmsub_round_ps
#define _mm512_maskz_fmsub_round_ps _mm512_maskz_fmsub_round_ps_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_fmsub_round_sd_dbg(__m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_fmsub_round_sd
#define _mm_fmsub_round_sd _mm_fmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask3_fmsub_round_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmsub_round_sd
#define _mm_mask3_fmsub_round_sd _mm_mask3_fmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
*/
static inline __m128d _mm_mask3_fmsub_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmsub_sd
#define _mm_mask3_fmsub_sd _mm_mask3_fmsub_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask_fmsub_round_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmsub_round_sd
#define _mm_mask_fmsub_round_sd _mm_mask_fmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_fmsub_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmsub_sd
#define _mm_mask_fmsub_sd _mm_mask_fmsub_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_maskz_fmsub_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmsub_round_sd
#define _mm_maskz_fmsub_round_sd _mm_maskz_fmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_fmsub_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmsub_sd
#define _mm_maskz_fmsub_sd _mm_maskz_fmsub_sd_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_fmsub_round_ss_dbg(__m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_fmsub_round_ss
#define _mm_fmsub_round_ss _mm_fmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask3_fmsub_round_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmsub_round_ss
#define _mm_mask3_fmsub_round_ss _mm_mask3_fmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to the upper elements of "dst".
	
*/
static inline __m128 _mm_mask3_fmsub_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmsub_ss
#define _mm_mask3_fmsub_ss _mm_mask3_fmsub_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask_fmsub_round_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmsub_round_ss
#define _mm_mask_fmsub_round_ss _mm_mask_fmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_fmsub_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmsub_ss
#define _mm_mask_fmsub_ss _mm_mask_fmsub_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_maskz_fmsub_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmsub_round_ss
#define _mm_maskz_fmsub_round_ss _mm_maskz_fmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the intermediate result. Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_fmsub_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmsub_ss
#define _mm_maskz_fmsub_ss _mm_maskz_fmsub_ss_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst". 
	
*/
static inline __m512d _mm512_fmsubadd_pd_dbg(__m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_fmsubadd_pd
#define _mm512_fmsubadd_pd _mm512_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_fmsubadd_round_pd_dbg(__m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_fmsubadd_round_pd
#define _mm512_fmsubadd_round_pd _mm512_fmsubadd_round_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m512d _mm512_mask3_fmsubadd_pd_dbg(__m512d a, __m512d b, __m512d c, __mmask8 k)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask3_fmsubadd_pd
#define _mm512_mask3_fmsubadd_pd _mm512_mask3_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_mask3_fmsubadd_round_pd_dbg(__m512d a, __m512d b, __m512d c, __mmask8 k, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask3_fmsubadd_round_pd
#define _mm512_mask3_fmsubadd_round_pd _mm512_mask3_fmsubadd_round_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_fmsubadd_pd_dbg(__m512d a, __mmask8 k, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_fmsubadd_pd
#define _mm512_mask_fmsubadd_pd _mm512_mask_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_mask_fmsubadd_round_pd_dbg(__m512d a, __mmask8 k, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_fmsubadd_round_pd
#define _mm512_mask_fmsubadd_round_pd _mm512_mask_fmsubadd_round_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512d _mm512_maskz_fmsubadd_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmsubadd_pd
#define _mm512_maskz_fmsubadd_pd _mm512_maskz_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_fmsubadd_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fmsubadd_round_pd
#define _mm512_maskz_fmsubadd_round_pd _mm512_maskz_fmsubadd_round_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst". 
	
*/
static inline __m512 _mm512_fmsubadd_ps_dbg(__m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_fmsubadd_ps
#define _mm512_fmsubadd_ps _mm512_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_fmsubadd_round_ps_dbg(__m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_fmsubadd_round_ps
#define _mm512_fmsubadd_round_ps _mm512_fmsubadd_round_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m512 _mm512_mask3_fmsubadd_ps_dbg(__m512 a, __m512 b, __m512 c, __mmask16 k)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask3_fmsubadd_ps
#define _mm512_mask3_fmsubadd_ps _mm512_mask3_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask3_fmsubadd_round_ps_dbg(__m512 a, __m512 b, __m512 c, __mmask16 k, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask3_fmsubadd_round_ps
#define _mm512_mask3_fmsubadd_round_ps _mm512_mask3_fmsubadd_round_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_fmsubadd_ps_dbg(__m512 a, __mmask16 k, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_fmsubadd_ps
#define _mm512_mask_fmsubadd_ps _mm512_mask_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask_fmsubadd_round_ps_dbg(__m512 a, __mmask16 k, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_fmsubadd_round_ps
#define _mm512_mask_fmsubadd_round_ps _mm512_mask_fmsubadd_round_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m512 _mm512_maskz_fmsubadd_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmsubadd_ps
#define _mm512_maskz_fmsubadd_ps _mm512_maskz_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_fmsubadd_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fmsubadd_round_ps
#define _mm512_maskz_fmsubadd_round_ps _mm512_maskz_fmsubadd_round_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_maskz_fnmadd_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fnmadd_pd
#define _mm512_maskz_fnmadd_pd _mm512_maskz_fnmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_fnmadd_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fnmadd_round_pd
#define _mm512_maskz_fnmadd_round_pd _mm512_maskz_fnmadd_round_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_maskz_fnmadd_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fnmadd_ps
#define _mm512_maskz_fnmadd_ps _mm512_maskz_fnmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_fnmadd_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fnmadd_round_ps
#define _mm512_maskz_fnmadd_round_ps _mm512_maskz_fnmadd_round_ps_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_fnmadd_round_sd_dbg(__m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_fnmadd_round_sd
#define _mm_fnmadd_round_sd _mm_fnmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask3_fnmadd_round_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fnmadd_round_sd
#define _mm_mask3_fnmadd_round_sd _mm_mask3_fnmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
*/
static inline __m128d _mm_mask3_fnmadd_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fnmadd_sd
#define _mm_mask3_fnmadd_sd _mm_mask3_fnmadd_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask_fnmadd_round_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fnmadd_round_sd
#define _mm_mask_fnmadd_round_sd _mm_mask_fnmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_fnmadd_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fnmadd_sd
#define _mm_mask_fnmadd_sd _mm_mask_fnmadd_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_maskz_fnmadd_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fnmadd_round_sd
#define _mm_maskz_fnmadd_round_sd _mm_maskz_fnmadd_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_fnmadd_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fnmadd_sd
#define _mm_maskz_fnmadd_sd _mm_maskz_fnmadd_sd_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_fnmadd_round_ss_dbg(__m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_fnmadd_round_ss
#define _mm_fnmadd_round_ss _mm_fnmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask3_fnmadd_round_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fnmadd_round_ss
#define _mm_mask3_fnmadd_round_ss _mm_mask3_fnmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "c" to the upper elements of "dst".
*/
static inline __m128 _mm_mask3_fnmadd_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fnmadd_ss
#define _mm_mask3_fnmadd_ss _mm_mask3_fnmadd_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask_fnmadd_round_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fnmadd_round_ss
#define _mm_mask_fnmadd_round_ss _mm_mask_fnmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using writemask "k" (the element is copied from "a" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_fnmadd_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fnmadd_ss
#define _mm_mask_fnmadd_ss _mm_mask_fnmadd_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_maskz_fnmadd_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fnmadd_round_ss
#define _mm_maskz_fnmadd_round_ss _mm_maskz_fnmadd_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and add the negated intermediate result to the lower element in "c". Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_fnmadd_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) + c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fnmadd_ss
#define _mm_maskz_fnmadd_ss _mm_maskz_fnmadd_ss_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_maskz_fnmsub_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fnmsub_pd
#define _mm512_maskz_fnmsub_pd _mm512_maskz_fnmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_fnmsub_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, __m512d c, const int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double c_vec[8];
  _mm512_storeu_pd((void*)c_vec, c);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_fnmsub_round_pd
#define _mm512_maskz_fnmsub_round_pd _mm512_maskz_fnmsub_round_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_fnmsub_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fnmsub_ps
#define _mm512_maskz_fnmsub_ps _mm512_maskz_fnmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_maskz_fnmsub_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, __m512 c, const int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_fnmsub_round_ps
#define _mm512_maskz_fnmsub_round_ps _mm512_maskz_fnmsub_round_ps_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_fnmsub_round_sd_dbg(__m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_fnmsub_round_sd
#define _mm_fnmsub_round_sd _mm_fnmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask3_fnmsub_round_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fnmsub_round_sd
#define _mm_mask3_fnmsub_round_sd _mm_mask3_fnmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "c" to the upper element of "dst".
*/
static inline __m128d _mm_mask3_fnmsub_sd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fnmsub_sd
#define _mm_mask3_fnmsub_sd _mm_mask3_fnmsub_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask_fnmsub_round_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fnmsub_round_sd
#define _mm_mask_fnmsub_round_sd _mm_mask_fnmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_fnmsub_sd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fnmsub_sd
#define _mm_mask_fnmsub_sd _mm_mask_fnmsub_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_maskz_fnmsub_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fnmsub_round_sd
#define _mm_maskz_fnmsub_round_sd _mm_maskz_fnmsub_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_fnmsub_sd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fnmsub_sd
#define _mm_maskz_fnmsub_sd _mm_maskz_fnmsub_sd_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", subtract the lower element in "c" from the negated intermediate result, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_fnmsub_round_ss_dbg(__m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_fnmsub_round_ss
#define _mm_fnmsub_round_ss _mm_fnmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", subtract the lower element in "c" from the negated intermediate result, store the result in the lower element of "dst", and copy the upper element from "c" to the upper element of "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask3_fnmsub_round_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fnmsub_round_ss
#define _mm_mask3_fnmsub_round_ss _mm_mask3_fnmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst", and copy the upper element from "c" to the upper element of "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).
*/
static inline __m128 _mm_mask3_fnmsub_ss_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = c_vec[0];
  }
  dst_vec[1] = c_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fnmsub_ss
#define _mm_mask3_fnmsub_ss _mm_mask3_fnmsub_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask_fnmsub_round_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fnmsub_round_ss
#define _mm_mask_fnmsub_round_ss _mm_mask_fnmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using writemask "k" (the element is copied from "c" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_fnmsub_ss_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = a_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fnmsub_ss
#define _mm_mask_fnmsub_ss _mm_mask_fnmsub_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_maskz_fnmsub_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fnmsub_round_ss
#define _mm_maskz_fnmsub_round_ss _mm_maskz_fnmsub_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point elements in "a" and "b", and subtract the lower element in "c" from the negated intermediate result. Store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_fnmsub_ss_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = -(a_vec[0] * b_vec[0]) - c_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fnmsub_ss
#define _mm_maskz_fnmsub_ss _mm_maskz_fnmsub_ss_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m512d _mm512_maskz_getexp_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_getexp_pd
#define _mm512_maskz_getexp_pd _mm512_maskz_getexp_pd_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_maskz_getexp_round_pd_dbg(__mmask8 k, __m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_getexp_round_pd
#define _mm512_maskz_getexp_round_pd _mm512_maskz_getexp_round_pd_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m512 _mm512_maskz_getexp_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ConvertExpFP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_getexp_ps
#define _mm512_maskz_getexp_ps _mm512_maskz_getexp_ps_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512 _mm512_maskz_getexp_round_ps_dbg(__mmask16 k, __m512 a, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ConvertExpFP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_getexp_round_ps
#define _mm512_maskz_getexp_round_ps _mm512_maskz_getexp_round_ps_dbg


/*
 Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_getexp_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = ConvertExpFP64_rounding(b_vec[0], rounding);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_getexp_round_sd
#define _mm_getexp_round_sd _mm_getexp_round_sd_dbg


/*
 Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
*/
static inline __m128d _mm_getexp_sd_dbg(__m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = ConvertExpFP64(b_vec[0]);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_getexp_sd
#define _mm_getexp_sd _mm_getexp_sd_dbg


/*
 Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_mask_getexp_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP64_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_getexp_round_sd
#define _mm_mask_getexp_round_sd _mm_mask_getexp_round_sd_dbg


/*
 Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
*/
static inline __m128d _mm_mask_getexp_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP64(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_getexp_sd
#define _mm_mask_getexp_sd _mm_mask_getexp_sd_dbg


/*
 Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_maskz_getexp_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP64_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_getexp_round_sd
#define _mm_maskz_getexp_round_sd _mm_maskz_getexp_round_sd_dbg


/*
 Convert the exponent of the lower double-precision (64-bit) floating-point element in "b" to a double-precision (64-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
*/
static inline __m128d _mm_maskz_getexp_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP64(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_getexp_sd
#define _mm_maskz_getexp_sd _mm_maskz_getexp_sd_dbg


/*
 Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_getexp_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = ConvertExpFP32_rounding(b_vec[0], rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_getexp_round_ss
#define _mm_getexp_round_ss _mm_getexp_round_ss_dbg


/*
 Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
*/
static inline __m128 _mm_getexp_ss_dbg(__m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = ConvertExpFP32(b_vec[0]);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_getexp_ss
#define _mm_getexp_ss _mm_getexp_ss_dbg


/*
 Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_mask_getexp_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP32_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_getexp_round_ss
#define _mm_mask_getexp_round_ss _mm_mask_getexp_round_ss_dbg


/*
 Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
*/
static inline __m128 _mm_mask_getexp_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP32(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_getexp_ss
#define _mm_mask_getexp_ss _mm_mask_getexp_ss_dbg


/*
 Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_maskz_getexp_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP32_rounding(b_vec[0], rounding);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_getexp_round_ss
#define _mm_maskz_getexp_round_ss _mm_maskz_getexp_round_ss_dbg


/*
 Convert the exponent of the lower single-precision (32-bit) floating-point element in "b" to a single-precision (32-bit) floating-point number representing the integer exponent, store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". This intrinsic essentially calculates "floor(log2(x))" for the lower element.
*/
static inline __m128 _mm_maskz_getexp_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ConvertExpFP32(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_getexp_ss
#define _mm_maskz_getexp_ss _mm_maskz_getexp_ss_dbg

/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_max_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_max_pd
#define _mm512_mask_max_pd _mm512_mask_max_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512d _mm512_mask_max_round_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b, int sae)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_max_round_pd
#define _mm512_mask_max_round_pd _mm512_mask_max_round_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_max_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_max_pd
#define _mm512_maskz_max_pd _mm512_maskz_max_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512d _mm512_maskz_max_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_max_round_pd
#define _mm512_maskz_max_round_pd _mm512_maskz_max_round_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m512d _mm512_max_pd_dbg(__m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = MAX(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_max_pd
#define _mm512_max_pd _mm512_max_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512d _mm512_max_round_pd_dbg(__m512d a, __m512d b, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = MAX(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_max_round_pd
#define _mm512_max_round_pd _mm512_max_round_pd_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_max_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_max_ps
#define _mm512_mask_max_ps _mm512_mask_max_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512 _mm512_mask_max_round_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b, int sae)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_max_round_ps
#define _mm512_mask_max_round_ps _mm512_mask_max_round_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_max_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_max_ps
#define _mm512_maskz_max_ps _mm512_maskz_max_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512 _mm512_maskz_max_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_max_round_ps
#define _mm512_maskz_max_round_ps _mm512_maskz_max_round_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m512 _mm512_max_ps_dbg(__m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = MAX(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_max_ps
#define _mm512_max_ps _mm512_max_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512 _mm512_max_round_ps_dbg(__m512 a, __m512 b, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = MAX(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_max_round_ps
#define _mm512_max_round_ps _mm512_max_round_ps_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128d _mm_mask_max_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int sae)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_max_round_sd
#define _mm_mask_max_round_sd _mm_mask_max_round_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_max_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_max_sd
#define _mm_mask_max_sd _mm_mask_max_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128d _mm_maskz_max_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int sae)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_max_round_sd
#define _mm_maskz_max_round_sd _mm_maskz_max_round_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_max_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_max_sd
#define _mm_maskz_max_sd _mm_maskz_max_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128d _mm_max_round_sd_dbg(__m128d a, __m128d b, int sae)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_max_round_sd
#define _mm_max_round_sd _mm_max_round_sd_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128 _mm_mask_max_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int sae)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_max_round_ss
#define _mm_mask_max_round_ss _mm_mask_max_round_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128 _mm_mask_max_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_max_ss
#define _mm_mask_max_ss _mm_mask_max_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128 _mm_maskz_max_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int sae)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_max_round_ss
#define _mm_maskz_max_round_ss _mm_maskz_max_round_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128 _mm_maskz_max_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_max_ss
#define _mm_maskz_max_ss _mm_maskz_max_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the maximum value in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128 _mm_max_round_ss_dbg(__m128 a, __m128 b, int sae)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = MAX(a_vec[0], b_vec[0]);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_max_round_ss
#define _mm_max_round_ss _mm_max_round_ss_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_min_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_min_pd
#define _mm512_mask_min_pd _mm512_mask_min_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512d _mm512_mask_min_round_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b, int sae)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_min_round_pd
#define _mm512_mask_min_round_pd _mm512_mask_min_round_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_min_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_min_pd
#define _mm512_maskz_min_pd _mm512_maskz_min_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512d _mm512_maskz_min_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_min_round_pd
#define _mm512_maskz_min_round_pd _mm512_maskz_min_round_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m512d _mm512_min_pd_dbg(__m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = MIN(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_min_pd
#define _mm512_min_pd _mm512_min_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512d _mm512_min_round_pd_dbg(__m512d a, __m512d b, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = MIN(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_min_round_pd
#define _mm512_min_round_pd _mm512_min_round_pd_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_min_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_min_ps
#define _mm512_mask_min_ps _mm512_mask_min_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512 _mm512_mask_min_round_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b, int sae)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_min_round_ps
#define _mm512_mask_min_round_ps _mm512_mask_min_round_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_min_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_min_ps
#define _mm512_maskz_min_ps _mm512_maskz_min_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512 _mm512_maskz_min_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_min_round_ps
#define _mm512_maskz_min_round_ps _mm512_maskz_min_round_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m512 _mm512_min_ps_dbg(__m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = MIN(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_min_ps
#define _mm512_min_ps _mm512_min_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst". 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512 _mm512_min_round_ps_dbg(__m512 a, __m512 b, int sae)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = MIN(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_min_round_ps
#define _mm512_min_round_ps _mm512_min_round_ps_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128d _mm_mask_min_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int sae)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_min_round_sd
#define _mm_mask_min_round_sd _mm_mask_min_round_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_min_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_min_sd
#define _mm_mask_min_sd _mm_mask_min_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128d _mm_maskz_min_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int sae)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_min_round_sd
#define _mm_maskz_min_round_sd _mm_maskz_min_round_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_min_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_min_sd
#define _mm_maskz_min_sd _mm_maskz_min_sd_dbg


/*
 Compare the lower double-precision (64-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" , and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128d _mm_min_round_sd_dbg(__m128d a, __m128d b, int sae)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_min_round_sd
#define _mm_min_round_sd _mm_min_round_sd_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128 _mm_mask_min_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int sae)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_min_round_ss
#define _mm_mask_min_round_ss _mm_mask_min_round_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128 _mm_mask_min_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_min_ss
#define _mm_mask_min_ss _mm_mask_min_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128 _mm_maskz_min_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int sae)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_min_round_ss
#define _mm_maskz_min_round_ss _mm_maskz_min_round_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128 _mm_maskz_min_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_min_ss
#define _mm_maskz_min_ss _mm_maskz_min_ss_dbg


/*
 Compare the lower single-precision (32-bit) floating-point elements in "a" and "b", store the minimum value in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m128 _mm_min_round_ss_dbg(__m128 a, __m128 b, int sae)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = MIN(a_vec[0], b_vec[0]);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_min_round_ss
#define _mm_min_round_ss _mm_min_round_ss_dbg

/*
 Move packed double-precision (64-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_mov_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_mov_pd
#define _mm512_maskz_mov_pd _mm512_maskz_mov_pd_dbg


/*
 Move packed single-precision (32-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_mov_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_mov_ps
#define _mm512_maskz_mov_ps _mm512_maskz_mov_ps_dbg

/*
 Move packed 32-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_mov_epi32_dbg(__mmask16 k, __m512i a)
{
  float a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mov_epi32
#define _mm512_maskz_mov_epi32 _mm512_maskz_mov_epi32_dbg


/*
 Move packed 64-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_mov_epi64_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mov_epi64
#define _mm512_maskz_mov_epi64 _mm512_maskz_mov_epi64_dbg

/*
 Move the lower double-precision (64-bit) floating-point element from "b" to the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_move_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_move_sd
#define _mm_mask_move_sd _mm_mask_move_sd_dbg


/*
 Move the lower double-precision (64-bit) floating-point element from "b" to the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_move_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_move_sd
#define _mm_maskz_move_sd _mm_maskz_move_sd_dbg


/*
 Move the lower single-precision (32-bit) floating-point element from "b" to the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_move_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_move_ss
#define _mm_mask_move_ss _mm_mask_move_ss_dbg


/*
 Move the lower single-precision (32-bit) floating-point element from "b" to the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_move_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_move_ss
#define _mm_maskz_move_ss _mm_maskz_move_ss_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_mul_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_mul_pd
#define _mm512_maskz_mul_pd _mm512_maskz_mul_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_mul_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_mul_round_pd
#define _mm512_maskz_mul_round_pd _mm512_maskz_mul_round_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_mul_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_mul_ps
#define _mm512_maskz_mul_ps _mm512_maskz_mul_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_mul_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_mul_round_ps
#define _mm512_maskz_mul_round_ps _mm512_maskz_mul_round_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m512 _mm512_mul_ps_dbg(__m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] * b_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mul_ps
#define _mm512_mul_ps _mm512_mul_ps_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_mask_mul_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_mul_round_sd
#define _mm_mask_mul_round_sd _mm_mask_mul_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_mul_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_mul_sd
#define _mm_mask_mul_sd _mm_mask_mul_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_maskz_mul_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_mul_round_sd
#define _mm_maskz_mul_round_sd _mm_maskz_mul_round_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_mul_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_mul_sd
#define _mm_maskz_mul_sd _mm_maskz_mul_sd_dbg


/*
 Multiply the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128d _mm_mul_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = a_vec[0] * b_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mul_round_sd
#define _mm_mul_round_sd _mm_mul_round_sd_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_mask_mul_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_mul_round_ss
#define _mm_mask_mul_round_ss _mm_mask_mul_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_mul_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_mul_ss
#define _mm_mask_mul_ss _mm_mask_mul_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_maskz_mul_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_mul_round_ss
#define _mm_maskz_mul_round_ss _mm_maskz_mul_round_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_mul_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] * b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_mul_ss
#define _mm_maskz_mul_ss _mm_maskz_mul_ss_dbg


/*
 Multiply the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
		(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
		
*/
static inline __m128 _mm_mul_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = a_vec[0] * b_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mul_round_ss
#define _mm_mul_round_ss _mm_mul_round_ss_dbg


/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m512i _mm512_abs_epi32_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = abs(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_abs_epi32
#define _mm512_abs_epi32 _mm512_abs_epi32_dbg

/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_abs_epi32_dbg(__m512i src, __mmask16 k, __m512i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_abs_epi32
#define _mm512_mask_abs_epi32 _mm512_mask_abs_epi32_dbg

/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_abs_epi32_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_abs_epi32
#define _mm512_maskz_abs_epi32 _mm512_maskz_abs_epi32_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m512i _mm512_abs_epi64_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = llabs(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_abs_epi64
#define _mm512_abs_epi64 _mm512_abs_epi64_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_abs_epi64_dbg(__m512i src, __mmask8 k, __m512i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = llabs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_abs_epi64
#define _mm512_mask_abs_epi64 _mm512_mask_abs_epi64_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_abs_epi64_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = llabs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_abs_epi64
#define _mm512_maskz_abs_epi64 _mm512_maskz_abs_epi64_dbg

/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_add_epi32_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_add_epi32
#define _mm512_add_epi32 _mm512_add_epi32_dbg


/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_add_epi32_dbg(__m512i src, __mmask16 k, __m512i a, __m512i b)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_add_epi32
#define _mm512_mask_add_epi32 _mm512_mask_add_epi32_dbg


/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_add_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_add_epi32
#define _mm512_maskz_add_epi32 _mm512_maskz_add_epi32_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_add_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_add_epi64
#define _mm512_add_epi64 _mm512_add_epi64_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_add_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_add_epi64
#define _mm512_mask_add_epi64 _mm512_mask_add_epi64_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_add_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_add_epi64
#define _mm512_maskz_add_epi64 _mm512_maskz_add_epi64_dbg


/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_and_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_and_epi32
#define _mm512_maskz_and_epi32 _mm512_maskz_and_epi32_dbg


/*
 Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_andnot_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (~ a_vec[j]) & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_andnot_epi32
#define _mm512_maskz_andnot_epi32 _mm512_maskz_andnot_epi32_dbg


/*
 Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_andnot_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (~ a_vec[j]) & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_andnot_epi64
#define _mm512_maskz_andnot_epi64 _mm512_maskz_andnot_epi64_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_and_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_and_epi64
#define _mm512_maskz_and_epi64 _mm512_maskz_and_epi64_dbg


/*
 Broadcast 8-bit integer "a" to all elements of "dst".
*/
static inline __m512i _mm512_set1_epi8_dbg(char a)
{
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = a;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set1_epi8
#define _mm512_set1_epi8 _mm512_set1_epi8_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst".
*/
static inline __m512i _mm512_broadcastd_epi32_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcastd_epi32
#define _mm512_broadcastd_epi32 _mm512_broadcastd_epi32_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcastd_epi32_dbg(__m512i src, __mmask16 k, __m128i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcastd_epi32
#define _mm512_mask_broadcastd_epi32 _mm512_mask_broadcastd_epi32_dbg


/*
 Broadcast 32-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_set1_epi32_dbg(__m512i src, __mmask16 k, int a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_set1_epi32
#define _mm512_mask_set1_epi32 _mm512_mask_set1_epi32_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_broadcastd_epi32_dbg(__mmask16 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcastd_epi32
#define _mm512_maskz_broadcastd_epi32 _mm512_maskz_broadcastd_epi32_dbg


/*
 Broadcast 32-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_set1_epi32_dbg(__mmask16 k, int a)
{
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_set1_epi32
#define _mm512_maskz_set1_epi32 _mm512_maskz_set1_epi32_dbg

/*
 Broadcast the low 8-bits from input mask "k" to all 64-bit elements of "dst".
*/
static inline __m512i _mm512_broadcastmb_epi64_dbg(__mmask8 k)
{
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)k);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcastmb_epi64
#define _mm512_broadcastmb_epi64 _mm512_broadcastmb_epi64_dbg


/*
 Broadcast the low 16-bits from input mask "k" to all 32-bit elements of "dst".
*/
static inline __m512i _mm512_broadcastmw_epi32_dbg(__mmask16 k)
{
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)k);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcastmw_epi32
#define _mm512_broadcastmw_epi32 _mm512_broadcastmw_epi32_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst". 
*/
static inline __m512i _mm512_broadcastq_epi64_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcastq_epi64
#define _mm512_broadcastq_epi64 _mm512_broadcastq_epi64_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcastq_epi64_dbg(__m512i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcastq_epi64
#define _mm512_mask_broadcastq_epi64 _mm512_mask_broadcastq_epi64_dbg


/*
 Broadcast 64-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_set1_epi64_dbg(__m512i src, __mmask8 k, int64_t a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_set1_epi64
#define _mm512_mask_set1_epi64 _mm512_mask_set1_epi64_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_broadcastq_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcastq_epi64
#define _mm512_maskz_broadcastq_epi64 _mm512_maskz_broadcastq_epi64_dbg


/*
 Broadcast 64-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_set1_epi64_dbg(__mmask8 k, int64_t a)
{
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_set1_epi64
#define _mm512_maskz_set1_epi64 _mm512_maskz_set1_epi64_dbg


/*
 Broadcast 64-bit integer "a" to all elements of "dst".
*/
static inline __m512i _mm512_set1_epi64_dbg(int64_t a)
{
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set1_epi64
#define _mm512_set1_epi64 _mm512_set1_epi64_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all all elements of "dst".
	
*/
static inline __m512i _mm512_set1_epi16_dbg(short a)
{
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set1_epi16
#define _mm512_set1_epi16 _mm512_set1_epi16_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm512_cmple_epi32_mask_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}
#undef _mm512_cmple_epi32_mask
#define _mm512_cmple_epi32_mask _mm512_cmple_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm512_cmplt_epi32_mask_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmplt_epi32_mask
#define _mm512_cmplt_epi32_mask _mm512_cmplt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm512_mask_cmplt_epi32_mask_dbg(__mmask16 k1, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmplt_epi32_mask
#define _mm512_mask_cmplt_epi32_mask _mm512_mask_cmplt_epi32_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpeq_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_epi64_mask
#define _mm512_cmpeq_epi64_mask _mm512_cmpeq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpge_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpge_epi64_mask
#define _mm512_cmpge_epi64_mask _mm512_cmpge_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpgt_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpgt_epi64_mask
#define _mm512_cmpgt_epi64_mask _mm512_cmpgt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmple_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmple_epi64_mask
#define _mm512_cmple_epi64_mask _mm512_cmple_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmplt_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmplt_epi64_mask
#define _mm512_cmplt_epi64_mask _mm512_cmplt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpneq_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpneq_epi64_mask
#define _mm512_cmpneq_epi64_mask _mm512_cmpneq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpeq_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpeq_epi64_mask
#define _mm512_mask_cmpeq_epi64_mask _mm512_mask_cmpeq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpge_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpge_epi64_mask
#define _mm512_mask_cmpge_epi64_mask _mm512_mask_cmpge_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpgt_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpgt_epi64_mask
#define _mm512_mask_cmpgt_epi64_mask _mm512_mask_cmpgt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmple_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmple_epi64_mask
#define _mm512_mask_cmple_epi64_mask _mm512_mask_cmple_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmplt_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmplt_epi64_mask
#define _mm512_mask_cmplt_epi64_mask _mm512_mask_cmplt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpneq_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpneq_epi64_mask
#define _mm512_mask_cmpneq_epi64_mask _mm512_mask_cmpneq_epi64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpeq_epu64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_epu64_mask
#define _mm512_cmpeq_epu64_mask _mm512_cmpeq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpge_epu64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpge_epu64_mask
#define _mm512_cmpge_epu64_mask _mm512_cmpge_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpgt_epu64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpgt_epu64_mask
#define _mm512_cmpgt_epu64_mask _mm512_cmpgt_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmple_epu64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmple_epu64_mask
#define _mm512_cmple_epu64_mask _mm512_cmple_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmplt_epu64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmplt_epu64_mask
#define _mm512_cmplt_epu64_mask _mm512_cmplt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpneq_epu64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpneq_epu64_mask
#define _mm512_cmpneq_epu64_mask _mm512_cmpneq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpeq_epu64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpeq_epu64_mask
#define _mm512_mask_cmpeq_epu64_mask _mm512_mask_cmpeq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpge_epu64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpge_epu64_mask
#define _mm512_mask_cmpge_epu64_mask _mm512_mask_cmpge_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpgt_epu64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpgt_epu64_mask
#define _mm512_mask_cmpgt_epu64_mask _mm512_mask_cmpgt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmple_epu64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmple_epu64_mask
#define _mm512_mask_cmple_epu64_mask _mm512_mask_cmple_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmplt_epu64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmplt_epu64_mask
#define _mm512_mask_cmplt_epu64_mask _mm512_mask_cmplt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm512_mask_cmpneq_epu64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpneq_epu64_mask
#define _mm512_mask_cmpneq_epu64_mask _mm512_mask_cmpneq_epu64_mask_dbg


/*
 Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_expand_epi32_dbg(__m512i src, __mmask16 k, __m512i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_expand_epi32
#define _mm512_mask_expand_epi32 _mm512_mask_expand_epi32_dbg


/*
 Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_expand_epi32_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_expand_epi32
#define _mm512_maskz_expand_epi32 _mm512_maskz_expand_epi32_dbg


/*
 Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_expand_epi64_dbg(__m512i src, __mmask8 k, __m512i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_expand_epi64
#define _mm512_mask_expand_epi64 _mm512_mask_expand_epi64_dbg


/*
 Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_expand_epi64_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[m];
      m = m + 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_expand_epi64
#define _mm512_maskz_expand_epi64 _mm512_maskz_expand_epi64_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_max_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epi32
#define _mm512_maskz_max_epi32 _mm512_maskz_max_epi32_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_max_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_max_epi64
#define _mm512_mask_max_epi64 _mm512_mask_max_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_max_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epi64
#define _mm512_maskz_max_epi64 _mm512_maskz_max_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m512i _mm512_max_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_max_epi64
#define _mm512_max_epi64 _mm512_max_epi64_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_max_epu32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epu32
#define _mm512_maskz_max_epu32 _mm512_maskz_max_epu32_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_max_epu64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_max_epu64
#define _mm512_mask_max_epu64 _mm512_mask_max_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_max_epu64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epu64
#define _mm512_maskz_max_epu64 _mm512_maskz_max_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst". 
*/
static inline __m512i _mm512_max_epu64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_max_epu64
#define _mm512_max_epu64 _mm512_max_epu64_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_min_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epi32
#define _mm512_maskz_min_epi32 _mm512_maskz_min_epi32_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_min_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_min_epi64
#define _mm512_mask_min_epi64 _mm512_mask_min_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_min_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epi64
#define _mm512_maskz_min_epi64 _mm512_maskz_min_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m512i _mm512_min_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_min_epi64
#define _mm512_min_epi64 _mm512_min_epi64_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_min_epu32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epu32
#define _mm512_maskz_min_epu32 _mm512_maskz_min_epu32_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_min_epu64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_min_epu64
#define _mm512_mask_min_epu64 _mm512_mask_min_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_min_epu64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epu64
#define _mm512_maskz_min_epu64 _mm512_maskz_min_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m512i _mm512_min_epu64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_min_epu64
#define _mm512_min_epu64 _mm512_min_epu64_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtepi32_epi8_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtepi32_epi8
#define _mm512_cvtepi32_epi8 _mm512_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtepi32_epi8_dbg(__m128i src, __mmask16 k, __m512i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtepi32_epi8
#define _mm512_mask_cvtepi32_epi8 _mm512_mask_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtepi32_epi8_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtepi32_epi8
#define _mm512_maskz_cvtepi32_epi8 _mm512_maskz_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm512_cvtepi32_epi16_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtepi32_epi16
#define _mm512_cvtepi32_epi16 _mm512_cvtepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtepi32_epi16_dbg(__m256i src, __mmask16 k, __m512i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtepi32_epi16
#define _mm512_mask_cvtepi32_epi16 _mm512_mask_cvtepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtepi32_epi16_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi32_epi16
#define _mm512_maskz_cvtepi32_epi16 _mm512_maskz_cvtepi32_epi16_dbg

/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtepi64_epi8_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtepi64_epi8
#define _mm512_cvtepi64_epi8 _mm512_cvtepi64_epi8_dbg

/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtepi64_epi8_dbg(__m128i src, __mmask8 k, __m512i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtepi64_epi8
#define _mm512_mask_cvtepi64_epi8 _mm512_mask_cvtepi64_epi8_dbg

/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtepi64_epi8_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtepi64_epi8
#define _mm512_maskz_cvtepi64_epi8 _mm512_maskz_cvtepi64_epi8_dbg

/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm512_cvtepi64_epi32_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtepi64_epi32
#define _mm512_cvtepi64_epi32 _mm512_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtepi64_epi32_dbg(__m256i src, __mmask8 k, __m512i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtepi64_epi32
#define _mm512_mask_cvtepi64_epi32 _mm512_mask_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtepi64_epi32_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi64_epi32
#define _mm512_maskz_cvtepi64_epi32 _mm512_maskz_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtepi64_epi16_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtepi64_epi16
#define _mm512_cvtepi64_epi16 _mm512_cvtepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtepi64_epi16_dbg(__m128i src, __mmask8 k, __m512i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtepi64_epi16
#define _mm512_mask_cvtepi64_epi16 _mm512_mask_cvtepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtepi64_epi16_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtepi64_epi16
#define _mm512_maskz_cvtepi64_epi16 _mm512_maskz_cvtepi64_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtsepi32_epi8_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtsepi32_epi8
#define _mm512_cvtsepi32_epi8 _mm512_cvtsepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtsepi32_epi8_dbg(__m128i src, __mmask16 k, __m512i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtsepi32_epi8
#define _mm512_mask_cvtsepi32_epi8 _mm512_mask_cvtsepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtsepi32_epi8_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtsepi32_epi8
#define _mm512_maskz_cvtsepi32_epi8 _mm512_maskz_cvtsepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m256i _mm512_cvtsepi32_epi16_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtsepi32_epi16
#define _mm512_cvtsepi32_epi16 _mm512_cvtsepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtsepi32_epi16_dbg(__m256i src, __mmask16 k, __m512i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtsepi32_epi16
#define _mm512_mask_cvtsepi32_epi16 _mm512_mask_cvtsepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtsepi32_epi16_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtsepi32_epi16
#define _mm512_maskz_cvtsepi32_epi16 _mm512_maskz_cvtsepi32_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtsepi64_epi8_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtsepi64_epi8
#define _mm512_cvtsepi64_epi8 _mm512_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtsepi64_epi8_dbg(__m128i src, __mmask8 k, __m512i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtsepi64_epi8
#define _mm512_mask_cvtsepi64_epi8 _mm512_mask_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtsepi64_epi8_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtsepi64_epi8
#define _mm512_maskz_cvtsepi64_epi8 _mm512_maskz_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m256i _mm512_cvtsepi64_epi32_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtsepi64_epi32
#define _mm512_cvtsepi64_epi32 _mm512_cvtsepi64_epi32_dbg

/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtsepi64_epi32_dbg(__m256i src, __mmask8 k, __m512i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtsepi64_epi32
#define _mm512_mask_cvtsepi64_epi32 _mm512_mask_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtsepi64_epi32_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtsepi64_epi32
#define _mm512_maskz_cvtsepi64_epi32 _mm512_maskz_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtsepi64_epi16_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtsepi64_epi16
#define _mm512_cvtsepi64_epi16 _mm512_cvtsepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtsepi64_epi16_dbg(__m128i src, __mmask8 k, __m512i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtsepi64_epi16
#define _mm512_mask_cvtsepi64_epi16 _mm512_mask_cvtsepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtsepi64_epi16_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtsepi64_epi16
#define _mm512_maskz_cvtsepi64_epi16 _mm512_maskz_cvtsepi64_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepi8_epi32_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepi8_epi32
#define _mm512_cvtepi8_epi32 _mm512_cvtepi8_epi32_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepi8_epi32_dbg(__m512i src, __mmask16 k, __m128i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepi8_epi32
#define _mm512_mask_cvtepi8_epi32 _mm512_mask_cvtepi8_epi32_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtepi8_epi32_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi8_epi32
#define _mm512_maskz_cvtepi8_epi32 _mm512_maskz_cvtepi8_epi32_dbg


/*
 Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepi8_epi64_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepi8_epi64
#define _mm512_cvtepi8_epi64 _mm512_cvtepi8_epi64_dbg


/*
 Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepi8_epi64_dbg(__m512i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepi8_epi64
#define _mm512_mask_cvtepi8_epi64 _mm512_mask_cvtepi8_epi64_dbg

/*
 Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtepi8_epi64_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi8_epi64
#define _mm512_maskz_cvtepi8_epi64 _mm512_maskz_cvtepi8_epi64_dbg

/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepi32_epi64_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepi32_epi64
#define _mm512_cvtepi32_epi64 _mm512_cvtepi32_epi64_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepi32_epi64_dbg(__m512i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepi32_epi64
#define _mm512_mask_cvtepi32_epi64 _mm512_mask_cvtepi32_epi64_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtepi32_epi64_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi32_epi64
#define _mm512_maskz_cvtepi32_epi64 _mm512_maskz_cvtepi32_epi64_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepi16_epi32_dbg(__m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepi16_epi32
#define _mm512_cvtepi16_epi32 _mm512_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepi16_epi32_dbg(__m512i src, __mmask16 k, __m256i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepi16_epi32
#define _mm512_mask_cvtepi16_epi32 _mm512_mask_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtepi16_epi32_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi16_epi32
#define _mm512_maskz_cvtepi16_epi32 _mm512_maskz_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepi16_epi64_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepi16_epi64
#define _mm512_cvtepi16_epi64 _mm512_cvtepi16_epi64_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepi16_epi64_dbg(__m512i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepi16_epi64
#define _mm512_mask_cvtepi16_epi64 _mm512_mask_cvtepi16_epi64_dbg

/*
 Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtepi16_epi64_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi16_epi64
#define _mm512_maskz_cvtepi16_epi64 _mm512_maskz_cvtepi16_epi64_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtusepi32_epi8_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtusepi32_epi8
#define _mm512_cvtusepi32_epi8 _mm512_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtusepi32_epi8_dbg(__m128i src, __mmask16 k, __m512i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtusepi32_epi8
#define _mm512_mask_cvtusepi32_epi8 _mm512_mask_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtusepi32_epi8_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtusepi32_epi8
#define _mm512_maskz_cvtusepi32_epi8 _mm512_maskz_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m256i _mm512_cvtusepi32_epi16_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtusepi32_epi16
#define _mm512_cvtusepi32_epi16 _mm512_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtusepi32_epi16_dbg(__m256i src, __mmask16 k, __m512i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtusepi32_epi16
#define _mm512_mask_cvtusepi32_epi16 _mm512_mask_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtusepi32_epi16_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtusepi32_epi16
#define _mm512_maskz_cvtusepi32_epi16 _mm512_maskz_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtusepi64_epi8_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtusepi64_epi8
#define _mm512_cvtusepi64_epi8 _mm512_cvtusepi64_epi8_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtusepi64_epi8_dbg(__m128i src, __mmask8 k, __m512i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtusepi64_epi8
#define _mm512_mask_cvtusepi64_epi8 _mm512_mask_cvtusepi64_epi8_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtusepi64_epi8_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtusepi64_epi8
#define _mm512_maskz_cvtusepi64_epi8 _mm512_maskz_cvtusepi64_epi8_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m256i _mm512_cvtusepi64_epi32_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtusepi64_epi32
#define _mm512_cvtusepi64_epi32 _mm512_cvtusepi64_epi32_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtusepi64_epi32_dbg(__m256i src, __mmask8 k, __m512i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtusepi64_epi32
#define _mm512_mask_cvtusepi64_epi32 _mm512_mask_cvtusepi64_epi32_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtusepi64_epi32_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtusepi64_epi32
#define _mm512_maskz_cvtusepi64_epi32 _mm512_maskz_cvtusepi64_epi32_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm512_cvtusepi64_epi16_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_cvtusepi64_epi16
#define _mm512_cvtusepi64_epi16 _mm512_cvtusepi64_epi16_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_cvtusepi64_epi16_dbg(__m128i src, __mmask8 k, __m512i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_cvtusepi64_epi16
#define _mm512_mask_cvtusepi64_epi16 _mm512_mask_cvtusepi64_epi16_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_cvtusepi64_epi16_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_cvtusepi64_epi16
#define _mm512_maskz_cvtusepi64_epi16 _mm512_maskz_cvtusepi64_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepu8_epi32_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepu8_epi32
#define _mm512_cvtepu8_epi32 _mm512_cvtepu8_epi32_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepu8_epi32_dbg(__m512i src, __mmask16 k, __m128i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepu8_epi32
#define _mm512_mask_cvtepu8_epi32 _mm512_mask_cvtepu8_epi32_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtepu8_epi32_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu8_epi32
#define _mm512_maskz_cvtepu8_epi32 _mm512_maskz_cvtepu8_epi32_dbg

/*
 Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepu8_epi64_dbg(__m128i a)
{
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepu8_epi64
#define _mm512_cvtepu8_epi64 _mm512_cvtepu8_epi64_dbg

/*
 Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepu8_epi64_dbg(__m512i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepu8_epi64
#define _mm512_mask_cvtepu8_epi64 _mm512_mask_cvtepu8_epi64_dbg

/*
 Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtepu8_epi64_dbg(__mmask8 k, __m128i a)
{
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu8_epi64
#define _mm512_maskz_cvtepu8_epi64 _mm512_maskz_cvtepu8_epi64_dbg

/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepu32_epi64_dbg(__m256i a)
{
  uint32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepu32_epi64
#define _mm512_cvtepu32_epi64 _mm512_cvtepu32_epi64_dbg

/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepu32_epi64_dbg(__m512i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  uint32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepu32_epi64
#define _mm512_mask_cvtepu32_epi64 _mm512_mask_cvtepu32_epi64_dbg

/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtepu32_epi64_dbg(__mmask8 k, __m256i a)
{
  uint32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu32_epi64
#define _mm512_maskz_cvtepu32_epi64 _mm512_maskz_cvtepu32_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepu16_epi32_dbg(__m256i a)
{
  uint16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepu16_epi32
#define _mm512_cvtepu16_epi32 _mm512_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepu16_epi32_dbg(__m512i src, __mmask16 k, __m256i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  uint16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepu16_epi32
#define _mm512_mask_cvtepu16_epi32 _mm512_mask_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtepu16_epi32_dbg(__mmask16 k, __m256i a)
{
  uint16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu16_epi32
#define _mm512_maskz_cvtepu16_epi32 _mm512_maskz_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepu16_epi64_dbg(__m128i a)
{
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepu16_epi64
#define _mm512_cvtepu16_epi64 _mm512_cvtepu16_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepu16_epi64_dbg(__m512i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepu16_epi64
#define _mm512_mask_cvtepu16_epi64 _mm512_mask_cvtepu16_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtepu16_epi64_dbg(__mmask8 k, __m128i a)
{
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu16_epi64
#define _mm512_maskz_cvtepu16_epi64 _mm512_maskz_cvtepu16_epi64_dbg

/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mul_epi32_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mul_epi32
#define _mm512_mask_mul_epi32 _mm512_mask_mul_epi32_dbg


/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mul_epi32_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mul_epi32
#define _mm512_maskz_mul_epi32 _mm512_maskz_mul_epi32_dbg


/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst". 
*/
static inline __m512i _mm512_mul_epi32_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j*2] * b_vec[j*2];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mul_epi32
#define _mm512_mul_epi32 _mm512_mul_epi32_dbg


/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mul_epu32_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  uint64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  uint32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  uint32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mul_epu32
#define _mm512_mask_mul_epu32 _mm512_mask_mul_epu32_dbg


/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mul_epu32_dbg(__mmask8 k, __m512i a, __m512i b)
{
  uint32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  uint32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mul_epu32
#define _mm512_maskz_mul_epu32 _mm512_maskz_mul_epu32_dbg


/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst". 
*/
static inline __m512i _mm512_mul_epu32_dbg(__m512i a, __m512i b)
{
  uint32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  uint32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j*2] * b_vec[j*2];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mul_epu32
#define _mm512_mul_epu32 _mm512_mul_epu32_dbg


/*
 Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_or_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_or_epi32
#define _mm512_maskz_or_epi32 _mm512_maskz_or_epi32_dbg


/*
 Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_or_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_or_epi64
#define _mm512_maskz_or_epi64 _mm512_maskz_or_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_rol_epi32_dbg(__m512i src, __mmask16 k, __m512i a, const int imm8)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_rol_epi32
#define _mm512_mask_rol_epi32 _mm512_mask_rol_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_rol_epi32_dbg(__mmask16 k, __m512i a, const int imm8)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_rol_epi32
#define _mm512_maskz_rol_epi32 _mm512_maskz_rol_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m512i _mm512_rol_epi32_dbg(__m512i a, const int imm8)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_rol_epi32
#define _mm512_rol_epi32 _mm512_rol_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_rol_epi64_dbg(__m512i src, __mmask8 k, __m512i a, const int imm8)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_rol_epi64
#define _mm512_mask_rol_epi64 _mm512_mask_rol_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_rol_epi64_dbg(__mmask8 k, __m512i a, const int imm8)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_rol_epi64
#define _mm512_maskz_rol_epi64 _mm512_maskz_rol_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m512i _mm512_rol_epi64_dbg(__m512i a, const int imm8)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_rol_epi64
#define _mm512_rol_epi64 _mm512_rol_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_rolv_epi32_dbg(__m512i src, __mmask16 k, __m512i a, __m512i b)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_rolv_epi32
#define _mm512_mask_rolv_epi32 _mm512_mask_rolv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_rolv_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_rolv_epi32
#define _mm512_maskz_rolv_epi32 _mm512_maskz_rolv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m512i _mm512_rolv_epi32_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_rolv_epi32
#define _mm512_rolv_epi32 _mm512_rolv_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_rolv_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_rolv_epi64
#define _mm512_mask_rolv_epi64 _mm512_mask_rolv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_rolv_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_rolv_epi64
#define _mm512_maskz_rolv_epi64 _mm512_maskz_rolv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m512i _mm512_rolv_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_rolv_epi64
#define _mm512_rolv_epi64 _mm512_rolv_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_ror_epi32_dbg(__m512i src, __mmask16 k, __m512i a, int imm8)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_ror_epi32
#define _mm512_mask_ror_epi32 _mm512_mask_ror_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_ror_epi32_dbg(__mmask16 k, __m512i a, int imm8)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_ror_epi32
#define _mm512_maskz_ror_epi32 _mm512_maskz_ror_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m512i _mm512_ror_epi32_dbg(__m512i a, int imm8)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_ror_epi32
#define _mm512_ror_epi32 _mm512_ror_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_ror_epi64_dbg(__m512i src, __mmask8 k, __m512i a, int imm8)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_ror_epi64
#define _mm512_mask_ror_epi64 _mm512_mask_ror_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_ror_epi64_dbg(__mmask8 k, __m512i a, int imm8)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_ror_epi64
#define _mm512_maskz_ror_epi64 _mm512_maskz_ror_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m512i _mm512_ror_epi64_dbg(__m512i a, int imm8)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_ror_epi64
#define _mm512_ror_epi64 _mm512_ror_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_rorv_epi32_dbg(__m512i src, __mmask16 k, __m512i a, __m512i b)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_rorv_epi32
#define _mm512_mask_rorv_epi32 _mm512_mask_rorv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_rorv_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_rorv_epi32
#define _mm512_maskz_rorv_epi32 _mm512_maskz_rorv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m512i _mm512_rorv_epi32_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_rorv_epi32
#define _mm512_rorv_epi32 _mm512_rorv_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_rorv_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_rorv_epi64
#define _mm512_mask_rorv_epi64 _mm512_mask_rorv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_rorv_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_rorv_epi64
#define _mm512_maskz_rorv_epi64 _mm512_maskz_rorv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m512i _mm512_rorv_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_rorv_epi64
#define _mm512_rorv_epi64 _mm512_rorv_epi64_dbg


/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_sll_epi32_dbg(__m512i src, __mmask16 k, __m512i a, __m128i count)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sll_epi32
#define _mm512_mask_sll_epi32 _mm512_mask_sll_epi32_dbg


/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_sll_epi32_dbg(__mmask16 k, __m512i a, __m128i count)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sll_epi32
#define _mm512_maskz_sll_epi32 _mm512_maskz_sll_epi32_dbg



/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_sll_epi32_dbg(__m512i a, __m128i count)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (count_vec[0] > 31) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sll_epi32
#define _mm512_sll_epi32 _mm512_sll_epi32_dbg


/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_sll_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m128i count)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sll_epi64
#define _mm512_mask_sll_epi64 _mm512_mask_sll_epi64_dbg



/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_sll_epi64_dbg(__mmask8 k, __m512i a, __m128i count)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sll_epi64
#define _mm512_maskz_sll_epi64 _mm512_maskz_sll_epi64_dbg



/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_sll_epi64_dbg(__m512i a, __m128i count)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (count_vec[0] > 63) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sll_epi64
#define _mm512_sll_epi64 _mm512_sll_epi64_dbg



/*
 Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_sllv_epi32_dbg(__mmask16 k, __m512i a, __m512i count)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t count_vec[16];
  _mm512_storeu_si512((void*)count_vec, count);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (count_vec[j] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sllv_epi32
#define _mm512_maskz_sllv_epi32 _mm512_maskz_sllv_epi32_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_sllv_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i count)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[8];
  _mm512_storeu_si512((void*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sllv_epi64
#define _mm512_mask_sllv_epi64 _mm512_mask_sllv_epi64_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_sllv_epi64_dbg(__mmask8 k, __m512i a, __m512i count)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[8];
  _mm512_storeu_si512((void*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sllv_epi64
#define _mm512_maskz_sllv_epi64 _mm512_maskz_sllv_epi64_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_sllv_epi64_dbg(__m512i a, __m512i count)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[8];
  _mm512_storeu_si512((void*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sllv_epi64
#define _mm512_sllv_epi64 _mm512_sllv_epi64_dbg


/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_srl_epi32_dbg(__m512i src, __mmask16 k, __m512i a, __m128i count)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_srl_epi32
#define _mm512_mask_srl_epi32 _mm512_mask_srl_epi32_dbg


/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_srl_epi32_dbg(__mmask16 k, __m512i a, __m128i count)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_srl_epi32
#define _mm512_maskz_srl_epi32 _mm512_maskz_srl_epi32_dbg


/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_srl_epi32_dbg(__m512i a, __m128i count)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (count_vec[0] > 31) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_srl_epi32
#define _mm512_srl_epi32 _mm512_srl_epi32_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_srl_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m128i count)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_srl_epi64
#define _mm512_mask_srl_epi64 _mm512_mask_srl_epi64_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_srl_epi64_dbg(__mmask8 k, __m512i a, __m128i count)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_srl_epi64
#define _mm512_maskz_srl_epi64 _mm512_maskz_srl_epi64_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_srl_epi64_dbg(__m512i a, __m128i count)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (count_vec[0] > 63) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_srl_epi64
#define _mm512_srl_epi64 _mm512_srl_epi64_dbg


/*
 Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_sub_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sub_epi32
#define _mm512_maskz_sub_epi32 _mm512_maskz_sub_epi32_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_sub_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sub_epi64
#define _mm512_mask_sub_epi64 _mm512_mask_sub_epi64_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_sub_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sub_epi64
#define _mm512_maskz_sub_epi64 _mm512_maskz_sub_epi64_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst".
*/
static inline __m512i _mm512_sub_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sub_epi64
#define _mm512_sub_epi64 _mm512_sub_epi64_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask8 _mm512_mask_test_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_test_epi64_mask
#define _mm512_mask_test_epi64_mask _mm512_mask_test_epi64_mask_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" if the intermediate value is non-zero.
*/
static inline __mmask8 _mm512_test_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_test_epi64_mask
#define _mm512_test_epi64_mask _mm512_test_epi64_mask_dbg


/*
 Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask16 _mm512_mask_testn_epi32_mask_dbg(__mmask16 k1, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_testn_epi32_mask
#define _mm512_mask_testn_epi32_mask _mm512_mask_testn_epi32_mask_dbg


/*
 Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask16 _mm512_testn_epi32_mask_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_testn_epi32_mask
#define _mm512_testn_epi32_mask _mm512_testn_epi32_mask_dbg


/*
 Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask8 _mm512_mask_testn_epi64_mask_dbg(__mmask8 k1, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_testn_epi64_mask
#define _mm512_mask_testn_epi64_mask _mm512_mask_testn_epi64_mask_dbg


/*
 Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask8 _mm512_testn_epi64_mask_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_testn_epi64_mask
#define _mm512_testn_epi64_mask _mm512_testn_epi64_mask_dbg


/*
 Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_xor_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_xor_epi32
#define _mm512_maskz_xor_epi32 _mm512_maskz_xor_epi32_dbg


/*
 Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_xor_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_xor_epi64
#define _mm512_maskz_xor_epi64 _mm512_maskz_xor_epi64_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512d _mm512_mask_rcp14_pd_dbg(__m512d src, __mmask8 k, __m512d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_rcp14_pd
#define _mm512_mask_rcp14_pd _mm512_mask_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512d _mm512_maskz_rcp14_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_rcp14_pd
#define _mm512_maskz_rcp14_pd _mm512_maskz_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512d _mm512_rcp14_pd_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_rcp14_pd
#define _mm512_rcp14_pd _mm512_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512 _mm512_mask_rcp14_ps_dbg(__m512 src, __mmask16 k, __m512 a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_rcp14_ps
#define _mm512_mask_rcp14_ps _mm512_mask_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512 _mm512_maskz_rcp14_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_rcp14_ps
#define _mm512_maskz_rcp14_ps _mm512_maskz_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512 _mm512_rcp14_ps_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_rcp14_ps
#define _mm512_rcp14_ps _mm512_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_mask_rcp14_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0/b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_rcp14_sd
#define _mm_mask_rcp14_sd _mm_mask_rcp14_sd_dbg


/*
 Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_maskz_rcp14_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0/b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_rcp14_sd
#define _mm_maskz_rcp14_sd _mm_maskz_rcp14_sd_dbg


/*
 Compute the approximate reciprocal of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_rcp14_sd_dbg(__m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = APPROXIMATE(1.0/b_vec[0]);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_rcp14_sd
#define _mm_rcp14_sd _mm_rcp14_sd_dbg


/*
 Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_mask_rcp14_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0/b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_rcp14_ss
#define _mm_mask_rcp14_ss _mm_mask_rcp14_ss_dbg


/*
 Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_maskz_rcp14_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0/b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_rcp14_ss
#define _mm_maskz_rcp14_ss _mm_maskz_rcp14_ss_dbg


/*
 Compute the approximate reciprocal of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_rcp14_ss_dbg(__m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = APPROXIMATE(1.0/b_vec[0]);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_rcp14_ss
#define _mm_rcp14_ss _mm_rcp14_ss_dbg


/*
 Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512d _mm512_mask_rsqrt14_pd_dbg(__m512d src, __mmask8 k, __m512d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_rsqrt14_pd
#define _mm512_mask_rsqrt14_pd _mm512_mask_rsqrt14_pd_dbg


/*
 Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512d _mm512_maskz_rsqrt14_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_rsqrt14_pd
#define _mm512_maskz_rsqrt14_pd _mm512_maskz_rsqrt14_pd_dbg


/*
 Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512d _mm512_rsqrt14_pd_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_rsqrt14_pd
#define _mm512_rsqrt14_pd _mm512_rsqrt14_pd_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512 _mm512_mask_rsqrt14_ps_dbg(__m512 src, __mmask16 k, __m512 a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_rsqrt14_ps
#define _mm512_mask_rsqrt14_ps _mm512_mask_rsqrt14_ps_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512 _mm512_maskz_rsqrt14_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_rsqrt14_ps
#define _mm512_maskz_rsqrt14_ps _mm512_maskz_rsqrt14_ps_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m512 _mm512_rsqrt14_ps_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_rsqrt14_ps
#define _mm512_rsqrt14_ps _mm512_rsqrt14_ps_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_mask_rsqrt14_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0 / sqrt(b_vec[0]));
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_rsqrt14_sd
#define _mm_mask_rsqrt14_sd _mm_mask_rsqrt14_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_maskz_rsqrt14_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0 / sqrt(b_vec[0]));
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_rsqrt14_sd
#define _mm_maskz_rsqrt14_sd _mm_maskz_rsqrt14_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_rsqrt14_sd_dbg(__m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = APPROXIMATE(1.0 / sqrt(b_vec[0]));
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_rsqrt14_sd
#define _mm_rsqrt14_sd _mm_rsqrt14_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_mask_rsqrt14_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0 / sqrt(b_vec[0]));
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_rsqrt14_ss
#define _mm_mask_rsqrt14_ss _mm_mask_rsqrt14_ss_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_maskz_rsqrt14_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = APPROXIMATE(1.0 / sqrt(b_vec[0]));
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_rsqrt14_ss
#define _mm_maskz_rsqrt14_ss _mm_maskz_rsqrt14_ss_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_rsqrt14_ss_dbg(__m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = APPROXIMATE(1.0 / sqrt(b_vec[0]));
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_rsqrt14_ss
#define _mm_rsqrt14_ss _mm_rsqrt14_ss_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_mask_sqrt_pd_dbg(__m512d src, __mmask8 k, __m512d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_sqrt_pd
#define _mm512_mask_sqrt_pd _mm512_mask_sqrt_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_mask_sqrt_round_pd_dbg(__m512d src, __mmask8 k, __m512d a, int rounding)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_sqrt_round_pd
#define _mm512_mask_sqrt_round_pd _mm512_mask_sqrt_round_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_sqrt_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_sqrt_pd
#define _mm512_maskz_sqrt_pd _mm512_maskz_sqrt_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE.
*/
static inline __m512d _mm512_maskz_sqrt_round_pd_dbg(__mmask8 k, __m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_sqrt_round_pd
#define _mm512_maskz_sqrt_round_pd _mm512_maskz_sqrt_round_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst".
*/
static inline __m512d _mm512_sqrt_pd_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = sqrt(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_sqrt_pd
#define _mm512_sqrt_pd _mm512_sqrt_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE.
*/
static inline __m512d _mm512_sqrt_round_pd_dbg(__m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = sqrt(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_sqrt_round_pd
#define _mm512_sqrt_round_pd _mm512_sqrt_round_pd_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_mask_sqrt_ps_dbg(__m512 src, __mmask16 k, __m512 a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_sqrt_ps
#define _mm512_mask_sqrt_ps _mm512_mask_sqrt_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask_sqrt_round_ps_dbg(__m512 src, __mmask16 k, __m512 a, int rounding)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_sqrt_round_ps
#define _mm512_mask_sqrt_round_ps _mm512_mask_sqrt_round_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_sqrt_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_sqrt_ps
#define _mm512_maskz_sqrt_ps _mm512_maskz_sqrt_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_sqrt_round_ps_dbg(__mmask16 k, __m512 a, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_sqrt_round_ps
#define _mm512_maskz_sqrt_round_ps _mm512_maskz_sqrt_round_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst".
*/
static inline __m512 _mm512_sqrt_ps_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = sqrt(a_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_sqrt_ps
#define _mm512_sqrt_ps _mm512_sqrt_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE.
*/
static inline __m512 _mm512_sqrt_round_ps_dbg(__m512 a, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = sqrt(a_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_sqrt_round_ps
#define _mm512_sqrt_round_ps _mm512_sqrt_round_ps_dbg


/*
 Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_mask_sqrt_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_sqrt_round_sd
#define _mm_mask_sqrt_round_sd _mm_mask_sqrt_round_sd_dbg


/*
 Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_sqrt_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_sqrt_sd
#define _mm_mask_sqrt_sd _mm_mask_sqrt_sd_dbg


/*
 Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_maskz_sqrt_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_sqrt_round_sd
#define _mm_maskz_sqrt_round_sd _mm_maskz_sqrt_round_sd_dbg


/*
 Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_sqrt_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_sqrt_sd
#define _mm_maskz_sqrt_sd _mm_maskz_sqrt_sd_dbg


/*
 Compute the square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_sqrt_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = sqrt(b_vec[0]);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_sqrt_round_sd
#define _mm_sqrt_round_sd _mm_sqrt_round_sd_dbg


/*
 Compute the square root of the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_mask_sqrt_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_sqrt_round_ss
#define _mm_mask_sqrt_round_ss _mm_mask_sqrt_round_ss_dbg


/*
 Compute the square root of the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_sqrt_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_sqrt_ss
#define _mm_mask_sqrt_ss _mm_mask_sqrt_ss_dbg


/*
 Compute the square root of the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_maskz_sqrt_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_sqrt_round_ss
#define _mm_maskz_sqrt_round_ss _mm_maskz_sqrt_round_ss_dbg


/*
 Compute the square root of the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_sqrt_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = sqrt(b_vec[0]);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_sqrt_ss
#define _mm_maskz_sqrt_ss _mm_maskz_sqrt_ss_dbg


/*
 Compute the square root of the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst", and copy the upper 3 packed elements from "b" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_sqrt_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = sqrt(b_vec[0]);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_sqrt_round_ss
#define _mm_sqrt_round_ss _mm_sqrt_round_ss_dbg

/*
Compute the absolute differences of packed unsigned 8-bit integers in a and b,
then horizontally sum each consecutive 8 differences to produce four unsigned 16-bit integers,
and pack these unsigned 16-bit integers in the low 16 bits of 64-bit elements in dst.
*/
static inline __m256i _mm256_sad_epu8_dbg(__m256i a, __m256i b)
{
    uint8_t a_vec[32];
    _mm256_storeu_si256((__m256i*)a_vec, a);
    uint8_t b_vec[32];
    _mm256_storeu_si256((__m256i*)b_vec, b);
    uint8_t tmp_vec[32];
    uint16_t dst_vec[16] = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 };
    uint16_t tmp_sum16 = 0;
    for (int j = 0, j16 = 0; j <= 31; j++) {
        tmp_vec[j] = abs(a_vec[j] - b_vec[j]);
        tmp_sum16 += tmp_vec[j];
        if ((j % 8) == 7) { // (j == 7) || (j == 15) || (j == 23) || (j == 31)
            dst_vec[j16] = tmp_sum16;
            j16 += 4;
            tmp_sum16 = 0;
        }
    }
    return _mm256_loadu_si256((__m256i*)dst_vec);
}
#undef _mm256_sad_epu8
#define _mm256_sad_epu8 _mm_sad_epu8_dbg

/*
Compute the sum of absolute differences (SADs) of quadruplets of unsigned 8-bit integers in a compared to those in b,
and store the 16-bit results in dst. Eight SADs are performed using one quadruplet from b and eight quadruplets from a.
One quadruplet is selected from b starting at on the offset specified in imm8.
Eight quadruplets are formed from sequential 8-bit integers selected from a starting at the offset specified in imm8.
*/
static inline __m256i _mm256_mpsadbw_epu8_dbg(__m256i a, __m256i b, const int imm8)
{
    uint8_t a_vec[32];
    _mm256_storeu_si256((__m256i*)a_vec, a);
    uint8_t b_vec[32];
    _mm256_storeu_si256((__m256i*)b_vec, b);
    uint16_t dst_vec[16];
    int a_offset = (imm8 & 0x4);
    int b_offset = (imm8 & 0x3) << 2;
    for (int j = 0; j <= 7; j++) {
        dst_vec[j] = abs(a_vec[j + a_offset] - b_vec[b_offset]) +
            abs(a_vec[j + a_offset + 1] - b_vec[b_offset + 1]) +
            abs(a_vec[j + a_offset + 2] - b_vec[b_offset + 2]) +
            abs(a_vec[j + a_offset + 3] - b_vec[b_offset + 3]);
    }
    a_offset = (imm8 & 0x20) >> 3;
    b_offset = (imm8 & 0x18) >> 1;
    for (int j = 8; j <= 15; j++) {
        dst_vec[j] = abs(a_vec[j + a_offset + 8] - b_vec[b_offset + 16]) +
            abs(a_vec[j + a_offset + 8 + 1] - b_vec[b_offset + 16 + 1]) +
            abs(a_vec[j + a_offset + 8 + 2] - b_vec[b_offset + 16 + 2]) +
            abs(a_vec[j + a_offset + 8 + 3] - b_vec[b_offset + 16 + 3]);
    }
    return _mm256_loadu_si256((__m256i*)dst_vec);
}
#undef _mm_mpsadbw_epu8
#define _mm_mpsadbw_epu8 _mm_mpsadbw_epu8_dbg

/*
 Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_sub_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_sub_pd
#define _mm512_maskz_sub_pd _mm512_maskz_sub_pd_dbg

/*
 Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_sub_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_sub_round_pd
#define _mm512_maskz_sub_round_pd _mm512_maskz_sub_round_pd_dbg


/*
 Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_sub_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_sub_ps
#define _mm512_maskz_sub_ps _mm512_maskz_sub_ps_dbg


/*
 Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_sub_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_sub_round_ps
#define _mm512_maskz_sub_round_ps _mm512_maskz_sub_round_ps_dbg


/*
 Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision (64-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_mask_sub_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_sub_round_sd
#define _mm_mask_sub_round_sd _mm_mask_sub_round_sd_dbg


/*
 Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision (64-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_mask_sub_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_sub_sd
#define _mm_mask_sub_sd _mm_mask_sub_sd_dbg


/*
 Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision (64-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_maskz_sub_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_sub_round_sd
#define _mm_maskz_sub_round_sd _mm_maskz_sub_round_sd_dbg


/*
 Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision (64-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_sub_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_sub_sd
#define _mm_maskz_sub_sd _mm_maskz_sub_sd_dbg


/*
 Subtract the lower double-precision (64-bit) floating-point element in "b" from the lower double-precision (64-bit) floating-point element in "a", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_sub_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = a_vec[0] - b_vec[0];
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_sub_round_sd
#define _mm_sub_round_sd _mm_sub_round_sd_dbg


/*
 Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_mask_sub_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_sub_round_ss
#define _mm_mask_sub_round_ss _mm_mask_sub_round_ss_dbg


/*
 Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_sub_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_sub_ss
#define _mm_mask_sub_ss _mm_mask_sub_ss_dbg


/*
 Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_maskz_sub_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_sub_round_ss
#define _mm_maskz_sub_round_ss _mm_maskz_sub_round_ss_dbg


/*
 Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_sub_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = a_vec[0] - b_vec[0];
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_sub_ss
#define _mm_maskz_sub_ss _mm_maskz_sub_ss_dbg


/*
 Subtract the lower single-precision (32-bit) floating-point element in "b" from the lower single-precision (32-bit) floating-point element in "a", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_sub_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = a_vec[0] - b_vec[0];
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_sub_round_ss
#define _mm_sub_round_ss _mm_sub_round_ss_dbg


/*
 Reduce the packed 32-bit integers in "a" by addition using mask "k". Returns the sum of all active elements in "a".
*/
static inline int _mm512_mask_reduce_add_epi32_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int dst;
  dst = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst = dst + a_vec[j];
    }
  }
return dst;
}

#undef _mm512_mask_reduce_add_epi32
#define _mm512_mask_reduce_add_epi32 _mm512_mask_reduce_add_epi32_dbg

/*
 Reduce the packed 32-bit integers in "a" by addition. Returns the sum of all active elements in "a".
*/
static inline int _mm512_reduce_add_epi32_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int dst;
  dst = 0;
  for (int j = 0; j <= 15; j++) {
    dst = dst + a_vec[j];
  }
return dst;
}

#undef _mm512_reduce_add_epi32
#define _mm512_reduce_add_epi32 _mm512_reduce_add_epi32_dbg

/*
 Reduce the packed 64-bit integers in "a" by addition using mask "k". Returns the sum of all active elements in "a".
*/
static inline int64_t _mm512_mask_reduce_add_epi64_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst;
  dst = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst = dst + a_vec[j];
    }
  }
return dst;
}

#undef _mm512_mask_reduce_add_epi64
#define _mm512_mask_reduce_add_epi64 _mm512_mask_reduce_add_epi64_dbg


/*
 Reduce the packed double-precision (64-bit) floating-point elements in "a" by addition using mask "k". Returns the sum of all active elements in "a".
*/
static inline double _mm512_mask_reduce_add_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst;
  dst = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst = dst + a_vec[j];
    }
  }
return dst;
}

#undef _mm512_mask_reduce_add_pd
#define _mm512_mask_reduce_add_pd _mm512_mask_reduce_add_pd_dbg


/*
 Reduce the packed single-precision (32-bit) floating-point elements in "a" by addition using mask "k". Returns the sum of all active elements in "a".
*/
static inline float _mm512_mask_reduce_add_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst;
  dst = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst = dst + a_vec[j];
    }
  }
return dst;
}

#undef _mm512_mask_reduce_add_ps
#define _mm512_mask_reduce_add_ps _mm512_mask_reduce_add_ps_dbg


/*
 Reduce the packed 32-bit integers in "a" by bitwise AND using mask "k". Returns the bitwise AND of all active elements in "a".
*/
static inline int _mm512_mask_reduce_and_epi32_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int dst;
  dst = 0xFFFFFFFF;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst = dst & a_vec[j];
    }
  }
return dst;
}

#undef _mm512_mask_reduce_and_epi32
#define _mm512_mask_reduce_and_epi32 _mm512_mask_reduce_and_epi32_dbg


/*
 Reduce the packed 64-bit integers in "a" by bitwise AND using mask "k". Returns the bitwise AND of all active elements in "a".
*/
static inline int64_t _mm512_mask_reduce_and_epi64_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst;
  dst = 0xFFFFFFFFFFFFFFFFULL;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst = dst & a_vec[j];
    }
  }
return dst;
}

#undef _mm512_mask_reduce_and_epi64
#define _mm512_mask_reduce_and_epi64 _mm512_mask_reduce_and_epi64_dbg


/*
 Reduce the packed 64-bit integers in "a" by addition. Returns the sum of all elements in "a".
*/
static inline int64_t _mm512_reduce_add_epi64_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst;
  dst = 0;
  for (int j = 0; j <= 7; j++) {
    dst = dst + a_vec[j];
  }
return dst;
}

#undef _mm512_reduce_add_epi64
#define _mm512_reduce_add_epi64 _mm512_reduce_add_epi64_dbg


/*
 Reduce the packed double-precision (64-bit) floating-point elements in "a" by addition. Returns the sum of all elements in "a".
*/
static inline double _mm512_reduce_add_pd_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst;
  dst = 0;
  for (int j = 0; j <= 7; j++) {
    dst = dst + a_vec[j];
  }
return dst;
}

#undef _mm512_reduce_add_pd
#define _mm512_reduce_add_pd _mm512_reduce_add_pd_dbg


/*
 Reduce the packed single-precision (32-bit) floating-point elements in "a" by addition. Returns the sum of all elements in "a".
*/
static inline float _mm512_reduce_add_ps_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst = 0;
  for (int j = 0; j <= 15; j++) {
    dst = dst + a_vec[j];
  }
return dst;
}

#undef _mm512_reduce_add_ps
#define _mm512_reduce_add_ps _mm512_reduce_add_ps_dbg

/*
 Broadcast double-precision (64-bit) floating-point value "a" to all elements of "dst".
*/
static inline __m512d _mm512_set1_pd_dbg(double a)
{
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a;
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_set1_pd
#define _mm512_set1_pd _mm512_set1_pd_dbg


/*
 Broadcast single-precision (32-bit) floating-point value "a" to all elements of "dst".
*/
static inline __m512 _mm512_set1_ps_dbg(float a)
{
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a;
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_set1_ps
#define _mm512_set1_ps _mm512_set1_ps_dbg


/*
 Set packed 32-bit integers in "dst" with the repeated 4 element sequence.
*/
static inline __m512i _mm512_set4_epi32_dbg(int d, int c, int b, int a)
{
  int32_t dst_vec[16];
  dst_vec[0] = a;
  dst_vec[1] = b;
  dst_vec[2] = c;
  dst_vec[3] = d;
  dst_vec[4] = a;
  dst_vec[5] = b;
  dst_vec[6] = c;
  dst_vec[7] = d;
  dst_vec[8] = a;
  dst_vec[9] = b;
  dst_vec[10] = c;
  dst_vec[11] = d;
  dst_vec[12] = a;
  dst_vec[13] = b;
  dst_vec[14] = c;
  dst_vec[15] = d;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set4_epi32
#define _mm512_set4_epi32 _mm512_set4_epi32_dbg


/*
 Set packed 64-bit integers in "dst" with the repeated 4 element sequence.
*/
static inline __m512i _mm512_set4_epi64_dbg(int64_t d, int64_t c, int64_t b, int64_t a)
{
  int64_t dst_vec[8];
  dst_vec[0] = a;
  dst_vec[1] = b;
  dst_vec[2] = c;
  dst_vec[3] = d;
  dst_vec[4] = a;
  dst_vec[5] = b;
  dst_vec[6] = c;
  dst_vec[7] = d;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set4_epi64
#define _mm512_set4_epi64 _mm512_set4_epi64_dbg


/*
 Set packed double-precision (64-bit) floating-point elements in "dst" with the repeated 4 element sequence.
*/
static inline __m512d _mm512_set4_pd_dbg(double d, double c, double b, double a)
{
  double dst_vec[8];
  dst_vec[0] = a;
  dst_vec[1] = b;
  dst_vec[2] = c;
  dst_vec[3] = d;
  dst_vec[4] = a;
  dst_vec[5] = b;
  dst_vec[6] = c;
  dst_vec[7] = d;
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_set4_pd
#define _mm512_set4_pd _mm512_set4_pd_dbg


/*
 Set packed single-precision (32-bit) floating-point elements in "dst" with the repeated 4 element sequence.
*/
static inline __m512 _mm512_set4_ps_dbg(float d, float c, float b, float a)
{
  float dst_vec[16];
  dst_vec[0] = a;
  dst_vec[1] = b;
  dst_vec[2] = c;
  dst_vec[3] = d;
  dst_vec[4] = a;
  dst_vec[5] = b;
  dst_vec[6] = c;
  dst_vec[7] = d;
  dst_vec[8] = a;
  dst_vec[9] = b;
  dst_vec[10] = c;
  dst_vec[11] = d;
  dst_vec[12] = a;
  dst_vec[13] = b;
  dst_vec[14] = c;
  dst_vec[15] = d;
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_set4_ps
#define _mm512_set4_ps _mm512_set4_ps_dbg


/*
 Set packed 8-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m512i _mm512_set_epi8_dbg(char e63, char e62, char e61, char e60, char e59, char e58, char e57, char e56, char e55, char e54, char e53, char e52, char e51, char e50, char e49, char e48, char e47, char e46, char e45, char e44, char e43, char e42, char e41, char e40, char e39, char e38, char e37, char e36, char e35, char e34, char e33, char e32, char e31, char e30, char e29, char e28, char e27, char e26, char e25, char e24, char e23, char e22, char e21, char e20, char e19, char e18, char e17, char e16, char e15, char e14, char e13, char e12, char e11, char e10, char e9, char e8, char e7, char e6, char e5, char e4, char e3, char e2, char e1, char e0)
{
  int8_t dst_vec[64];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  dst_vec[8] = e8;
  dst_vec[9] = e9;
  dst_vec[10] = e10;
  dst_vec[11] = e11;
  dst_vec[12] = e12;
  dst_vec[13] = e13;
  dst_vec[14] = e14;
  dst_vec[15] = e15;
  dst_vec[16] = e16;
  dst_vec[17] = e17;
  dst_vec[18] = e18;
  dst_vec[19] = e19;
  dst_vec[20] = e20;
  dst_vec[21] = e21;
  dst_vec[22] = e22;
  dst_vec[23] = e23;
  dst_vec[24] = e24;
  dst_vec[25] = e25;
  dst_vec[26] = e26;
  dst_vec[27] = e27;
  dst_vec[28] = e28;
  dst_vec[29] = e29;
  dst_vec[30] = e30;
  dst_vec[31] = e31;
  dst_vec[32] = e32;
  dst_vec[33] = e33;
  dst_vec[34] = e34;
  dst_vec[35] = e35;
  dst_vec[36] = e36;
  dst_vec[37] = e37;
  dst_vec[38] = e38;
  dst_vec[39] = e39;
  dst_vec[40] = e40;
  dst_vec[41] = e41;
  dst_vec[42] = e42;
  dst_vec[43] = e43;
  dst_vec[44] = e44;
  dst_vec[45] = e45;
  dst_vec[46] = e46;
  dst_vec[47] = e47;
  dst_vec[48] = e48;
  dst_vec[49] = e49;
  dst_vec[50] = e50;
  dst_vec[51] = e51;
  dst_vec[52] = e52;
  dst_vec[53] = e53;
  dst_vec[54] = e54;
  dst_vec[55] = e55;
  dst_vec[56] = e56;
  dst_vec[57] = e57;
  dst_vec[58] = e58;
  dst_vec[59] = e59;
  dst_vec[60] = e60;
  dst_vec[61] = e61;
  dst_vec[62] = e62;
  dst_vec[63] = e63;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set_epi8
#define _mm512_set_epi8 _mm512_set_epi8_dbg


/*
 Set packed 16-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m512i _mm512_set_epi16_dbg(short e31, short e30, short e29, short e28, short e27, short e26, short e25, short e24, short e23, short e22, short e21, short e20, short e19, short e18, short e17, short e16, short e15, short e14, short e13, short e12, short e11, short e10, short e9, short e8, short e7, short e6, short e5, short e4, short e3, short e2, short e1, short e0)
{
  int16_t dst_vec[32];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  dst_vec[8] = e8;
  dst_vec[9] = e9;
  dst_vec[10] = e10;
  dst_vec[11] = e11;
  dst_vec[12] = e12;
  dst_vec[13] = e13;
  dst_vec[14] = e14;
  dst_vec[15] = e15;
  dst_vec[16] = e16;
  dst_vec[17] = e17;
  dst_vec[18] = e18;
  dst_vec[19] = e19;
  dst_vec[20] = e20;
  dst_vec[21] = e21;
  dst_vec[22] = e22;
  dst_vec[23] = e23;
  dst_vec[24] = e24;
  dst_vec[25] = e25;
  dst_vec[26] = e26;
  dst_vec[27] = e27;
  dst_vec[28] = e28;
  dst_vec[29] = e29;
  dst_vec[30] = e30;
  dst_vec[31] = e31;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set_epi16
#define _mm512_set_epi16 _mm512_set_epi16_dbg


/*
 Set packed 32-bit integers in "dst" with the supplied values.
*/
static inline __m512i _mm512_set_epi32_dbg(int e15, int e14, int e13, int e12, int e11, int e10, int e9, int e8, int e7, int e6, int e5, int e4, int e3, int e2, int e1, int e0)
{
  int32_t dst_vec[16];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  dst_vec[8] = e8;
  dst_vec[9] = e9;
  dst_vec[10] = e10;
  dst_vec[11] = e11;
  dst_vec[12] = e12;
  dst_vec[13] = e13;
  dst_vec[14] = e14;
  dst_vec[15] = e15;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set_epi32
#define _mm512_set_epi32 _mm512_set_epi32_dbg


/*
 Set packed 64-bit integers in "dst" with the supplied values.
*/
static inline __m512i _mm512_set_epi64_dbg(int64_t e7, int64_t e6, int64_t e5, int64_t e4, int64_t e3, int64_t e2, int64_t e1, int64_t e0)
{
  int64_t dst_vec[8];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_set_epi64
#define _mm512_set_epi64 _mm512_set_epi64_dbg


/*
 Set packed double-precision (64-bit) floating-point elements in "dst" with the supplied values.
*/
static inline __m512d _mm512_set_pd_dbg(double e7, double e6, double e5, double e4, double e3, double e2, double e1, double e0)
{
  double dst_vec[8];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_set_pd
#define _mm512_set_pd _mm512_set_pd_dbg


/*
 Set packed single-precision (32-bit) floating-point elements in "dst" with the supplied values.
*/
static inline __m512 _mm512_set_ps_dbg(float e15, float e14, float e13, float e12, float e11, float e10, float e9, float e8, float e7, float e6, float e5, float e4, float e3, float e2, float e1, float e0)
{
  float dst_vec[16];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  dst_vec[8] = e8;
  dst_vec[9] = e9;
  dst_vec[10] = e10;
  dst_vec[11] = e11;
  dst_vec[12] = e12;
  dst_vec[13] = e13;
  dst_vec[14] = e14;
  dst_vec[15] = e15;
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_set_ps
#define _mm512_set_ps _mm512_set_ps_dbg


/*
 Set packed 32-bit integers in "dst" with the repeated 4 element sequence in reverse order.
*/
static inline __m512i _mm512_setr4_epi32_dbg(int d, int c, int b, int a)
{
  int32_t dst_vec[16];
  dst_vec[0] = d;
  dst_vec[1] = c;
  dst_vec[2] = b;
  dst_vec[3] = a;
  dst_vec[4] = d;
  dst_vec[5] = c;
  dst_vec[6] = b;
  dst_vec[7] = a;
  dst_vec[8] = d;
  dst_vec[9] = c;
  dst_vec[10] = b;
  dst_vec[11] = a;
  dst_vec[12] = d;
  dst_vec[13] = c;
  dst_vec[14] = b;
  dst_vec[15] = a;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_setr4_epi32
#define _mm512_setr4_epi32 _mm512_setr4_epi32_dbg


/*
 Set packed 64-bit integers in "dst" with the repeated 4 element sequence in reverse order.
*/
static inline __m512i _mm512_setr4_epi64_dbg(int64_t d, int64_t c, int64_t b, int64_t a)
{
  int64_t dst_vec[8];
  dst_vec[0] = d;
  dst_vec[1] = c;
  dst_vec[2] = b;
  dst_vec[3] = a;
  dst_vec[4] = d;
  dst_vec[5] = c;
  dst_vec[6] = b;
  dst_vec[7] = a;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_setr4_epi64
#define _mm512_setr4_epi64 _mm512_setr4_epi64_dbg


/*
 Set packed double-precision (64-bit) floating-point elements in "dst" with the repeated 4 element sequence in reverse order.
*/
static inline __m512d _mm512_setr4_pd_dbg(double d, double c, double b, double a)
{
  double dst_vec[8];
  dst_vec[0] = d;
  dst_vec[1] = c;
  dst_vec[2] = b;
  dst_vec[3] = a;
  dst_vec[4] = d;
  dst_vec[5] = c;
  dst_vec[6] = b;
  dst_vec[7] = a;
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_setr4_pd
#define _mm512_setr4_pd _mm512_setr4_pd_dbg


/*
 Set packed single-precision (32-bit) floating-point elements in "dst" with the repeated 4 element sequence in reverse order.
*/
static inline __m512 _mm512_setr4_ps_dbg(float d, float c, float b, float a)
{
  float dst_vec[16];
  dst_vec[0] = d;
  dst_vec[1] = c;
  dst_vec[2] = b;
  dst_vec[3] = a;
  dst_vec[4] = d;
  dst_vec[5] = c;
  dst_vec[6] = b;
  dst_vec[7] = a;
  dst_vec[8] = d;
  dst_vec[9] = c;
  dst_vec[10] = b;
  dst_vec[11] = a;
  dst_vec[12] = d;
  dst_vec[13] = c;
  dst_vec[14] = b;
  dst_vec[15] = a;
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_setr4_ps
#define _mm512_setr4_ps _mm512_setr4_ps_dbg


/*
 Set packed 32-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m512i _mm512_setr_epi32_dbg(int e15, int e14, int e13, int e12, int e11, int e10, int e9, int e8, int e7, int e6, int e5, int e4, int e3, int e2, int e1, int e0)
{
  int32_t dst_vec[16];
  dst_vec[0] = e15;
  dst_vec[1] = e14;
  dst_vec[2] = e13;
  dst_vec[3] = e12;
  dst_vec[4] = e11;
  dst_vec[5] = e10;
  dst_vec[6] = e9;
  dst_vec[7] = e8;
  dst_vec[8] = e7;
  dst_vec[9] = e6;
  dst_vec[10] = e5;
  dst_vec[11] = e4;
  dst_vec[12] = e3;
  dst_vec[13] = e2;
  dst_vec[14] = e1;
  dst_vec[15] = e0;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_setr_epi32
#define _mm512_setr_epi32 _mm512_setr_epi32_dbg


/*
 Set packed 64-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m512i _mm512_setr_epi64_dbg(int64_t e7, int64_t e6, int64_t e5, int64_t e4, int64_t e3, int64_t e2, int64_t e1, int64_t e0)
{
  int64_t dst_vec[8];
  dst_vec[0] = e7;
  dst_vec[1] = e6;
  dst_vec[2] = e5;
  dst_vec[3] = e4;
  dst_vec[4] = e3;
  dst_vec[5] = e2;
  dst_vec[6] = e1;
  dst_vec[7] = e0;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_setr_epi64
#define _mm512_setr_epi64 _mm512_setr_epi64_dbg


/*
 Set packed double-precision (64-bit) floating-point elements in "dst" with the supplied values in reverse order.
*/
static inline __m512d _mm512_setr_pd_dbg(double e7, double e6, double e5, double e4, double e3, double e2, double e1, double e0)
{
  double dst_vec[8];
  dst_vec[0] = e7;
  dst_vec[1] = e6;
  dst_vec[2] = e5;
  dst_vec[3] = e4;
  dst_vec[4] = e3;
  dst_vec[5] = e2;
  dst_vec[6] = e1;
  dst_vec[7] = e0;
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_setr_pd
#define _mm512_setr_pd _mm512_setr_pd_dbg


/*
 Set packed single-precision (32-bit) floating-point elements in "dst" with the supplied values in reverse order.
*/
static inline __m512 _mm512_setr_ps_dbg(float e15, float e14, float e13, float e12, float e11, float e10, float e9, float e8, float e7, float e6, float e5, float e4, float e3, float e2, float e1, float e0)
{
  float dst_vec[16];
  dst_vec[0] = e15;
  dst_vec[1] = e14;
  dst_vec[2] = e13;
  dst_vec[3] = e12;
  dst_vec[4] = e11;
  dst_vec[5] = e10;
  dst_vec[6] = e9;
  dst_vec[7] = e8;
  dst_vec[8] = e7;
  dst_vec[9] = e6;
  dst_vec[10] = e5;
  dst_vec[11] = e4;
  dst_vec[12] = e3;
  dst_vec[13] = e2;
  dst_vec[14] = e1;
  dst_vec[15] = e0;
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_setr_ps
#define _mm512_setr_ps _mm512_setr_ps_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-28. (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_rsqrt28_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = (1.0/sqrt(b_vec[0]));
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_rsqrt28_round_sd
#define _mm_rsqrt28_round_sd _mm_rsqrt28_round_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-28.
*/
static inline __m128d _mm_rsqrt28_sd_dbg(__m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = (1.0/sqrt(b_vec[0]));
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_rsqrt28_sd
#define _mm_rsqrt28_sd _mm_rsqrt28_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-28. (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask_rsqrt28_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_rsqrt28_round_sd
#define _mm_mask_rsqrt28_round_sd _mm_mask_rsqrt28_round_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-28.
*/
static inline __m128d _mm_mask_rsqrt28_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_rsqrt28_sd
#define _mm_mask_rsqrt28_sd _mm_mask_rsqrt28_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-28. (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_maskz_rsqrt28_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_rsqrt28_round_sd
#define _mm_maskz_rsqrt28_round_sd _mm_maskz_rsqrt28_round_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower double-precision (64-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst". The maximum relative error for this approximation is less than 2^-28.
*/
static inline __m128d _mm_maskz_rsqrt28_sd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_rsqrt28_sd
#define _mm_maskz_rsqrt28_sd _mm_maskz_rsqrt28_sd_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-28. (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_rsqrt28_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = (1.0/sqrt(b_vec[0]));
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_rsqrt28_round_ss
#define _mm_rsqrt28_round_ss _mm_rsqrt28_round_ss_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-28.
*/
static inline __m128 _mm_rsqrt28_ss_dbg(__m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = (1.0/sqrt(b_vec[0]));
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_rsqrt28_ss
#define _mm_rsqrt28_ss _mm_rsqrt28_ss_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-28. (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask_rsqrt28_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_rsqrt28_round_ss
#define _mm_mask_rsqrt28_round_ss _mm_mask_rsqrt28_round_ss_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-28.
*/
static inline __m128 _mm_mask_rsqrt28_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_rsqrt28_ss
#define _mm_mask_rsqrt28_ss _mm_mask_rsqrt28_ss_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-28. (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_maskz_rsqrt28_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_rsqrt28_round_ss
#define _mm_maskz_rsqrt28_round_ss _mm_maskz_rsqrt28_round_ss_dbg


/*
 Compute the approximate reciprocal square root of the lower single-precision (32-bit) floating-point element in "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst". The maximum relative error for this approximation is less than 2^-28.
*/
static inline __m128 _mm_maskz_rsqrt28_ss_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = (1.0/sqrt(b_vec[0]));
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_rsqrt28_ss
#define _mm_maskz_rsqrt28_ss _mm_maskz_rsqrt28_ss_dbg


/*
 Converts bit mask "k1" into an integer value, storing the results in "dst".
*/
static inline int _mm512_mask2int_dbg(__mmask16 k1)
{
  int dst;
  dst = ZeroExtend((uint16_t)k1);
return dst;
}

#undef _mm512_mask2int
#define _mm512_mask2int _mm512_mask2int_dbg


/*
 Converts integer "mask" into bitmask, storing the result in "dst".
*/
static inline __mmask16 _mm512_int2mask_dbg(int mask)
{
  __mmask16 dst = mask;
  dst = (mask & 0xffff) >> 0;
return dst;
}

#undef _mm512_int2mask
#define _mm512_int2mask _mm512_int2mask_dbg


/*
 Multiplies elements in packed 64-bit integer vectors "a" and "b" together, storing the lower 64 bits of the result in "dst".
*/
static inline __m512i _mm512_mullox_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] * b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mullox_epi64
#define _mm512_mullox_epi64 _mm512_mullox_epi64_dbg


/*
 Multiplies elements in packed 64-bit integer vectors "a" and "b" together, storing the lower 64 bits of the result in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_mullox_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mullox_epi64
#define _mm512_mask_mullox_epi64 _mm512_mask_mullox_epi64_dbg


/*
 Copy the lower single-precision (32-bit) floating-point element of "a" to "dst".
*/
static inline float _mm512_cvtss_f32_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst;
  dst = a_vec[0];
return dst;
}

#undef _mm512_cvtss_f32
#define _mm512_cvtss_f32 _mm512_cvtss_f32_dbg


/*
 Copy the lower double-precision (64-bit) floating-point element of "a" to "dst".
*/
static inline double _mm512_cvtsd_f64_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst;
  dst = a_vec[0];
return dst;
}

#undef _mm512_cvtsd_f64
#define _mm512_cvtsd_f64 _mm512_cvtsd_f64_dbg




















/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst".
*/
static inline __m512i _mm512_popcnt_epi32_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_popcnt_epi32
#define _mm512_popcnt_epi32 _mm512_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_popcnt_epi32_dbg(__m512i src, __mmask16 k, __m512i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_popcnt_epi32
#define _mm512_mask_popcnt_epi32 _mm512_mask_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 32-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_popcnt_epi32_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_popcnt_epi32
#define _mm512_maskz_popcnt_epi32 _mm512_maskz_popcnt_epi32_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst".
*/
static inline __m512i _mm512_popcnt_epi64_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_popcnt_epi64
#define _mm512_popcnt_epi64 _mm512_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_popcnt_epi64_dbg(__m512i src, __mmask8 k, __m512i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_popcnt_epi64
#define _mm512_mask_popcnt_epi64 _mm512_mask_popcnt_epi64_dbg


/*
 Count the number of logical 1 bits in packed 64-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_popcnt_epi64_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = POPCNT(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_popcnt_epi64
#define _mm512_maskz_popcnt_epi64 _mm512_maskz_popcnt_epi64_dbg


/*
 Unpack and interleave 32 bits from masks "a" and "b", and store the 64-bit result in "k".
*/
static inline __mmask64 _mm512_kunpackd_dbg(__mmask64 a, __mmask64 b)
{
  __mmask64 k;
  k = (b & 0xffffffffULL);
  k |= (a & 0xffffffffULL) << 32;
  return k;
}

#undef _mm512_kunpackd
#define _mm512_kunpackd _mm512_kunpackd_dbg

/*
 Unpack and interleave 16 bits from masks "a" and "b", and store the 32-bit result in "k".
*/
static inline __mmask32 _mm512_kunpackw_dbg(__mmask32 a, __mmask32 b)
{
  __mmask32 k;
  k = (b & 0xffff);
  k |= (a & 0xffff) << 16;
  return k;
}

#undef _mm512_kunpackw
#define _mm512_kunpackw _mm512_kunpackw_dbg


/*
 Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_add_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_add_pd
#define _mm256_mask_add_pd _mm256_mask_add_pd_dbg


/*
 Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_add_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_add_pd
#define _mm256_maskz_add_pd _mm256_maskz_add_pd_dbg


/*
 Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_add_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_add_pd
#define _mm_mask_add_pd _mm_mask_add_pd_dbg


/*
 Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_add_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_add_pd
#define _mm_maskz_add_pd _mm_maskz_add_pd_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_add_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_add_ps
#define _mm256_mask_add_ps _mm256_mask_add_ps_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_add_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_add_ps
#define _mm256_maskz_add_ps _mm256_maskz_add_ps_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_add_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_add_ps
#define _mm_mask_add_ps _mm_mask_add_ps_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_add_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_add_ps
#define _mm_maskz_add_ps _mm_maskz_add_ps_dbg


/*
 Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_andnot_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  uint64_t src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint64_t)a_vec[j]) & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_andnot_pd
#define _mm256_mask_andnot_pd _mm256_mask_andnot_pd_dbg


/*
 Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_andnot_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint64_t)a_vec[j]) & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_andnot_pd
#define _mm256_maskz_andnot_pd _mm256_maskz_andnot_pd_dbg


/*
 Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst".
*/
static inline __m512d _mm512_andnot_pd_dbg(__m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ((~ (uint64_t)a_vec[j]) & (uint64_t)b_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_andnot_pd
#define _mm512_andnot_pd _mm512_andnot_pd_dbg


/*
 Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_andnot_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b)
{
  uint64_t src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint64_t)a_vec[j]) & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_andnot_pd
#define _mm512_mask_andnot_pd _mm512_mask_andnot_pd_dbg

/*
 Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_andnot_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint64_t)a_vec[j]) & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_andnot_pd
#define _mm512_maskz_andnot_pd _mm512_maskz_andnot_pd_dbg

/*
 Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_andnot_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  uint64_t src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint64_t)a_vec[j]) & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_andnot_pd
#define _mm_mask_andnot_pd _mm_mask_andnot_pd_dbg

/*
 Compute the bitwise NOT of packed double-precision (64-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_andnot_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint64_t)a_vec[j]) & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_andnot_pd
#define _mm_maskz_andnot_pd _mm_maskz_andnot_pd_dbg

/*
 Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_andnot_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  int32_t src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint32_t)a_vec[j]) & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_andnot_ps
#define _mm256_mask_andnot_ps _mm256_mask_andnot_ps_dbg


/*
 Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_andnot_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint32_t)a_vec[j]) & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_andnot_ps
#define _mm256_maskz_andnot_ps _mm256_maskz_andnot_ps_dbg


/*
 Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst".
*/
static inline __m512 _mm512_andnot_ps_dbg(__m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ((~ (uint32_t)a_vec[j]) & (uint32_t)b_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_andnot_ps
#define _mm512_andnot_ps _mm512_andnot_ps_dbg


/*
 Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_andnot_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  int32_t src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ((~ (uint32_t)a_vec[j]) & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_andnot_ps
#define _mm512_mask_andnot_ps _mm512_mask_andnot_ps_dbg


/*
 Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_andnot_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ((~ (uint32_t)a_vec[j]) & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_andnot_ps
#define _mm512_maskz_andnot_ps _mm512_maskz_andnot_ps_dbg


/*
 Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_andnot_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  int32_t src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint32_t)a_vec[j]) & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_andnot_ps
#define _mm_mask_andnot_ps _mm_mask_andnot_ps_dbg


/*
 Compute the bitwise NOT of packed single-precision (32-bit) floating-point elements in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_andnot_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ (uint32_t)a_vec[j]) & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_andnot_ps
#define _mm_maskz_andnot_ps _mm_maskz_andnot_ps_dbg


/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_and_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  uint64_t src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_and_pd
#define _mm256_mask_and_pd _mm256_mask_and_pd_dbg


/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_and_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_and_pd
#define _mm256_maskz_and_pd _mm256_maskz_and_pd_dbg


/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
*/
static inline __m512d _mm512_and_pd_dbg(__m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_and_pd
#define _mm512_and_pd _mm512_and_pd_dbg

/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_and_epi32_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] & b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_and_epi32
#define _mm512_and_epi32 _mm512_and_epi32_dbg

/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_and_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b)
{
  uint64_t src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_and_pd
#define _mm512_mask_and_pd _mm512_mask_and_pd_dbg


/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_and_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_and_pd
#define _mm512_maskz_and_pd _mm512_maskz_and_pd_dbg


/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_and_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  uint64_t src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_and_pd
#define _mm_mask_and_pd _mm_mask_and_pd_dbg


/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_and_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_and_pd
#define _mm_maskz_and_pd _mm_maskz_and_pd_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_and_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  int32_t src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_and_ps
#define _mm256_mask_and_ps _mm256_mask_and_ps_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_and_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_and_ps
#define _mm256_maskz_and_ps _mm256_maskz_and_ps_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
*/
static inline __m512 _mm512_and_ps_dbg(__m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_and_ps
#define _mm512_and_ps _mm512_and_ps_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_and_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  int32_t src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_and_ps
#define _mm512_mask_and_ps _mm512_mask_and_ps_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_and_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_and_ps
#define _mm512_maskz_and_ps _mm512_maskz_and_ps_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_and_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  int32_t src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_and_ps
#define _mm_mask_and_ps _mm_mask_and_ps_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_and_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_and_ps
#define _mm_maskz_and_ps _mm_maskz_and_ps_dbg


/*
 Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m256d _mm256_mask_blend_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_blend_pd
#define _mm256_mask_blend_pd _mm256_mask_blend_pd_dbg


/*
 Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m128d _mm_mask_blend_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_blend_pd
#define _mm_mask_blend_pd _mm_mask_blend_pd_dbg


/*
 Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m256 _mm256_mask_blend_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_blend_ps
#define _mm256_mask_blend_ps _mm256_mask_blend_ps_dbg


/*
 Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m128 _mm_mask_blend_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_blend_ps
#define _mm_mask_blend_ps _mm_mask_blend_ps_dbg


/*
 Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m256 _mm256_broadcast_f32x2_dbg(__m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_broadcast_f32x2
#define _mm256_broadcast_f32x2 _mm256_broadcast_f32x2_dbg


/*
 Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_broadcast_f32x2_dbg(__m256 src, __mmask8 k, __m128 a)
{
  int32_t src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_broadcast_f32x2
#define _mm256_mask_broadcast_f32x2 _mm256_mask_broadcast_f32x2_dbg


/*
 Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_broadcast_f32x2_dbg(__mmask8 k, __m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_broadcast_f32x2
#define _mm256_maskz_broadcast_f32x2 _mm256_maskz_broadcast_f32x2_dbg


/*
 Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m512 _mm512_broadcast_f32x2_dbg(__m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_broadcast_f32x2
#define _mm512_broadcast_f32x2 _mm512_broadcast_f32x2_dbg


/*
 Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_broadcast_f32x2_dbg(__m512 src, __mmask16 k, __m128 a)
{
  int32_t src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_broadcast_f32x2
#define _mm512_mask_broadcast_f32x2 _mm512_mask_broadcast_f32x2_dbg


/*
 Broadcast the lower 2 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_broadcast_f32x2_dbg(__mmask16 k, __m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_f32x2
#define _mm512_maskz_broadcast_f32x2 _mm512_maskz_broadcast_f32x2_dbg


/*
 Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m256 _mm256_broadcast_f32x4_dbg(__m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    dst_vec[j] = a_vec[n];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_broadcast_f32x4
#define _mm256_broadcast_f32x4 _mm256_broadcast_f32x4_dbg


/*
 Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_broadcast_f32x4_dbg(__m256 src, __mmask8 k, __m128 a)
{
  int32_t src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_broadcast_f32x4
#define _mm256_mask_broadcast_f32x4 _mm256_mask_broadcast_f32x4_dbg


/*
 Broadcast the 4 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_broadcast_f32x4_dbg(__mmask8 k, __m128 a)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_broadcast_f32x4
#define _mm256_maskz_broadcast_f32x4 _mm256_maskz_broadcast_f32x4_dbg


/*
 Broadcast the 8 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m512 _mm512_broadcast_f32x8_dbg(__m256 a)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 8);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_broadcast_f32x8
#define _mm512_broadcast_f32x8 _mm512_broadcast_f32x8_dbg


/*
 Broadcast the 8 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_broadcast_f32x8_dbg(__m512 src, __mmask16 k, __m256 a)
{
  int32_t src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 8);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_broadcast_f32x8
#define _mm512_mask_broadcast_f32x8 _mm512_mask_broadcast_f32x8_dbg


/*
 Broadcast the 8 packed single-precision (32-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_broadcast_f32x8_dbg(__mmask16 k, __m256 a)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 8);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_f32x8
#define _mm512_maskz_broadcast_f32x8 _mm512_maskz_broadcast_f32x8_dbg


/*
 Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m256d _mm256_broadcast_f64x2_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_broadcast_f64x2
#define _mm256_broadcast_f64x2 _mm256_broadcast_f64x2_dbg


/*
 Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_broadcast_f64x2_dbg(__m256d src, __mmask8 k, __m128d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_broadcast_f64x2
#define _mm256_mask_broadcast_f64x2 _mm256_mask_broadcast_f64x2_dbg


/*
 Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_broadcast_f64x2_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_broadcast_f64x2
#define _mm256_maskz_broadcast_f64x2 _mm256_maskz_broadcast_f64x2_dbg


/*
 Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst".
*/
static inline __m512d _mm512_broadcast_f64x2_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_broadcast_f64x2
#define _mm512_broadcast_f64x2 _mm512_broadcast_f64x2_dbg


/*
 Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_broadcast_f64x2_dbg(__m512d src, __mmask8 k, __m128d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_broadcast_f64x2
#define _mm512_mask_broadcast_f64x2 _mm512_mask_broadcast_f64x2_dbg


/*
 Broadcast the 2 packed double-precision (64-bit) floating-point elements from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_broadcast_f64x2_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_f64x2
#define _mm512_maskz_broadcast_f64x2 _mm512_maskz_broadcast_f64x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst.
*/
static inline __m256i _mm256_broadcast_i32x2_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcast_i32x2
#define _mm256_broadcast_i32x2 _mm256_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_broadcast_i32x2_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_broadcast_i32x2
#define _mm256_mask_broadcast_i32x2 _mm256_mask_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_broadcast_i32x2_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_broadcast_i32x2
#define _mm256_maskz_broadcast_i32x2 _mm256_maskz_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst.
*/
static inline __m512i _mm512_broadcast_i32x2_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcast_i32x2
#define _mm512_broadcast_i32x2 _mm512_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcast_i32x2_dbg(__m512i src, __mmask16 k, __m128i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcast_i32x2
#define _mm512_mask_broadcast_i32x2 _mm512_mask_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_broadcast_i32x2_dbg(__mmask16 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_i32x2
#define _mm512_maskz_broadcast_i32x2 _mm512_maskz_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst.
*/
static inline __m128i _mm_broadcast_i32x2_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_broadcast_i32x2
#define _mm_broadcast_i32x2 _mm_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_broadcast_i32x2_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_broadcast_i32x2
#define _mm_mask_broadcast_i32x2 _mm_mask_broadcast_i32x2_dbg


/*
 Broadcast the lower 2 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_broadcast_i32x2_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_broadcast_i32x2
#define _mm_maskz_broadcast_i32x2 _mm_maskz_broadcast_i32x2_dbg


/*
 Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst".
*/
static inline __m256i _mm256_broadcast_i32x4_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    dst_vec[j] = a_vec[n];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcast_i32x4
#define _mm256_broadcast_i32x4 _mm256_broadcast_i32x4_dbg


/*
 Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_broadcast_i32x4_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_broadcast_i32x4
#define _mm256_mask_broadcast_i32x4 _mm256_mask_broadcast_i32x4_dbg


/*
 Broadcast the 4 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_broadcast_i32x4_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 4);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_broadcast_i32x4
#define _mm256_maskz_broadcast_i32x4 _mm256_maskz_broadcast_i32x4_dbg


/*
 Broadcast the 8 packed 32-bit integers from "a" to all elements of "dst".
*/
static inline __m512i _mm512_broadcast_i32x8_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 8);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcast_i32x8
#define _mm512_broadcast_i32x8 _mm512_broadcast_i32x8_dbg


/*
 Broadcast the 8 packed 32-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcast_i32x8_dbg(__m512i src, __mmask16 k, __m256i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 8);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcast_i32x8
#define _mm512_mask_broadcast_i32x8 _mm512_mask_broadcast_i32x8_dbg


/*
 Broadcast the 8 packed 32-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_broadcast_i32x8_dbg(__mmask16 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int n = (j % 8);
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_i32x8
#define _mm512_maskz_broadcast_i32x8 _mm512_maskz_broadcast_i32x8_dbg


/*
 Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst".
*/
static inline __m256i _mm256_broadcast_i64x2_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcast_i64x2
#define _mm256_broadcast_i64x2 _mm256_broadcast_i64x2_dbg


/*
 Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_broadcast_i64x2_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_broadcast_i64x2
#define _mm256_mask_broadcast_i64x2 _mm256_mask_broadcast_i64x2_dbg


/*
 Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_broadcast_i64x2_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_broadcast_i64x2
#define _mm256_maskz_broadcast_i64x2 _mm256_maskz_broadcast_i64x2_dbg


/*
 Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst".
*/
static inline __m512i _mm512_broadcast_i64x2_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    dst_vec[j] = a_vec[n];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcast_i64x2
#define _mm512_broadcast_i64x2 _mm512_broadcast_i64x2_dbg


/*
 Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcast_i64x2_dbg(__m512i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcast_i64x2
#define _mm512_mask_broadcast_i64x2 _mm512_mask_broadcast_i64x2_dbg


/*
 Broadcast the 2 packed 64-bit integers from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_broadcast_i64x2_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    int n = (j % 2);
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[n];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcast_i64x2
#define _mm512_maskz_broadcast_i64x2 _mm512_maskz_broadcast_i64x2_dbg


/*
 Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_broadcastsd_pd_dbg(__m256d src, __mmask8 k, __m128d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_broadcastsd_pd
#define _mm256_mask_broadcastsd_pd _mm256_mask_broadcastsd_pd_dbg


/*
 Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_broadcastsd_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_broadcastsd_pd
#define _mm256_maskz_broadcastsd_pd _mm256_maskz_broadcastsd_pd_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_broadcastss_ps_dbg(__m256 src, __mmask8 k, __m128 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_broadcastss_ps
#define _mm256_mask_broadcastss_ps _mm256_mask_broadcastss_ps_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_broadcastss_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_broadcastss_ps
#define _mm256_maskz_broadcastss_ps _mm256_maskz_broadcastss_ps_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_broadcastss_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_broadcastss_ps
#define _mm_mask_broadcastss_ps _mm_mask_broadcastss_ps_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_broadcastss_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_broadcastss_ps
#define _mm_maskz_broadcastss_ps _mm_maskz_broadcastss_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_cvtepi32_pd_dbg(__m256d src, __mmask8 k, __m128i a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_cvtepi32_pd
#define _mm256_mask_cvtepi32_pd _mm256_mask_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_cvtepi32_pd_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi32_pd
#define _mm256_maskz_cvtepi32_pd _mm256_maskz_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_cvtepi32_pd_dbg(__m128d src, __mmask8 k, __m128i a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_cvtepi32_pd
#define _mm_mask_cvtepi32_pd _mm_mask_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_cvtepi32_pd_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_cvtepi32_pd
#define _mm_maskz_cvtepi32_pd _mm_maskz_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_cvtepi32_ps_dbg(__m256 src, __mmask8 k, __m256i a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_cvtepi32_ps
#define _mm256_mask_cvtepi32_ps _mm256_mask_cvtepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_cvtepi32_ps_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi32_ps
#define _mm256_maskz_cvtepi32_ps _mm256_maskz_cvtepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_cvtepi32_ps_dbg(__m128 src, __mmask8 k, __m128i a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_cvtepi32_ps
#define _mm_mask_cvtepi32_ps _mm_mask_cvtepi32_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_cvtepi32_ps_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_cvtepi32_ps
#define _mm_maskz_cvtepi32_ps _mm_maskz_cvtepi32_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtpd_epi32_dbg(__m128i src, __mmask8 k, __m256d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtpd_epi32
#define _mm256_mask_cvtpd_epi32 _mm256_mask_cvtpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm256_maskz_cvtpd_epi32_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtpd_epi32
#define _mm256_maskz_cvtpd_epi32 _mm256_maskz_cvtpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtpd_epi32_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtpd_epi32
#define _mm_mask_cvtpd_epi32 _mm_mask_cvtpd_epi32_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtpd_epi32_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtpd_epi32
#define _mm_maskz_cvtpd_epi32 _mm_maskz_cvtpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm256_mask_cvtpd_ps_dbg(__m128 src, __mmask8 k, __m256d a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_mask_cvtpd_ps
#define _mm256_mask_cvtpd_ps _mm256_mask_cvtpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm256_maskz_cvtpd_ps_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_maskz_cvtpd_ps
#define _mm256_maskz_cvtpd_ps _mm256_maskz_cvtpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_cvtpd_ps_dbg(__m128 src, __mmask8 k, __m128d a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_cvtpd_ps
#define _mm_mask_cvtpd_ps _mm_mask_cvtpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_cvtpd_ps_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_cvtpd_ps
#define _mm_maskz_cvtpd_ps _mm_maskz_cvtpd_ps_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtpd_epi64_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtpd_epi64
#define _mm256_cvtpd_epi64 _mm256_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtpd_epi64_dbg(__m256i src, __mmask8 k, __m256d a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtpd_epi64
#define _mm256_mask_cvtpd_epi64 _mm256_mask_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtpd_epi64_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtpd_epi64
#define _mm256_maskz_cvtpd_epi64 _mm256_maskz_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_cvt_roundpd_epi64_dbg(__m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_Int64_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvt_roundpd_epi64
#define _mm512_cvt_roundpd_epi64 _mm512_cvt_roundpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtpd_epi64_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtpd_epi64
#define _mm512_cvtpd_epi64 _mm512_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_mask_cvt_roundpd_epi64_dbg(__m512i src, __mmask8 k, __m512d a, int rounding)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundpd_epi64
#define _mm512_mask_cvt_roundpd_epi64 _mm512_mask_cvt_roundpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtpd_epi64_dbg(__m512i src, __mmask8 k, __m512d a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtpd_epi64
#define _mm512_mask_cvtpd_epi64 _mm512_mask_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_maskz_cvt_roundpd_epi64_dbg(__mmask8 k, __m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundpd_epi64
#define _mm512_maskz_cvt_roundpd_epi64 _mm512_maskz_cvt_roundpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtpd_epi64_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtpd_epi64
#define _mm512_maskz_cvtpd_epi64 _mm512_maskz_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtpd_epi64_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtpd_epi64
#define _mm_cvtpd_epi64 _mm_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtpd_epi64_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtpd_epi64
#define _mm_mask_cvtpd_epi64 _mm_mask_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtpd_epi64_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtpd_epi64
#define _mm_maskz_cvtpd_epi64 _mm_maskz_cvtpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm256_cvtpd_epu32_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtpd_epu32
#define _mm256_cvtpd_epu32 _mm256_cvtpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtpd_epu32_dbg(__m128i src, __mmask8 k, __m256d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtpd_epu32
#define _mm256_mask_cvtpd_epu32 _mm256_mask_cvtpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm256_maskz_cvtpd_epu32_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtpd_epu32
#define _mm256_maskz_cvtpd_epu32 _mm256_maskz_cvtpd_epu32_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtpd_epu32_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
  }
  for (int j = 2; j <= 4; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtpd_epu32
#define _mm_cvtpd_epu32 _mm_cvtpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtpd_epu32_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 4; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtpd_epu32
#define _mm_mask_cvtpd_epu32 _mm_mask_cvtpd_epu32_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtpd_epu32_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 4; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtpd_epu32
#define _mm_maskz_cvtpd_epu32 _mm_maskz_cvtpd_epu32_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtpd_epu64_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtpd_epu64
#define _mm256_cvtpd_epu64 _mm256_cvtpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtpd_epu64_dbg(__m256i src, __mmask8 k, __m256d a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtpd_epu64
#define _mm256_mask_cvtpd_epu64 _mm256_mask_cvtpd_epu64_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtpd_epu64_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtpd_epu64
#define _mm256_maskz_cvtpd_epu64 _mm256_maskz_cvtpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_cvt_roundpd_epu64_dbg(__m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvt_roundpd_epu64
#define _mm512_cvt_roundpd_epu64 _mm512_cvt_roundpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtpd_epu64_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtpd_epu64
#define _mm512_cvtpd_epu64 _mm512_cvtpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_mask_cvt_roundpd_epu64_dbg(__m512i src, __mmask8 k, __m512d a, int rounding)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundpd_epu64
#define _mm512_mask_cvt_roundpd_epu64 _mm512_mask_cvt_roundpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtpd_epu64_dbg(__m512i src, __mmask8 k, __m512d a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtpd_epu64
#define _mm512_mask_cvtpd_epu64 _mm512_mask_cvtpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_maskz_cvt_roundpd_epu64_dbg(__mmask8 k, __m512d a, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundpd_epu64
#define _mm512_maskz_cvt_roundpd_epu64 _mm512_maskz_cvt_roundpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtpd_epu64_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtpd_epu64
#define _mm512_maskz_cvtpd_epu64 _mm512_maskz_cvtpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtpd_epu64_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtpd_epu64
#define _mm_cvtpd_epu64 _mm_cvtpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtpd_epu64_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtpd_epu64
#define _mm_mask_cvtpd_epu64 _mm_mask_cvtpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtpd_epu64_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtpd_epu64
#define _mm_maskz_cvtpd_epu64 _mm_maskz_cvtpd_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtps_epi32_dbg(__m256i src, __mmask8 k, __m256 a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtps_epi32
#define _mm256_mask_cvtps_epi32 _mm256_mask_cvtps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtps_epi32_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtps_epi32
#define _mm256_maskz_cvtps_epi32 _mm256_maskz_cvtps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtps_epi32_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtps_epi32
#define _mm_mask_cvtps_epi32 _mm_mask_cvtps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtps_epi32_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtps_epi32
#define _mm_maskz_cvtps_epi32 _mm_maskz_cvtps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtps_epi64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtps_epi64
#define _mm256_cvtps_epi64 _mm256_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtps_epi64_dbg(__m256i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtps_epi64
#define _mm256_mask_cvtps_epi64 _mm256_mask_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtps_epi64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtps_epi64
#define _mm256_maskz_cvtps_epi64 _mm256_maskz_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_cvt_roundps_epi64_dbg(__m256 a, int rounding)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_Int64_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvt_roundps_epi64
#define _mm512_cvt_roundps_epi64 _mm512_cvt_roundps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtps_epi64_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtps_epi64
#define _mm512_cvtps_epi64 _mm512_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	 (_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	 
*/
static inline __m512i _mm512_mask_cvt_roundps_epi64_dbg(__m512i src, __mmask8 k, __m256 a, int rounding)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundps_epi64
#define _mm512_mask_cvt_roundps_epi64 _mm512_mask_cvt_roundps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtps_epi64_dbg(__m512i src, __mmask8 k, __m256 a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtps_epi64
#define _mm512_mask_cvtps_epi64 _mm512_mask_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_maskz_cvt_roundps_epi64_dbg(__mmask8 k, __m256 a, int rounding)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundps_epi64
#define _mm512_maskz_cvt_roundps_epi64 _mm512_maskz_cvt_roundps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_cvtps_epi64_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtps_epi64
#define _mm512_maskz_cvtps_epi64 _mm512_maskz_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtps_epi64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtps_epi64
#define _mm_cvtps_epi64 _mm_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtps_epi64_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtps_epi64
#define _mm_mask_cvtps_epi64 _mm_mask_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtps_epi64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtps_epi64
#define _mm_maskz_cvtps_epi64 _mm_maskz_cvtps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtps_epu32_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtps_epu32
#define _mm256_cvtps_epu32 _mm256_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_cvtps_epu32_dbg(__m256i src, __mmask8 k, __m256 a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtps_epu32
#define _mm256_mask_cvtps_epu32 _mm256_mask_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtps_epu32_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtps_epu32
#define _mm256_maskz_cvtps_epu32 _mm256_maskz_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtps_epu32_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtps_epu32
#define _mm_cvtps_epu32 _mm_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_cvtps_epu32_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtps_epu32
#define _mm_mask_cvtps_epu32 _mm_mask_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtps_epu32_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtps_epu32
#define _mm_maskz_cvtps_epu32 _mm_maskz_cvtps_epu32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtps_epu64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtps_epu64
#define _mm256_cvtps_epu64 _mm256_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m256i _mm256_mask_cvtps_epu64_dbg(__m256i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtps_epu64
#define _mm256_mask_cvtps_epu64 _mm256_mask_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtps_epu64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtps_epu64
#define _mm256_maskz_cvtps_epu64 _mm256_maskz_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_cvt_roundps_epu64_dbg(__m256 a, int rounding)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvt_roundps_epu64
#define _mm512_cvt_roundps_epu64 _mm512_cvt_roundps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtps_epu64_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtps_epu64
#define _mm512_cvtps_epu64 _mm512_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_mask_cvt_roundps_epu64_dbg(__m512i src, __mmask8 k, __m256 a, int rounding)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundps_epu64
#define _mm512_mask_cvt_roundps_epu64 _mm512_mask_cvt_roundps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m512i _mm512_mask_cvtps_epu64_dbg(__m512i src, __mmask8 k, __m256 a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtps_epu64
#define _mm512_mask_cvtps_epu64 _mm512_mask_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512i _mm512_maskz_cvt_roundps_epu64_dbg(__mmask8 k, __m256 a, int rounding)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundps_epu64
#define _mm512_maskz_cvt_roundps_epu64 _mm512_maskz_cvt_roundps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvtps_epu64_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtps_epu64
#define _mm512_maskz_cvtps_epu64 _mm512_maskz_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtps_epu64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtps_epu64
#define _mm_cvtps_epu64 _mm_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m128i _mm_mask_cvtps_epu64_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtps_epu64
#define _mm_mask_cvtps_epu64 _mm_mask_cvtps_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtps_epu64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtps_epu64
#define _mm_maskz_cvtps_epu64 _mm_maskz_cvtps_epu64_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256d _mm256_cvtepi64_pd_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_cvtepi64_pd
#define _mm256_cvtepi64_pd _mm256_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_cvtepi64_pd_dbg(__m256d src, __mmask8 k, __m256i a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_cvtepi64_pd
#define _mm256_mask_cvtepi64_pd _mm256_mask_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_cvtepi64_pd_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi64_pd
#define _mm256_maskz_cvtepi64_pd _mm256_maskz_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst". 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_cvt_roundepi64_pd_dbg(__m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_Int64_To_FP64_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvt_roundepi64_pd
#define _mm512_cvt_roundepi64_pd _mm512_cvt_roundepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m512d _mm512_cvtepi64_pd_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvtepi64_pd
#define _mm512_cvtepi64_pd _mm512_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_mask_cvt_roundepi64_pd_dbg(__m512d src, __mmask8 k, __m512i a, int rounding)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundepi64_pd
#define _mm512_mask_cvt_roundepi64_pd _mm512_mask_cvt_roundepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_cvtepi64_pd_dbg(__m512d src, __mmask8 k, __m512i a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvtepi64_pd
#define _mm512_mask_cvtepi64_pd _mm512_mask_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_maskz_cvt_roundepi64_pd_dbg(__mmask8 k, __m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundepi64_pd
#define _mm512_maskz_cvt_roundepi64_pd _mm512_maskz_cvt_roundepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_cvtepi64_pd_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi64_pd
#define _mm512_maskz_cvtepi64_pd _mm512_maskz_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128d _mm_cvtepi64_pd_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvtepi64_pd
#define _mm_cvtepi64_pd _mm_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_cvtepi64_pd_dbg(__m128d src, __mmask8 k, __m128i a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_cvtepi64_pd
#define _mm_mask_cvtepi64_pd _mm_mask_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_cvtepi64_pd_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_cvtepi64_pd
#define _mm_maskz_cvtepi64_pd _mm_maskz_cvtepi64_pd_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128 _mm256_cvtepi64_ps_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_cvtepi64_ps
#define _mm256_cvtepi64_ps _mm256_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm256_mask_cvtepi64_ps_dbg(__m128 src, __mmask8 k, __m256i a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_mask_cvtepi64_ps
#define _mm256_mask_cvtepi64_ps _mm256_mask_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm256_maskz_cvtepi64_ps_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_maskz_cvtepi64_ps
#define _mm256_maskz_cvtepi64_ps _mm256_maskz_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_cvt_roundepi64_ps_dbg(__m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_Int64_To_FP32_rounding(a_vec[j], rounding);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvt_roundepi64_ps
#define _mm512_cvt_roundepi64_ps _mm512_cvt_roundepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256 _mm512_cvtepi64_ps_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvtepi64_ps
#define _mm512_cvtepi64_ps _mm512_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_mask_cvt_roundepi64_ps_dbg(__m256 src, __mmask8 k, __m512i a, int rounding)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundepi64_ps
#define _mm512_mask_cvt_roundepi64_ps _mm512_mask_cvt_roundepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm512_mask_cvtepi64_ps_dbg(__m256 src, __mmask8 k, __m512i a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvtepi64_ps
#define _mm512_mask_cvtepi64_ps _mm512_mask_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_maskz_cvt_roundepi64_ps_dbg(__mmask8 k, __m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundepi64_ps
#define _mm512_maskz_cvt_roundepi64_ps _mm512_maskz_cvt_roundepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm512_maskz_cvtepi64_ps_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi64_ps
#define _mm512_maskz_cvtepi64_ps _mm512_maskz_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128 _mm_cvtepi64_ps_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvtepi64_ps
#define _mm_cvtepi64_ps _mm_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_cvtepi64_ps_dbg(__m128 src, __mmask8 k, __m128i a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_cvtepi64_ps
#define _mm_mask_cvtepi64_ps _mm_mask_cvtepi64_ps_dbg


/*
 Convert packed 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_cvtepi64_ps_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_Int64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_cvtepi64_ps
#define _mm_maskz_cvtepi64_ps _mm_maskz_cvtepi64_ps_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvttpd_epi32_dbg(__m128i src, __mmask8 k, __m256d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvttpd_epi32
#define _mm256_mask_cvttpd_epi32 _mm256_mask_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm256_maskz_cvttpd_epi32_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvttpd_epi32
#define _mm256_maskz_cvttpd_epi32 _mm256_maskz_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttpd_epi32_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttpd_epi32
#define _mm_mask_cvttpd_epi32 _mm_mask_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvttpd_epi32_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttpd_epi32
#define _mm_maskz_cvttpd_epi32 _mm_maskz_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm256_cvttpd_epi64_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvttpd_epi64
#define _mm256_cvttpd_epi64 _mm256_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvttpd_epi64_dbg(__m256i src, __mmask8 k, __m256d a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvttpd_epi64
#define _mm256_mask_cvttpd_epi64 _mm256_mask_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_cvttpd_epi64_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvttpd_epi64
#define _mm256_maskz_cvttpd_epi64 _mm256_maskz_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst". Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_cvtt_roundpd_epi64_dbg(__m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtt_roundpd_epi64
#define _mm512_cvtt_roundpd_epi64 _mm512_cvtt_roundpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m512i _mm512_cvttpd_epi64_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvttpd_epi64
#define _mm512_cvttpd_epi64 _mm512_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_mask_cvtt_roundpd_epi64_dbg(__m512i src, __mmask8 k, __m512d a, int sae)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundpd_epi64
#define _mm512_mask_cvtt_roundpd_epi64 _mm512_mask_cvtt_roundpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvttpd_epi64_dbg(__m512i src, __mmask8 k, __m512d a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvttpd_epi64
#define _mm512_mask_cvttpd_epi64 _mm512_mask_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512i _mm512_maskz_cvtt_roundpd_epi64_dbg(__mmask8 k, __m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundpd_epi64
#define _mm512_maskz_cvtt_roundpd_epi64 _mm512_maskz_cvtt_roundpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_cvttpd_epi64_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvttpd_epi64
#define _mm512_maskz_cvttpd_epi64 _mm512_maskz_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvttpd_epi64_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvttpd_epi64
#define _mm_cvttpd_epi64 _mm_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttpd_epi64_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttpd_epi64
#define _mm_mask_cvttpd_epi64 _mm_mask_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_cvttpd_epi64_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttpd_epi64
#define _mm_maskz_cvttpd_epi64 _mm_maskz_cvttpd_epi64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm256_cvttpd_epu32_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvttpd_epu32
#define _mm256_cvttpd_epu32 _mm256_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvttpd_epu32_dbg(__m128i src, __mmask8 k, __m256d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvttpd_epu32
#define _mm256_mask_cvttpd_epu32 _mm256_mask_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm256_maskz_cvttpd_epu32_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvttpd_epu32
#define _mm256_maskz_cvttpd_epu32 _mm256_maskz_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvttpd_epu32_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvttpd_epu32
#define _mm_cvttpd_epu32 _mm_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttpd_epu32_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttpd_epu32
#define _mm_mask_cvttpd_epu32 _mm_mask_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvttpd_epu32_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttpd_epu32
#define _mm_maskz_cvttpd_epu32 _mm_maskz_cvttpd_epu32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm256_cvttpd_epu64_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvttpd_epu64
#define _mm256_cvttpd_epu64 _mm256_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvttpd_epu64_dbg(__m256i src, __mmask8 k, __m256d a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvttpd_epu64
#define _mm256_mask_cvttpd_epu64 _mm256_mask_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_cvttpd_epu64_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvttpd_epu64
#define _mm256_maskz_cvttpd_epu64 _mm256_maskz_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst". Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_cvtt_roundpd_epu64_dbg(__m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtt_roundpd_epu64
#define _mm512_cvtt_roundpd_epu64 _mm512_cvtt_roundpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m512i _mm512_cvttpd_epu64_dbg(__m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvttpd_epu64
#define _mm512_cvttpd_epu64 _mm512_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_mask_cvtt_roundpd_epu64_dbg(__m512i src, __mmask8 k, __m512d a, int sae)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundpd_epu64
#define _mm512_mask_cvtt_roundpd_epu64 _mm512_mask_cvtt_roundpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvttpd_epu64_dbg(__m512i src, __mmask8 k, __m512d a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvttpd_epu64
#define _mm512_mask_cvttpd_epu64 _mm512_mask_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512i _mm512_maskz_cvtt_roundpd_epu64_dbg(__mmask8 k, __m512d a, int sae)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundpd_epu64
#define _mm512_maskz_cvtt_roundpd_epu64 _mm512_maskz_cvtt_roundpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_cvttpd_epu64_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvttpd_epu64
#define _mm512_maskz_cvttpd_epu64 _mm512_maskz_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvttpd_epu64_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvttpd_epu64
#define _mm_cvttpd_epu64 _mm_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttpd_epu64_dbg(__m128i src, __mmask8 k, __m128d a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttpd_epu64
#define _mm_mask_cvttpd_epu64 _mm_mask_cvttpd_epu64_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_cvttpd_epu64_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttpd_epu64
#define _mm_maskz_cvttpd_epu64 _mm_maskz_cvttpd_epu64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvttps_epi32_dbg(__m256i src, __mmask8 k, __m256 a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvttps_epi32
#define _mm256_mask_cvttps_epi32 _mm256_mask_cvttps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvttps_epi32_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_IntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvttps_epi32
#define _mm256_maskz_cvttps_epi32 _mm256_maskz_cvttps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttps_epi32_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttps_epi32
#define _mm_mask_cvttps_epi32 _mm_mask_cvttps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvttps_epi32_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_IntegerTruncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttps_epi32
#define _mm_maskz_cvttps_epi32 _mm_maskz_cvttps_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm256_cvttps_epi64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvttps_epi64
#define _mm256_cvttps_epi64 _mm256_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvttps_epi64_dbg(__m256i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvttps_epi64
#define _mm256_mask_cvttps_epi64 _mm256_mask_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvttps_epi64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvttps_epi64
#define _mm256_maskz_cvttps_epi64 _mm256_maskz_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst". Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_cvtt_roundps_epi64_dbg(__m256 a, int sae)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtt_roundps_epi64
#define _mm512_cvtt_roundps_epi64 _mm512_cvtt_roundps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m512i _mm512_cvttps_epi64_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvttps_epi64
#define _mm512_cvttps_epi64 _mm512_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512i _mm512_mask_cvtt_roundps_epi64_dbg(__m512i src, __mmask8 k, __m256 a, int sae)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundps_epi64
#define _mm512_mask_cvtt_roundps_epi64 _mm512_mask_cvtt_roundps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvttps_epi64_dbg(__m512i src, __mmask8 k, __m256 a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvttps_epi64
#define _mm512_mask_cvttps_epi64 _mm512_mask_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512i _mm512_maskz_cvtt_roundps_epi64_dbg(__mmask8 k, __m256 a, int sae)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundps_epi64
#define _mm512_maskz_cvtt_roundps_epi64 _mm512_maskz_cvtt_roundps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_cvttps_epi64_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvttps_epi64
#define _mm512_maskz_cvttps_epi64 _mm512_maskz_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvttps_epi64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvttps_epi64
#define _mm_cvttps_epi64 _mm_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttps_epi64_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttps_epi64
#define _mm_mask_cvttps_epi64 _mm_mask_cvttps_epi64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvttps_epi64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_Int64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttps_epi64
#define _mm_maskz_cvttps_epi64 _mm_maskz_cvttps_epi64_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm256_cvttps_epu32_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt32_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvttps_epu32
#define _mm256_cvttps_epu32 _mm256_cvttps_epu32_dbg

/*
 Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvttps_epu32_dbg(__m256i src, __mmask8 k, __m256 a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvttps_epu32
#define _mm256_mask_cvttps_epu32 _mm256_mask_cvttps_epu32_dbg

/*
 Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvttps_epu32_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvttps_epu32
#define _mm256_maskz_cvttps_epu32 _mm256_maskz_cvttps_epu32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvttps_epu32_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt32_Truncate(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvttps_epu32
#define _mm_cvttps_epu32 _mm_cvttps_epu32_dbg

/*
 Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttps_epu32_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttps_epu32
#define _mm_mask_cvttps_epu32 _mm_mask_cvttps_epu32_dbg

/*
 Convert packed double-precision (32-bit) floating-point elements in "a" to packed unsigned 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvttps_epu32_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP64_To_UnsignedInt32_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttps_epu32
#define _mm_maskz_cvttps_epu32 _mm_maskz_cvttps_epu32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm256_cvttps_epu64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvttps_epu64
#define _mm256_cvttps_epu64 _mm256_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvttps_epu64_dbg(__m256i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvttps_epu64
#define _mm256_mask_cvttps_epu64 _mm256_mask_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_cvttps_epu64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvttps_epu64
#define _mm256_maskz_cvttps_epu64 _mm256_maskz_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst". Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
*/
static inline __m512i _mm512_cvtt_roundps_epu64_dbg(__m256 a, int sae)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtt_roundps_epu64
#define _mm512_cvtt_roundps_epu64 _mm512_cvtt_roundps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m512i _mm512_cvttps_epu64_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvttps_epu64
#define _mm512_cvttps_epu64 _mm512_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512i _mm512_mask_cvtt_roundps_epu64_dbg(__m512i src, __mmask8 k, __m256 a, int sae)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtt_roundps_epu64
#define _mm512_mask_cvtt_roundps_epu64 _mm512_mask_cvtt_roundps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvttps_epu64_dbg(__m512i src, __mmask8 k, __m256 a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvttps_epu64
#define _mm512_mask_cvttps_epu64 _mm512_mask_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). Pass __MM_FROUND_NO_EXC to "sae" to suppress all exceptions.
	
*/
static inline __m512i _mm512_maskz_cvtt_roundps_epu64_dbg(__mmask8 k, __m256 a, int sae)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtt_roundps_epu64
#define _mm512_maskz_cvtt_roundps_epu64 _mm512_maskz_cvtt_roundps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_cvttps_epu64_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvttps_epu64
#define _mm512_maskz_cvttps_epu64 _mm512_maskz_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvttps_epu64_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvttps_epu64
#define _mm_cvttps_epu64 _mm_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvttps_epu64_dbg(__m128i src, __mmask8 k, __m128 a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvttps_epu64
#define _mm_mask_cvttps_epu64 _mm_mask_cvttps_epu64_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed unsigned 64-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_cvttps_epu64_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Convert_FP32_To_UnsignedInt64_Truncate(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvttps_epu64
#define _mm_maskz_cvttps_epu64 _mm_maskz_cvttps_epu64_dbg

/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256d _mm256_cvtepu32_pd_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_cvtepu32_pd
#define _mm256_cvtepu32_pd _mm256_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_mask_cvtepu32_pd_dbg(__m256d src, __mmask8 k, __m128i a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_cvtepu32_pd
#define _mm256_mask_cvtepu32_pd _mm256_mask_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_cvtepu32_pd_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu32_pd
#define _mm256_maskz_cvtepu32_pd _mm256_maskz_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128d _mm_cvtepu32_pd_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvtepu32_pd
#define _mm_cvtepu32_pd _mm_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_mask_cvtepu32_pd_dbg(__m128d src, __mmask8 k, __m128i a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_cvtepu32_pd
#define _mm_mask_cvtepu32_pd _mm_mask_cvtepu32_pd_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_cvtepu32_pd_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedIntegerTo_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_cvtepu32_pd
#define _mm_maskz_cvtepu32_pd _mm_maskz_cvtepu32_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256d _mm256_cvtepu64_pd_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_cvtepu64_pd
#define _mm256_cvtepu64_pd _mm256_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_mask_cvtepu64_pd_dbg(__m256d src, __mmask8 k, __m256i a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_cvtepu64_pd
#define _mm256_mask_cvtepu64_pd _mm256_mask_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_cvtepu64_pd_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu64_pd
#define _mm256_maskz_cvtepu64_pd _mm256_maskz_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_cvt_roundepu64_pd_dbg(__m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP64_rounding(a_vec[j], rounding);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvt_roundepu64_pd
#define _mm512_cvt_roundepu64_pd _mm512_cvt_roundepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m512d _mm512_cvtepu64_pd_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_cvtepu64_pd
#define _mm512_cvtepu64_pd _mm512_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_mask_cvt_roundepu64_pd_dbg(__m512d src, __mmask8 k, __m512i a, int rounding)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundepu64_pd
#define _mm512_mask_cvt_roundepu64_pd _mm512_mask_cvt_roundepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m512d _mm512_mask_cvtepu64_pd_dbg(__m512d src, __mmask8 k, __m512i a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_cvtepu64_pd
#define _mm512_mask_cvtepu64_pd _mm512_mask_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m512d _mm512_maskz_cvt_roundepu64_pd_dbg(__mmask8 k, __m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundepu64_pd
#define _mm512_maskz_cvt_roundepu64_pd _mm512_maskz_cvt_roundepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_maskz_cvtepu64_pd_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu64_pd
#define _mm512_maskz_cvtepu64_pd _mm512_maskz_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128d _mm_cvtepu64_pd_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_cvtepu64_pd
#define _mm_cvtepu64_pd _mm_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_mask_cvtepu64_pd_dbg(__m128d src, __mmask8 k, __m128i a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_cvtepu64_pd
#define _mm_mask_cvtepu64_pd _mm_mask_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_cvtepu64_pd_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_cvtepu64_pd
#define _mm_maskz_cvtepu64_pd _mm_maskz_cvtepu64_pd_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128 _mm256_cvtepu64_ps_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_cvtepu64_ps
#define _mm256_cvtepu64_ps _mm256_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m128 _mm256_mask_cvtepu64_ps_dbg(__m128 src, __mmask8 k, __m256i a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_mask_cvtepu64_ps
#define _mm256_mask_cvtepu64_ps _mm256_mask_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm256_maskz_cvtepu64_ps_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_maskz_cvtepu64_ps
#define _mm256_maskz_cvtepu64_ps _mm256_maskz_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_cvt_roundepu64_ps_dbg(__m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP32_rounding(a_vec[j], rounding);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvt_roundepu64_ps
#define _mm512_cvt_roundepu64_ps _mm512_cvt_roundepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256 _mm512_cvtepu64_ps_dbg(__m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_cvtepu64_ps
#define _mm512_cvtepu64_ps _mm512_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_mask_cvt_roundepu64_ps_dbg(__m256 src, __mmask8 k, __m512i a, int rounding)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvt_roundepu64_ps
#define _mm512_mask_cvt_roundepu64_ps _mm512_mask_cvt_roundepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m256 _mm512_mask_cvtepu64_ps_dbg(__m256 src, __mmask8 k, __m512i a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_cvtepu64_ps
#define _mm512_mask_cvtepu64_ps _mm512_mask_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm512_maskz_cvt_roundepu64_ps_dbg(__mmask8 k, __m512i a, int rounding)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32_rounding(a_vec[j], rounding);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvt_roundepu64_ps
#define _mm512_maskz_cvt_roundepu64_ps _mm512_maskz_cvt_roundepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm512_maskz_cvtepu64_ps_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu64_ps
#define _mm512_maskz_cvtepu64_ps _mm512_maskz_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128 _mm_cvtepu64_ps_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_cvtepu64_ps
#define _mm_cvtepu64_ps _mm_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
	
*/
static inline __m128 _mm_mask_cvtepu64_ps_dbg(__m128 src, __mmask8 k, __m128i a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_cvtepu64_ps
#define _mm_mask_cvtepu64_ps _mm_mask_cvtepu64_ps_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_cvtepu64_ps_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertUnsignedInt64_To_FP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_cvtepu64_ps
#define _mm_maskz_cvtepu64_ps _mm_maskz_cvtepu64_ps_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_div_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_div_pd
#define _mm256_mask_div_pd _mm256_mask_div_pd_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_div_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_div_pd
#define _mm256_maskz_div_pd _mm256_maskz_div_pd_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_div_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_div_pd
#define _mm_mask_div_pd _mm_mask_div_pd_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_div_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_div_pd
#define _mm_maskz_div_pd _mm_maskz_div_pd_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_div_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_div_ps
#define _mm256_mask_div_ps _mm256_mask_div_ps_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_div_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_div_ps
#define _mm256_maskz_div_ps _mm256_maskz_div_ps_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_div_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_div_ps
#define _mm_mask_div_ps _mm_mask_div_ps_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_div_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] / b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_div_ps
#define _mm_maskz_div_ps _mm_maskz_div_ps_dbg


/*
 Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_mask_expand_pd_dbg(__m256d src, __mmask8 k, __m256d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_expand_pd
#define _mm256_mask_expand_pd _mm256_mask_expand_pd_dbg


/*
 Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_expand_pd_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_expand_pd
#define _mm256_maskz_expand_pd _mm256_maskz_expand_pd_dbg


/*
 Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128d _mm_mask_expand_pd_dbg(__m128d src, __mmask8 k, __m128d a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_expand_pd
#define _mm_mask_expand_pd _mm_mask_expand_pd_dbg


/*
 Load contiguous active double-precision (64-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_expand_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_expand_pd
#define _mm_maskz_expand_pd _mm_maskz_expand_pd_dbg


/*
 Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_mask_expand_ps_dbg(__m256 src, __mmask8 k, __m256 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_expand_ps
#define _mm256_mask_expand_ps _mm256_mask_expand_ps_dbg


/*
 Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_expand_ps_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_expand_ps
#define _mm256_maskz_expand_ps _mm256_maskz_expand_ps_dbg


/*
 Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128 _mm_mask_expand_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_expand_ps
#define _mm_mask_expand_ps _mm_mask_expand_ps_dbg


/*
 Load contiguous active single-precision (32-bit) floating-point elements from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_expand_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_expand_ps
#define _mm_maskz_expand_ps _mm_maskz_expand_ps_dbg

/*
 Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm256_mask_extractf32x4_ps_dbg(__m128 src, __mmask8 k, __m256 a, int imm8)
{
  float tmp_vec[4];
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  __m128 a_vec[2];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_ps(tmp_vec, (__m128)a_vec[0]);
break;
    case 1:
    _mm_storeu_ps(tmp_vec, (__m128)a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_mask_extractf32x4_ps
#define _mm256_mask_extractf32x4_ps _mm256_mask_extractf32x4_ps_dbg


/*
 Extract 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128 _mm256_maskz_extractf32x4_ps_dbg(__mmask8 k, __m256 a, int imm8)
{
  float tmp_vec[4];
  __m128 a_vec[2];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_ps(tmp_vec, (__m128)a_vec[0]);
break;
    case 1:
    _mm_storeu_ps(tmp_vec, (__m128)a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_maskz_extractf32x4_ps
#define _mm256_maskz_extractf32x4_ps _mm256_maskz_extractf32x4_ps_dbg


/*
 Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m256 _mm512_extractf32x8_ps_dbg(__m512 a, int imm8)
{
  __m256 a_vec[2];
  _mm512_storeu_ps((void*)a_vec, a);
  __m256 dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_extractf32x8_ps
#define _mm512_extractf32x8_ps _mm512_extractf32x8_ps_dbg


/*
 Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm512_mask_extractf32x8_ps_dbg(__m256 src, __mmask8 k, __m512 a, int imm8)
{
  float tmp_vec[4];
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  __m256 a_vec[2];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[8];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_ps(tmp_vec, (__m256)a_vec[0]);
break;
    case 1:
    _mm256_storeu_ps(tmp_vec, (__m256)a_vec[1]);
break;
  }
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_extractf32x8_ps
#define _mm512_mask_extractf32x8_ps _mm512_mask_extractf32x8_ps_dbg


/*
 Extract 256 bits (composed of 8 packed single-precision (32-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256 _mm512_maskz_extractf32x8_ps_dbg(__mmask8 k, __m512 a, int imm8)
{
  float tmp_vec[4];
  __m256 a_vec[2];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[8];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_ps(tmp_vec, (__m256)a_vec[0]);
break;
    case 1:
    _mm256_storeu_ps(tmp_vec, (__m256)a_vec[1]);
break;
  }
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_extractf32x8_ps
#define _mm512_maskz_extractf32x8_ps _mm512_maskz_extractf32x8_ps_dbg


/*
 Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm256_mask_extractf64x2_pd_dbg(__m128d src, __mmask8 k, __m256d a, int imm8)
{
  double tmp_vec[2];
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  __m128d a_vec[2];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_pd(tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_pd(tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm256_mask_extractf64x2_pd
#define _mm256_mask_extractf64x2_pd _mm256_mask_extractf64x2_pd_dbg


/*
 Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128d _mm256_maskz_extractf64x2_pd_dbg(__mmask8 k, __m256d a, int imm8)
{
  double tmp_vec[2];
  __m128d a_vec[2];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_pd(tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_pd(tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm256_maskz_extractf64x2_pd
#define _mm256_maskz_extractf64x2_pd _mm256_maskz_extractf64x2_pd_dbg


/*
 Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m128d _mm512_extractf64x2_pd_dbg(__m512d a, int imm8)
{
  __m128d a_vec[4];
  _mm512_storeu_pd((void*)a_vec, a);
  __m128d dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
    case 2:
    dst_vec[0] = a_vec[2];
break;
    case 3:
    dst_vec[0] = a_vec[3];
break;
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm512_extractf64x2_pd
#define _mm512_extractf64x2_pd _mm512_extractf64x2_pd_dbg


/*
 Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm512_mask_extractf64x2_pd_dbg(__m128d src, __mmask8 k, __m512d a, int imm8)
{
  double tmp_vec[2];
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  __m128d a_vec[4];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_pd(tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_pd(tmp_vec, a_vec[1]);
break;
    case 2:
    _mm_storeu_pd(tmp_vec, a_vec[2]);
break;
    case 3:
    _mm_storeu_pd(tmp_vec, a_vec[3]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm512_mask_extractf64x2_pd
#define _mm512_mask_extractf64x2_pd _mm512_mask_extractf64x2_pd_dbg


/*
 Extract 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128d _mm512_maskz_extractf64x2_pd_dbg(__mmask8 k, __m512d a, int imm8)
{
  double tmp_vec[2];
  __m128d a_vec[4];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_pd(tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_pd(tmp_vec, a_vec[1]);
break;
    case 2:
    _mm_storeu_pd(tmp_vec, a_vec[2]);
break;
    case 3:
    _mm_storeu_pd(tmp_vec, a_vec[3]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm512_maskz_extractf64x2_pd
#define _mm512_maskz_extractf64x2_pd _mm512_maskz_extractf64x2_pd_dbg


/*
 Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_extracti32x4_epi32_dbg(__m128i src, __mmask8 k, __m256i a, int imm8)
{
  int32_t tmp_vec[4];
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  __m128i a_vec[2];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_extracti32x4_epi32
#define _mm256_mask_extracti32x4_epi32 _mm256_mask_extracti32x4_epi32_dbg


/*
 Extract 128 bits (composed of 4 packed 32-bit integers) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_extracti32x4_epi32_dbg(__mmask8 k, __m256i a, int imm8)
{
  int32_t tmp_vec[4];
  __m128i a_vec[2];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_extracti32x4_epi32
#define _mm256_maskz_extracti32x4_epi32 _mm256_maskz_extracti32x4_epi32_dbg


/*
 Extract 256 bits (composed of 8 packed 32-bit integers) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m256i _mm512_extracti32x8_epi32_dbg(__m512i a, int imm8)
{
  __m256i a_vec[2];
  _mm512_storeu_si512((void*)a_vec, a);
  __m256i dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_extracti32x8_epi32
#define _mm512_extracti32x8_epi32 _mm512_extracti32x8_epi32_dbg


/*
 Extract 256 bits (composed of 8 packed 32-bit integers) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_extracti32x8_epi32_dbg(__m256i src, __mmask8 k, __m512i a, int imm8)
{
  int32_t tmp_vec[8];
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  __m256i a_vec[2];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_si256((__m256i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm256_storeu_si256((__m256i*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_extracti32x8_epi32
#define _mm512_mask_extracti32x8_epi32 _mm512_mask_extracti32x8_epi32_dbg


/*
 Extract 256 bits (composed of 8 packed 32-bit integers) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_extracti32x8_epi32_dbg(__mmask8 k, __m512i a, int imm8)
{
  int32_t tmp_vec[8];
  __m256i a_vec[2];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[8];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm256_storeu_si256((__m256i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm256_storeu_si256((__m256i*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_extracti32x8_epi32
#define _mm512_maskz_extracti32x8_epi32 _mm512_maskz_extracti32x8_epi32_dbg


/*
 Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_extracti64x2_epi64_dbg(__m128i src, __mmask8 k, __m256i a, int imm8)
{
  int64_t tmp_vec[2];
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  __m128i a_vec[2];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_extracti64x2_epi64
#define _mm256_mask_extracti64x2_epi64 _mm256_mask_extracti64x2_epi64_dbg


/*
 Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_extracti64x2_epi64_dbg(__mmask8 k, __m256i a, int imm8)
{
  int64_t tmp_vec[2];
  __m128i a_vec[2];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[1]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_extracti64x2_epi64
#define _mm256_maskz_extracti64x2_epi64 _mm256_maskz_extracti64x2_epi64_dbg


/*
 Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the result in "dst".
*/
static inline __m128i _mm512_extracti64x2_epi64_dbg(__m512i a, int imm8)
{
  __m128i a_vec[4];
  _mm512_storeu_si512((void*)a_vec, a);
  __m128i dst_vec[1];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = a_vec[0];
break;
    case 1:
    dst_vec[0] = a_vec[1];
break;
    case 2:
    dst_vec[0] = a_vec[2];
break;
    case 3:
    dst_vec[0] = a_vec[3];
break;
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_extracti64x2_epi64
#define _mm512_extracti64x2_epi64 _mm512_extracti64x2_epi64_dbg


/*
 Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_mask_extracti64x2_epi64_dbg(__m128i src, __mmask8 k, __m512i a, int imm8)
{
  int64_t tmp_vec[2];
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  __m128i a_vec[4];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[1]);
break;
    case 2:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[2]);
break;
    case 3:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[3]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_mask_extracti64x2_epi64
#define _mm512_mask_extracti64x2_epi64 _mm512_mask_extracti64x2_epi64_dbg


/*
 Extract 128 bits (composed of 2 packed 64-bit integers) from "a", selected with "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm512_maskz_extracti64x2_epi64_dbg(__mmask8 k, __m512i a, int imm8)
{
  int64_t tmp_vec[2];
  __m128i a_vec[4];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[2];
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[0]);
break;
    case 1:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[1]);
break;
    case 2:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[2]);
break;
    case 3:
    _mm_storeu_si128((__m128i*)tmp_vec, a_vec[3]);
break;
  }
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = tmp_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm512_maskz_extracti64x2_epi64
#define _mm512_maskz_extracti64x2_epi64 _mm512_maskz_extracti64x2_epi64_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_mask3_fmadd_pd_dbg(__m256d a, __m256d b, __m256d c, __mmask8 k)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask3_fmadd_pd
#define _mm256_mask3_fmadd_pd _mm256_mask3_fmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_mask_fmadd_pd_dbg(__m256d a, __mmask8 k, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_fmadd_pd
#define _mm256_mask_fmadd_pd _mm256_mask_fmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_maskz_fmadd_pd_dbg(__mmask8 k, __m256d a, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_fmadd_pd
#define _mm256_maskz_fmadd_pd _mm256_maskz_fmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_mask3_fmadd_pd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmadd_pd
#define _mm_mask3_fmadd_pd _mm_mask3_fmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_mask_fmadd_pd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmadd_pd
#define _mm_mask_fmadd_pd _mm_mask_fmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_maskz_fmadd_pd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmadd_pd
#define _mm_maskz_fmadd_pd _mm_maskz_fmadd_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set). 
	
*/
static inline __m256 _mm256_mask3_fmadd_ps_dbg(__m256 a, __m256 b, __m256 c, __mmask8 k)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask3_fmadd_ps
#define _mm256_mask3_fmadd_ps _mm256_mask3_fmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
	
*/
static inline __m256 _mm256_mask_fmadd_ps_dbg(__m256 a, __mmask8 k, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_fmadd_ps
#define _mm256_mask_fmadd_ps _mm256_mask_fmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256 _mm256_maskz_fmadd_ps_dbg(__mmask8 k, __m256 a, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_fmadd_ps
#define _mm256_maskz_fmadd_ps _mm256_maskz_fmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask3_fmadd_ps_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmadd_ps
#define _mm_mask3_fmadd_ps _mm_mask3_fmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
	
*/
static inline __m128 _mm_mask_fmadd_ps_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmadd_ps
#define _mm_mask_fmadd_ps _mm_mask_fmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128 _mm_maskz_fmadd_ps_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmadd_ps
#define _mm_maskz_fmadd_ps _mm_maskz_fmadd_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256d _mm256_mask3_fmaddsub_pd_dbg(__m256d a, __m256d b, __m256d c, __mmask8 k)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask3_fmaddsub_pd
#define _mm256_mask3_fmaddsub_pd _mm256_mask3_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_fmaddsub_pd_dbg(__m256d a, __mmask8 k, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_fmaddsub_pd
#define _mm256_mask_fmaddsub_pd _mm256_mask_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_maskz_fmaddsub_pd_dbg(__mmask8 k, __m256d a, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_fmaddsub_pd
#define _mm256_maskz_fmaddsub_pd _mm256_maskz_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128d _mm_mask3_fmaddsub_pd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmaddsub_pd
#define _mm_mask3_fmaddsub_pd _mm_mask3_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_fmaddsub_pd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmaddsub_pd
#define _mm_mask_fmaddsub_pd _mm_mask_fmaddsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_maskz_fmaddsub_pd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmaddsub_pd
#define _mm_maskz_fmaddsub_pd _mm_maskz_fmaddsub_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256 _mm256_mask3_fmaddsub_ps_dbg(__m256 a, __m256 b, __m256 c, __mmask8 k)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask3_fmaddsub_ps
#define _mm256_mask3_fmaddsub_ps _mm256_mask3_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_fmaddsub_ps_dbg(__m256 a, __mmask8 k, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_fmaddsub_ps
#define _mm256_mask_fmaddsub_ps _mm256_mask_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256 _mm256_maskz_fmaddsub_ps_dbg(__mmask8 k, __m256 a, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_fmaddsub_ps
#define _mm256_maskz_fmaddsub_ps _mm256_maskz_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128 _mm_mask3_fmaddsub_ps_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmaddsub_ps
#define _mm_mask3_fmaddsub_ps _mm_mask3_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_fmaddsub_ps_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmaddsub_ps
#define _mm_mask_fmaddsub_ps _mm_mask_fmaddsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively add and subtract packed elements in "c" to/from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128 _mm_maskz_fmaddsub_ps_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmaddsub_ps
#define _mm_maskz_fmaddsub_ps _mm_maskz_fmaddsub_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256d _mm256_mask3_fmsub_pd_dbg(__m256d a, __m256d b, __m256d c, __mmask8 k)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask3_fmsub_pd
#define _mm256_mask3_fmsub_pd _mm256_mask3_fmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_fmsub_pd_dbg(__m256d a, __mmask8 k, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_fmsub_pd
#define _mm256_mask_fmsub_pd _mm256_mask_fmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_maskz_fmsub_pd_dbg(__mmask8 k, __m256d a, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_fmsub_pd
#define _mm256_maskz_fmsub_pd _mm256_maskz_fmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128d _mm_mask3_fmsub_pd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmsub_pd
#define _mm_mask3_fmsub_pd _mm_mask3_fmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_fmsub_pd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmsub_pd
#define _mm_mask_fmsub_pd _mm_mask_fmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_maskz_fmsub_pd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmsub_pd
#define _mm_maskz_fmsub_pd _mm_maskz_fmsub_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256 _mm256_mask3_fmsub_ps_dbg(__m256 a, __m256 b, __m256 c, __mmask8 k)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask3_fmsub_ps
#define _mm256_mask3_fmsub_ps _mm256_mask3_fmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_fmsub_ps_dbg(__m256 a, __mmask8 k, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_fmsub_ps
#define _mm256_mask_fmsub_ps _mm256_mask_fmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256 _mm256_maskz_fmsub_ps_dbg(__mmask8 k, __m256 a, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_fmsub_ps
#define _mm256_maskz_fmsub_ps _mm256_maskz_fmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128 _mm_mask3_fmsub_ps_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmsub_ps
#define _mm_mask3_fmsub_ps _mm_mask3_fmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_fmsub_ps_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmsub_ps
#define _mm_mask_fmsub_ps _mm_mask_fmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128 _mm_maskz_fmsub_ps_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmsub_ps
#define _mm_maskz_fmsub_ps _mm_maskz_fmsub_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256d _mm256_mask3_fmsubadd_pd_dbg(__m256d a, __m256d b, __m256d c, __mmask8 k)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask3_fmsubadd_pd
#define _mm256_mask3_fmsubadd_pd _mm256_mask3_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_fmsubadd_pd_dbg(__m256d a, __mmask8 k, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_fmsubadd_pd
#define _mm256_mask_fmsubadd_pd _mm256_mask_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256d _mm256_maskz_fmsubadd_pd_dbg(__mmask8 k, __m256d a, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_fmsubadd_pd
#define _mm256_maskz_fmsubadd_pd _mm256_maskz_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128d _mm_mask3_fmsubadd_pd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fmsubadd_pd
#define _mm_mask3_fmsubadd_pd _mm_mask3_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_fmsubadd_pd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fmsubadd_pd
#define _mm_mask_fmsubadd_pd _mm_mask_fmsubadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128d _mm_maskz_fmsubadd_pd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fmsubadd_pd
#define _mm_maskz_fmsubadd_pd _mm_maskz_fmsubadd_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256 _mm256_mask3_fmsubadd_ps_dbg(__m256 a, __m256 b, __m256 c, __mmask8 k)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask3_fmsubadd_ps
#define _mm256_mask3_fmsubadd_ps _mm256_mask3_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_fmsubadd_ps_dbg(__m256 a, __mmask8 k, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_fmsubadd_ps
#define _mm256_mask_fmsubadd_ps _mm256_mask_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m256 _mm256_maskz_fmsubadd_ps_dbg(__mmask8 k, __m256 a, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_fmsubadd_ps
#define _mm256_maskz_fmsubadd_ps _mm256_maskz_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128 _mm_mask3_fmsubadd_ps_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fmsubadd_ps
#define _mm_mask3_fmsubadd_ps _mm_mask3_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_fmsubadd_ps_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fmsubadd_ps
#define _mm_mask_fmsubadd_ps _mm_mask_fmsubadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", alternatively subtract and add packed elements in "c" from/to the intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
	
*/
static inline __m128 _mm_maskz_fmsubadd_ps_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (j % 2 == 0) {
        dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
      } else {
        dst_vec[j] = (a_vec[j] * b_vec[j]) - c_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fmsubadd_ps
#define _mm_maskz_fmsubadd_ps _mm_maskz_fmsubadd_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256d _mm256_mask3_fnmadd_pd_dbg(__m256d a, __m256d b, __m256d c, __mmask8 k)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask3_fnmadd_pd
#define _mm256_mask3_fnmadd_pd _mm256_mask3_fnmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m256d _mm256_mask_fnmadd_pd_dbg(__m256d a, __mmask8 k, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_fnmadd_pd
#define _mm256_mask_fnmadd_pd _mm256_mask_fnmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_maskz_fnmadd_pd_dbg(__mmask8 k, __m256d a, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_fnmadd_pd
#define _mm256_maskz_fnmadd_pd _mm256_maskz_fnmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128d _mm_mask3_fnmadd_pd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fnmadd_pd
#define _mm_mask3_fnmadd_pd _mm_mask3_fnmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m128d _mm_mask_fnmadd_pd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fnmadd_pd
#define _mm_mask_fnmadd_pd _mm_mask_fnmadd_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_maskz_fnmadd_pd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fnmadd_pd
#define _mm_maskz_fnmadd_pd _mm_maskz_fnmadd_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256 _mm256_mask3_fnmadd_ps_dbg(__m256 a, __m256 b, __m256 c, __mmask8 k)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask3_fnmadd_ps
#define _mm256_mask3_fnmadd_ps _mm256_mask3_fnmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m256 _mm256_mask_fnmadd_ps_dbg(__m256 a, __mmask8 k, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_fnmadd_ps
#define _mm256_mask_fnmadd_ps _mm256_mask_fnmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_maskz_fnmadd_ps_dbg(__mmask8 k, __m256 a, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_fnmadd_ps
#define _mm256_maskz_fnmadd_ps _mm256_maskz_fnmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128 _mm_mask3_fnmadd_ps_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fnmadd_ps
#define _mm_mask3_fnmadd_ps _mm_mask3_fnmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m128 _mm_mask_fnmadd_ps_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fnmadd_ps
#define _mm_mask_fnmadd_ps _mm_mask_fnmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the negated intermediate result to packed elements in "c", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_maskz_fnmadd_ps_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) + c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fnmadd_ps
#define _mm_maskz_fnmadd_ps _mm_maskz_fnmadd_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256d _mm256_mask3_fnmsub_pd_dbg(__m256d a, __m256d b, __m256d c, __mmask8 k)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask3_fnmsub_pd
#define _mm256_mask3_fnmsub_pd _mm256_mask3_fnmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m256d _mm256_mask_fnmsub_pd_dbg(__m256d a, __mmask8 k, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_fnmsub_pd
#define _mm256_mask_fnmsub_pd _mm256_mask_fnmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_maskz_fnmsub_pd_dbg(__mmask8 k, __m256d a, __m256d b, __m256d c)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double c_vec[4];
  _mm256_storeu_pd((double*)c_vec, c);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_fnmsub_pd
#define _mm256_maskz_fnmsub_pd _mm256_maskz_fnmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128d _mm_mask3_fnmsub_pd_dbg(__m128d a, __m128d b, __m128d c, __mmask8 k)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask3_fnmsub_pd
#define _mm_mask3_fnmsub_pd _mm_mask3_fnmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m128d _mm_mask_fnmsub_pd_dbg(__m128d a, __mmask8 k, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_fnmsub_pd
#define _mm_mask_fnmsub_pd _mm_mask_fnmsub_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_maskz_fnmsub_pd_dbg(__mmask8 k, __m128d a, __m128d b, __m128d c)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double c_vec[2];
  _mm_storeu_pd((double*)c_vec, c);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_fnmsub_pd
#define _mm_maskz_fnmsub_pd _mm_maskz_fnmsub_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m256 _mm256_mask3_fnmsub_ps_dbg(__m256 a, __m256 b, __m256 c, __mmask8 k)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask3_fnmsub_ps
#define _mm256_mask3_fnmsub_ps _mm256_mask3_fnmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m256 _mm256_mask_fnmsub_ps_dbg(__m256 a, __mmask8 k, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_fnmsub_ps
#define _mm256_mask_fnmsub_ps _mm256_mask_fnmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_fnmsub_ps_dbg(__mmask8 k, __m256 a, __m256 b, __m256 c)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float c_vec[8];
  _mm256_storeu_ps((float*)c_vec, c);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_fnmsub_ps
#define _mm256_maskz_fnmsub_ps _mm256_maskz_fnmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "c" when the corresponding mask bit is not set).  
*/
static inline __m128 _mm_mask3_fnmsub_ps_dbg(__m128 a, __m128 b, __m128 c, __mmask8 k)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = c_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask3_fnmsub_ps
#define _mm_mask3_fnmsub_ps _mm_mask3_fnmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using writemask "k" (elements are copied from "a" when the corresponding mask bit is not set).  
*/
static inline __m128 _mm_mask_fnmsub_ps_dbg(__m128 a, __mmask8 k, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_fnmsub_ps
#define _mm_mask_fnmsub_ps _mm_mask_fnmsub_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", subtract packed elements in "c" from the negated intermediate result, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_fnmsub_ps_dbg(__mmask8 k, __m128 a, __m128 b, __m128 c)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float c_vec[4];
  _mm_storeu_ps((float*)c_vec, c);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = -(a_vec[j] * b_vec[j]) - c_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_fnmsub_ps
#define _mm_maskz_fnmsub_ps _mm_maskz_fnmsub_ps_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst". This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m256d _mm256_getexp_pd_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ConvertExpFP64(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_getexp_pd
#define _mm256_getexp_pd _mm256_getexp_pd_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m256d _mm256_mask_getexp_pd_dbg(__m256d src, __mmask8 k, __m256d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_getexp_pd
#define _mm256_mask_getexp_pd _mm256_mask_getexp_pd_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m256d _mm256_maskz_getexp_pd_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_getexp_pd
#define _mm256_maskz_getexp_pd _mm256_maskz_getexp_pd_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst". This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m128d _mm_getexp_pd_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ConvertExpFP64(a_vec[j]);
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_getexp_pd
#define _mm_getexp_pd _mm_getexp_pd_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m128d _mm_mask_getexp_pd_dbg(__m128d src, __mmask8 k, __m128d a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP64(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_getexp_pd
#define _mm_mask_getexp_pd _mm_mask_getexp_pd_dbg


/*
 Convert the exponent of each packed double-precision (64-bit) floating-point element in "a" to a double-precision (64-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m128d _mm_maskz_getexp_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP64(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_getexp_pd
#define _mm_maskz_getexp_pd _mm_maskz_getexp_pd_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst". This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m256 _mm256_getexp_ps_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ConvertExpFP32(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_getexp_ps
#define _mm256_getexp_ps _mm256_getexp_ps_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m256 _mm256_mask_getexp_ps_dbg(__m256 src, __mmask8 k, __m256 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_getexp_ps
#define _mm256_mask_getexp_ps _mm256_mask_getexp_ps_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m256 _mm256_maskz_getexp_ps_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_getexp_ps
#define _mm256_maskz_getexp_ps _mm256_maskz_getexp_ps_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst". This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m128 _mm_getexp_ps_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ConvertExpFP32(a_vec[j]);
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_getexp_ps
#define _mm_getexp_ps _mm_getexp_ps_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m128 _mm_mask_getexp_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_getexp_ps
#define _mm_mask_getexp_ps _mm_mask_getexp_ps_dbg


/*
 Convert the exponent of each packed single-precision (32-bit) floating-point element in "a" to a single-precision (32-bit) floating-point number representing the integer exponent, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). This intrinsic essentially calculates "floor(log2(x))" for each element.
*/
static inline __m128 _mm_maskz_getexp_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ConvertExpFP32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_getexp_ps
#define _mm_maskz_getexp_ps _mm_maskz_getexp_ps_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_max_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_max_pd
#define _mm256_mask_max_pd _mm256_mask_max_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256d _mm256_maskz_max_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_max_pd
#define _mm256_maskz_max_pd _mm256_maskz_max_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_max_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_max_pd
#define _mm_mask_max_pd _mm_mask_max_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128d _mm_maskz_max_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_max_pd
#define _mm_maskz_max_pd _mm_maskz_max_pd_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_max_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_max_ps
#define _mm256_mask_max_ps _mm256_mask_max_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256 _mm256_maskz_max_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_max_ps
#define _mm256_maskz_max_ps _mm256_maskz_max_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_max_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_max_ps
#define _mm_mask_max_ps _mm_mask_max_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128 _mm_maskz_max_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MAX(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_max_ps
#define _mm_maskz_max_ps _mm_maskz_max_ps_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_min_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_min_pd
#define _mm256_mask_min_pd _mm256_mask_min_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256d _mm256_maskz_min_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_min_pd
#define _mm256_maskz_min_pd _mm256_maskz_min_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_min_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_min_pd
#define _mm_mask_min_pd _mm_mask_min_pd_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128d _mm_maskz_min_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_min_pd
#define _mm_maskz_min_pd _mm_maskz_min_pd_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_min_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_min_ps
#define _mm256_mask_min_ps _mm256_mask_min_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256 _mm256_maskz_min_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_min_ps
#define _mm256_maskz_min_ps _mm256_maskz_min_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_min_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_min_ps
#define _mm_mask_min_ps _mm_mask_min_ps_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128 _mm_maskz_min_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = MIN(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_min_ps
#define _mm_maskz_min_ps _mm_maskz_min_ps_dbg


/*
 Move packed double-precision (64-bit) floating-point elements from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_mov_pd_dbg(__m256d src, __mmask8 k, __m256d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_mov_pd
#define _mm256_mask_mov_pd _mm256_mask_mov_pd_dbg


/*
 Move packed double-precision (64-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_mov_pd_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_mov_pd
#define _mm256_maskz_mov_pd _mm256_maskz_mov_pd_dbg


/*
 Move packed double-precision (64-bit) floating-point elements from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_mov_pd_dbg(__m128d src, __mmask8 k, __m128d a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_mov_pd
#define _mm_mask_mov_pd _mm_mask_mov_pd_dbg


/*
 Move packed double-precision (64-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_mov_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_mov_pd
#define _mm_maskz_mov_pd _mm_maskz_mov_pd_dbg


/*
 Move packed single-precision (32-bit) floating-point elements from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_mov_ps_dbg(__m256 src, __mmask8 k, __m256 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_mov_ps
#define _mm256_mask_mov_ps _mm256_mask_mov_ps_dbg

/*
 Move packed single-precision (32-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_mov_ps_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_mov_ps
#define _mm256_maskz_mov_ps _mm256_maskz_mov_ps_dbg


/*
 Move packed single-precision (32-bit) floating-point elements from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_mov_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_mov_ps
#define _mm_mask_mov_ps _mm_mask_mov_ps_dbg

/*
 Move packed single-precision (32-bit) floating-point elements from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_mov_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_mov_ps
#define _mm_maskz_mov_ps _mm_maskz_mov_ps_dbg


/*
 Move packed 32-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mov_epi32_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mov_epi32
#define _mm256_mask_mov_epi32 _mm256_mask_mov_epi32_dbg


/*
 Move packed 32-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_mov_epi32_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mov_epi32
#define _mm256_maskz_mov_epi32 _mm256_maskz_mov_epi32_dbg


/*
 Move packed 32-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mov_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mov_epi32
#define _mm_mask_mov_epi32 _mm_mask_mov_epi32_dbg


/*
 Move packed 32-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_mov_epi32_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mov_epi32
#define _mm_maskz_mov_epi32 _mm_maskz_mov_epi32_dbg


/*
 Move packed 64-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mov_epi64_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mov_epi64
#define _mm256_mask_mov_epi64 _mm256_mask_mov_epi64_dbg


/*
 Move packed 64-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_mov_epi64_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mov_epi64
#define _mm256_maskz_mov_epi64 _mm256_maskz_mov_epi64_dbg


/*
 Move packed 64-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mov_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mov_epi64
#define _mm_mask_mov_epi64 _mm_mask_mov_epi64_dbg


/*
 Move packed 64-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_mov_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mov_epi64
#define _mm_maskz_mov_epi64 _mm_maskz_mov_epi64_dbg


/*
 Move packed 16-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mov_epi16_dbg(__m256i src, __mmask16 k, __m256i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mov_epi16
#define _mm256_mask_mov_epi16 _mm256_mask_mov_epi16_dbg


/*
 Move packed 16-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_mov_epi16_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mov_epi16
#define _mm256_maskz_mov_epi16 _mm256_maskz_mov_epi16_dbg


/*
 Move packed 16-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mov_epi16_dbg(__m512i src, __mmask32 k, __m512i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mov_epi16
#define _mm512_mask_mov_epi16 _mm512_mask_mov_epi16_dbg


/*
 Move packed 16-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_mov_epi16_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mov_epi16
#define _mm512_maskz_mov_epi16 _mm512_maskz_mov_epi16_dbg


/*
 Move packed 16-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mov_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mov_epi16
#define _mm_mask_mov_epi16 _mm_mask_mov_epi16_dbg


/*
 Move packed 16-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_mov_epi16_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mov_epi16
#define _mm_maskz_mov_epi16 _mm_maskz_mov_epi16_dbg


/*
 Move packed 8-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mov_epi8_dbg(__m256i src, __mmask32 k, __m256i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mov_epi8
#define _mm256_mask_mov_epi8 _mm256_mask_mov_epi8_dbg


/*
 Move packed 8-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_mov_epi8_dbg(__mmask32 k, __m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mov_epi8
#define _mm256_maskz_mov_epi8 _mm256_maskz_mov_epi8_dbg


/*
 Move packed 8-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mov_epi8_dbg(__m512i src, __mmask64 k, __m512i a)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mov_epi8
#define _mm512_mask_mov_epi8 _mm512_mask_mov_epi8_dbg

/*
 Move packed 8-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_mov_epi8_dbg(__mmask64 k, __m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mov_epi8
#define _mm512_maskz_mov_epi8 _mm512_maskz_mov_epi8_dbg

/*
 Move packed 8-bit integers from "a" into "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mov_epi8_dbg(__m128i src, __mmask16 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mov_epi8
#define _mm_mask_mov_epi8 _mm_mask_mov_epi8_dbg


/*
 Move packed 8-bit integers from "a" into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_mov_epi8_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mov_epi8
#define _mm_maskz_mov_epi8 _mm_maskz_mov_epi8_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_mask_mul_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_mul_pd
#define _mm256_mask_mul_pd _mm256_mask_mul_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256d _mm256_maskz_mul_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_mul_pd
#define _mm256_maskz_mul_pd _mm256_maskz_mul_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128d _mm_mask_mul_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_mul_pd
#define _mm_mask_mul_pd _mm_mask_mul_pd_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128d _mm_maskz_mul_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_mul_pd
#define _mm_maskz_mul_pd _mm_maskz_mul_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  RM.
*/
static inline __m256 _mm256_mask_mul_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_mul_ps
#define _mm256_mask_mul_ps _mm256_mask_mul_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256 _mm256_maskz_mul_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_mul_ps
#define _mm256_maskz_mul_ps _mm256_maskz_mul_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).  RM.
*/
static inline __m128 _mm_mask_mul_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_mul_ps
#define _mm_mask_mul_ps _mm_mask_mul_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128 _mm_maskz_mul_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] * b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_mul_ps
#define _mm_maskz_mul_ps _mm_maskz_mul_ps_dbg

/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_or_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  uint64_t src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_or_pd
#define _mm256_mask_or_pd _mm256_mask_or_pd_dbg

/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_or_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_or_pd
#define _mm256_maskz_or_pd _mm256_maskz_or_pd_dbg


/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_or_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b)
{
  uint64_t src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_or_pd
#define _mm512_mask_or_pd _mm512_mask_or_pd_dbg


/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512d _mm512_maskz_or_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_or_pd
#define _mm512_maskz_or_pd _mm512_maskz_or_pd_dbg

/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m512d _mm512_or_pd_dbg(__m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_or_pd
#define _mm512_or_pd _mm512_or_pd_dbg

/*
 Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_or_epi32_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] | b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_or_epi32
#define _mm512_or_epi32 _mm512_or_epi32_dbg

/*
 Compute the bitwise OR of 512 bits (representing integer data) in "a" and "b", and store the result in "dst".
*/
static inline __m512i _mm512_or_si512_dbg(__m512i a, __m512i b)
{
  return _mm512_or_epi32(a, b);
}

#undef _mm512_or_si512
#define _mm512_or_si512 _mm512_or_si512_dbg

/*
 Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the resut in "dst".

*/
static inline __m512i _mm512_or_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] | b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_or_epi64
#define _mm512_or_epi64 _mm512_or_epi64_dbg

/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_or_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  uint64_t src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_or_pd
#define _mm_mask_or_pd _mm_mask_or_pd_dbg

/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_or_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_or_pd
#define _mm_maskz_or_pd _mm_maskz_or_pd_dbg

/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_or_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  int32_t src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_or_ps
#define _mm256_mask_or_ps _mm256_mask_or_ps_dbg


/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_or_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_or_ps
#define _mm256_maskz_or_ps _mm256_maskz_or_ps_dbg


/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_or_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  int32_t src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_or_ps
#define _mm512_mask_or_ps _mm512_mask_or_ps_dbg


/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512 _mm512_maskz_or_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_or_ps
#define _mm512_maskz_or_ps _mm512_maskz_or_ps_dbg


/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m512 _mm512_or_ps_dbg(__m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_or_ps
#define _mm512_or_ps _mm512_or_ps_dbg


/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_or_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  int32_t src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_or_ps
#define _mm_mask_or_ps _mm_mask_or_ps_dbg


/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_or_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_or_ps
#define _mm_maskz_or_ps _mm_maskz_or_ps_dbg


/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_abs_epi8_dbg(__m256i src, __mmask32 k, __m256i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_abs_epi8
#define _mm256_mask_abs_epi8 _mm256_mask_abs_epi8_dbg


/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_abs_epi8_dbg(__mmask32 k, __m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_abs_epi8
#define _mm256_maskz_abs_epi8 _mm256_maskz_abs_epi8_dbg


/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m512i _mm512_abs_epi8_dbg(__m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = abs(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_abs_epi8
#define _mm512_abs_epi8 _mm512_abs_epi8_dbg

/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_abs_epi8_dbg(__m512i src, __mmask64 k, __m512i a)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_abs_epi8
#define _mm512_mask_abs_epi8 _mm512_mask_abs_epi8_dbg

/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_abs_epi8_dbg(__mmask64 k, __m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_abs_epi8
#define _mm512_maskz_abs_epi8 _mm512_maskz_abs_epi8_dbg

/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_abs_epi8_dbg(__m128i src, __mmask16 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_abs_epi8
#define _mm_mask_abs_epi8 _mm_mask_abs_epi8_dbg


/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_abs_epi8_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_abs_epi8
#define _mm_maskz_abs_epi8 _mm_maskz_abs_epi8_dbg


/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_abs_epi32_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_abs_epi32
#define _mm256_mask_abs_epi32 _mm256_mask_abs_epi32_dbg


/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_abs_epi32_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_abs_epi32
#define _mm256_maskz_abs_epi32 _mm256_maskz_abs_epi32_dbg


/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_abs_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_abs_epi32
#define _mm_mask_abs_epi32 _mm_mask_abs_epi32_dbg


/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_abs_epi32_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_abs_epi32
#define _mm_maskz_abs_epi32 _mm_maskz_abs_epi32_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m256i _mm256_abs_epi64_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = llabs(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_abs_epi64
#define _mm256_abs_epi64 _mm256_abs_epi64_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_abs_epi64_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = llabs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_abs_epi64
#define _mm256_mask_abs_epi64 _mm256_mask_abs_epi64_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_abs_epi64_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = llabs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_abs_epi64
#define _mm256_maskz_abs_epi64 _mm256_maskz_abs_epi64_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m128i _mm_abs_epi64_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = llabs(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_abs_epi64
#define _mm_abs_epi64 _mm_abs_epi64_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_abs_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = llabs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_abs_epi64
#define _mm_mask_abs_epi64 _mm_mask_abs_epi64_dbg


/*
 Compute the absolute value of packed 64-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_abs_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = llabs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_abs_epi64
#define _mm_maskz_abs_epi64 _mm_maskz_abs_epi64_dbg

/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_abs_epi16_dbg(__m256i src, __mmask16 k, __m256i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_abs_epi16
#define _mm256_mask_abs_epi16 _mm256_mask_abs_epi16_dbg


/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_abs_epi16_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_abs_epi16
#define _mm256_maskz_abs_epi16 _mm256_maskz_abs_epi16_dbg


/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m512i _mm512_abs_epi16_dbg(__m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = abs(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_abs_epi16
#define _mm512_abs_epi16 _mm512_abs_epi16_dbg


/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_abs_epi16_dbg(__m512i src, __mmask32 k, __m512i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_abs_epi16
#define _mm512_mask_abs_epi16 _mm512_mask_abs_epi16_dbg


/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_abs_epi16_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_abs_epi16
#define _mm512_maskz_abs_epi16 _mm512_maskz_abs_epi16_dbg


/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_abs_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_abs_epi16
#define _mm_mask_abs_epi16 _mm_mask_abs_epi16_dbg


/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_abs_epi16_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = abs(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_abs_epi16
#define _mm_maskz_abs_epi16 _mm_maskz_abs_epi16_dbg


/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_add_epi8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_add_epi8
#define _mm256_mask_add_epi8 _mm256_mask_add_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_add_epi8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_add_epi8
#define _mm256_maskz_add_epi8 _mm256_maskz_add_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_add_epi8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_add_epi8
#define _mm512_add_epi8 _mm512_add_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_add_epi8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_add_epi8
#define _mm512_mask_add_epi8 _mm512_mask_add_epi8_dbg

/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_add_epi8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_add_epi8
#define _mm512_maskz_add_epi8 _mm512_maskz_add_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_add_epi8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_add_epi8
#define _mm_mask_add_epi8 _mm_mask_add_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_add_epi8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_add_epi8
#define _mm_maskz_add_epi8 _mm_maskz_add_epi8_dbg


/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_add_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_add_epi32
#define _mm256_mask_add_epi32 _mm256_mask_add_epi32_dbg


/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_add_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_add_epi32
#define _mm256_maskz_add_epi32 _mm256_maskz_add_epi32_dbg


/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_add_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_add_epi32
#define _mm_mask_add_epi32 _mm_mask_add_epi32_dbg


/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_add_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_add_epi32
#define _mm_maskz_add_epi32 _mm_maskz_add_epi32_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_add_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_add_epi64
#define _mm256_mask_add_epi64 _mm256_mask_add_epi64_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_add_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_add_epi64
#define _mm256_maskz_add_epi64 _mm256_maskz_add_epi64_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_add_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_add_epi64
#define _mm_mask_add_epi64 _mm_mask_add_epi64_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_add_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_add_epi64
#define _mm_maskz_add_epi64 _mm_maskz_add_epi64_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_adds_epi8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_adds_epi8
#define _mm256_mask_adds_epi8 _mm256_mask_adds_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_adds_epi8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_adds_epi8
#define _mm256_maskz_adds_epi8 _mm256_maskz_adds_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_adds_epi8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_adds_epi8
#define _mm512_adds_epi8 _mm512_adds_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_adds_epi8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_adds_epi8
#define _mm512_mask_adds_epi8 _mm512_mask_adds_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_adds_epi8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_adds_epi8
#define _mm512_maskz_adds_epi8 _mm512_maskz_adds_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_adds_epi8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_adds_epi8
#define _mm_mask_adds_epi8 _mm_mask_adds_epi8_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_adds_epi8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_adds_epi8
#define _mm_maskz_adds_epi8 _mm_maskz_adds_epi8_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_adds_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_adds_epi16
#define _mm256_mask_adds_epi16 _mm256_mask_adds_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_adds_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_adds_epi16
#define _mm256_maskz_adds_epi16 _mm256_maskz_adds_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_adds_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_adds_epi16
#define _mm512_adds_epi16 _mm512_adds_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_adds_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_adds_epi16
#define _mm512_mask_adds_epi16 _mm512_mask_adds_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_adds_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_adds_epi16
#define _mm512_maskz_adds_epi16 _mm512_maskz_adds_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_adds_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_adds_epi16
#define _mm_mask_adds_epi16 _mm_mask_adds_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_adds_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_adds_epi16
#define _mm_maskz_adds_epi16 _mm_maskz_adds_epi16_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_adds_epu8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_adds_epu8
#define _mm256_mask_adds_epu8 _mm256_mask_adds_epu8_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_adds_epu8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_adds_epu8
#define _mm256_maskz_adds_epu8 _mm256_maskz_adds_epu8_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_adds_epu8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_adds_epu8
#define _mm512_adds_epu8 _mm512_adds_epu8_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_adds_epu8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_adds_epu8
#define _mm512_mask_adds_epu8 _mm512_mask_adds_epu8_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_adds_epu8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_adds_epu8
#define _mm512_maskz_adds_epu8 _mm512_maskz_adds_epu8_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_adds_epu8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_adds_epu8
#define _mm_mask_adds_epu8 _mm_mask_adds_epu8_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_adds_epu8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_adds_epu8
#define _mm_maskz_adds_epu8 _mm_maskz_adds_epu8_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_adds_epu16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_adds_epu16
#define _mm256_mask_adds_epu16 _mm256_mask_adds_epu16_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_adds_epu16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_adds_epu16
#define _mm256_maskz_adds_epu16 _mm256_maskz_adds_epu16_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_adds_epu16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_adds_epu16
#define _mm512_adds_epu16 _mm512_adds_epu16_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_adds_epu16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_adds_epu16
#define _mm512_mask_adds_epu16 _mm512_mask_adds_epu16_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_adds_epu16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_adds_epu16
#define _mm512_maskz_adds_epu16 _mm512_maskz_adds_epu16_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_adds_epu16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_adds_epu16
#define _mm_mask_adds_epu16 _mm_mask_adds_epu16_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_adds_epu16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_adds_epu16
#define _mm_maskz_adds_epu16 _mm_maskz_adds_epu16_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_add_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_add_epi16
#define _mm256_mask_add_epi16 _mm256_mask_add_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_add_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_add_epi16
#define _mm256_maskz_add_epi16 _mm256_maskz_add_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_add_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_add_epi16
#define _mm512_add_epi16 _mm512_add_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_add_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_add_epi16
#define _mm512_mask_add_epi16 _mm512_mask_add_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_add_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_add_epi16
#define _mm512_maskz_add_epi16 _mm512_maskz_add_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_add_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_add_epi16
#define _mm_mask_add_epi16 _mm_mask_add_epi16_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_add_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_add_epi16
#define _mm_maskz_add_epi16 _mm_maskz_add_epi16_dbg


/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_and_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_and_epi32
#define _mm256_mask_and_epi32 _mm256_mask_and_epi32_dbg


/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_and_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_and_epi32
#define _mm256_maskz_and_epi32 _mm256_maskz_and_epi32_dbg


/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_and_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_and_epi32
#define _mm_mask_and_epi32 _mm_mask_and_epi32_dbg


/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_and_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_and_epi32
#define _mm_maskz_and_epi32 _mm_maskz_and_epi32_dbg


/*
 Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_andnot_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ a_vec[j]) & b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_andnot_epi32
#define _mm256_mask_andnot_epi32 _mm256_mask_andnot_epi32_dbg


/*
 Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_andnot_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (~ a_vec[j]) & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_andnot_epi32
#define _mm256_maskz_andnot_epi32 _mm256_maskz_andnot_epi32_dbg


/*
 Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_andnot_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ a_vec[j]) & b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_andnot_epi32
#define _mm_mask_andnot_epi32 _mm_mask_andnot_epi32_dbg


/*
 Compute the bitwise NOT of packed 32-bit integers in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_andnot_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (~ a_vec[j]) & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_andnot_epi32
#define _mm_maskz_andnot_epi32 _mm_maskz_andnot_epi32_dbg


/*
 Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_andnot_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ a_vec[j]) & b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_andnot_epi64
#define _mm256_mask_andnot_epi64 _mm256_mask_andnot_epi64_dbg


/*
 Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_andnot_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (~ a_vec[j]) & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_andnot_epi64
#define _mm256_maskz_andnot_epi64 _mm256_maskz_andnot_epi64_dbg


/*
 Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_andnot_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ((~ a_vec[j]) & b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_andnot_epi64
#define _mm_mask_andnot_epi64 _mm_mask_andnot_epi64_dbg


/*
 Compute the bitwise NOT of packed 64-bit integers in "a" and then AND with "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_andnot_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (~ a_vec[j]) & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_andnot_epi64
#define _mm_maskz_andnot_epi64 _mm_maskz_andnot_epi64_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_and_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_and_epi64
#define _mm256_mask_and_epi64 _mm256_mask_and_epi64_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_and_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_and_epi64
#define _mm256_maskz_and_epi64 _mm256_maskz_and_epi64_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_and_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_and_epi64
#define _mm_mask_and_epi64 _mm_mask_and_epi64_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_and_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] & b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_and_epi64
#define _mm_maskz_and_epi64 _mm_maskz_and_epi64_dbg


/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_avg_epu8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_avg_epu8
#define _mm256_mask_avg_epu8 _mm256_mask_avg_epu8_dbg


/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_avg_epu8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_avg_epu8
#define _mm256_maskz_avg_epu8 _mm256_maskz_avg_epu8_dbg


/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_avg_epu8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_avg_epu8
#define _mm512_avg_epu8 _mm512_avg_epu8_dbg


/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_avg_epu8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_avg_epu8
#define _mm512_mask_avg_epu8 _mm512_mask_avg_epu8_dbg

/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_avg_epu8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_avg_epu8
#define _mm512_maskz_avg_epu8 _mm512_maskz_avg_epu8_dbg

/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_avg_epu8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_avg_epu8
#define _mm_mask_avg_epu8 _mm_mask_avg_epu8_dbg


/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_avg_epu8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_avg_epu8
#define _mm_maskz_avg_epu8 _mm_maskz_avg_epu8_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_avg_epu16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_avg_epu16
#define _mm256_mask_avg_epu16 _mm256_mask_avg_epu16_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_avg_epu16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_avg_epu16
#define _mm256_maskz_avg_epu16 _mm256_maskz_avg_epu16_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_avg_epu16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_avg_epu16
#define _mm512_avg_epu16 _mm512_avg_epu16_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_avg_epu16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_avg_epu16
#define _mm512_mask_avg_epu16 _mm512_mask_avg_epu16_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_avg_epu16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_avg_epu16
#define _mm512_maskz_avg_epu16 _mm512_maskz_avg_epu16_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_avg_epu16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_avg_epu16
#define _mm_mask_avg_epu16 _mm_mask_avg_epu16_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_avg_epu16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_avg_epu16
#define _mm_maskz_avg_epu16 _mm_maskz_avg_epu16_dbg


/*
 Blend packed 8-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m256i _mm256_mask_blend_epi8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_blend_epi8
#define _mm256_mask_blend_epi8 _mm256_mask_blend_epi8_dbg

/*
 Blend packed 8-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m512i _mm512_mask_blend_epi8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_blend_epi8
#define _mm512_mask_blend_epi8 _mm512_mask_blend_epi8_dbg

/*
 Blend packed 16-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m512i _mm512_mask_blend_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffffULL)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_blend_epi16
#define _mm512_mask_blend_epi16 _mm512_mask_blend_epi16_dbg

/*
 Blend packed 32-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m512i _mm512_mask_blend_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_blend_epi32
#define _mm512_mask_blend_epi32 _mm512_mask_blend_epi32_dbg

/*
 Blend packed 64-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m512i _mm512_mask_blend_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_blend_epi64
#define _mm512_mask_blend_epi64 _mm512_mask_blend_epi64_dbg

/*
 Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m512 _mm512_mask_blend_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_blend_ps
#define _mm512_mask_blend_ps _mm512_mask_blend_ps_dbg

/*
 Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m512d _mm512_mask_blend_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_blend_pd
#define _mm512_mask_blend_pd _mm512_mask_blend_pd_dbg

/*
 Blend packed 8-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m128i _mm_mask_blend_epi8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_blend_epi8
#define _mm_mask_blend_epi8 _mm_mask_blend_epi8_dbg


/*
 Blend packed 32-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m256i _mm256_mask_blend_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_blend_epi32
#define _mm256_mask_blend_epi32 _mm256_mask_blend_epi32_dbg


/*
 Blend packed 32-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m128i _mm_mask_blend_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_blend_epi32
#define _mm_mask_blend_epi32 _mm_mask_blend_epi32_dbg


/*
 Blend packed 64-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m256i _mm256_mask_blend_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_blend_epi64
#define _mm256_mask_blend_epi64 _mm256_mask_blend_epi64_dbg


/*
 Blend packed 64-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m128i _mm_mask_blend_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_blend_epi64
#define _mm_mask_blend_epi64 _mm_mask_blend_epi64_dbg


/*
 Blend packed 16-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m256i _mm256_mask_blend_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_blend_epi16
#define _mm256_mask_blend_epi16 _mm256_mask_blend_epi16_dbg

/*
 Blend packed 16-bit integers from "a" and "b" using control mask "k", and store the results in "dst".
*/
static inline __m128i _mm_mask_blend_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_blend_epi16
#define _mm_mask_blend_epi16 _mm_mask_blend_epi16_dbg


/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_broadcastb_epi8_dbg(__m256i src, __mmask32 k, __m128i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_broadcastb_epi8
#define _mm256_mask_broadcastb_epi8 _mm256_mask_broadcastb_epi8_dbg


/*
 Broadcast 8-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_set1_epi8_dbg(__m256i src, __mmask32 k, char a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_set1_epi8
#define _mm256_mask_set1_epi8 _mm256_mask_set1_epi8_dbg


/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_broadcastb_epi8_dbg(__mmask32 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_broadcastb_epi8
#define _mm256_maskz_broadcastb_epi8 _mm256_maskz_broadcastb_epi8_dbg


/*
 Broadcast 8-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_set1_epi8_dbg(__mmask32 k, char a)
{
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_set1_epi8
#define _mm256_maskz_set1_epi8 _mm256_maskz_set1_epi8_dbg


/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst".
*/
static inline __m512i _mm512_broadcastb_epi8_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcastb_epi8
#define _mm512_broadcastb_epi8 _mm512_broadcastb_epi8_dbg

/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_broadcastb_epi8_dbg(__m512i src, __mmask64 k, __m128i a)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcastb_epi8
#define _mm512_mask_broadcastb_epi8 _mm512_mask_broadcastb_epi8_dbg

/*
 Broadcast 8-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_set1_epi8_dbg(__m512i src, __mmask64 k, char a)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_set1_epi8
#define _mm512_mask_set1_epi8 _mm512_mask_set1_epi8_dbg

/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_broadcastb_epi8_dbg(__mmask64 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcastb_epi8
#define _mm512_maskz_broadcastb_epi8 _mm512_maskz_broadcastb_epi8_dbg

/*
 Broadcast 8-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_set1_epi8_dbg(__mmask64 k, char a)
{
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_set1_epi8
#define _mm512_maskz_set1_epi8 _mm512_maskz_set1_epi8_dbg

/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_broadcastb_epi8_dbg(__m128i src, __mmask16 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_broadcastb_epi8
#define _mm_mask_broadcastb_epi8 _mm_mask_broadcastb_epi8_dbg


/*
 Broadcast 8-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_set1_epi8_dbg(__m128i src, __mmask16 k, char a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_set1_epi8
#define _mm_mask_set1_epi8 _mm_mask_set1_epi8_dbg


/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_broadcastb_epi8_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_broadcastb_epi8
#define _mm_maskz_broadcastb_epi8 _mm_maskz_broadcastb_epi8_dbg


/*
 Broadcast 8-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_set1_epi8_dbg(__mmask16 k, char a)
{
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_set1_epi8
#define _mm_maskz_set1_epi8 _mm_maskz_set1_epi8_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_broadcastd_epi32_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_broadcastd_epi32
#define _mm256_mask_broadcastd_epi32 _mm256_mask_broadcastd_epi32_dbg


/*
 Broadcast 32-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_set1_epi32_dbg(__m256i src, __mmask8 k, int a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_set1_epi32
#define _mm256_mask_set1_epi32 _mm256_mask_set1_epi32_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_broadcastd_epi32_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_broadcastd_epi32
#define _mm256_maskz_broadcastd_epi32 _mm256_maskz_broadcastd_epi32_dbg


/*
 Broadcast 32-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_set1_epi32_dbg(__mmask8 k, int a)
{
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_set1_epi32
#define _mm256_maskz_set1_epi32 _mm256_maskz_set1_epi32_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_broadcastd_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_broadcastd_epi32
#define _mm_mask_broadcastd_epi32 _mm_mask_broadcastd_epi32_dbg


/*
 Broadcast 32-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_set1_epi32_dbg(__m128i src, __mmask8 k, int a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_set1_epi32
#define _mm_mask_set1_epi32 _mm_mask_set1_epi32_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_broadcastd_epi32_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_broadcastd_epi32
#define _mm_maskz_broadcastd_epi32 _mm_maskz_broadcastd_epi32_dbg


/*
 Broadcast 32-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_set1_epi32_dbg(__mmask8 k, int a)
{
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_set1_epi32
#define _mm_maskz_set1_epi32 _mm_maskz_set1_epi32_dbg


/*
 Broadcast the low 8-bits from input mask "k" to all 64-bit elements of "dst".
*/
static inline __m256i _mm256_broadcastmb_epi64_dbg(__mmask8 k)
{
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)k);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcastmb_epi64
#define _mm256_broadcastmb_epi64 _mm256_broadcastmb_epi64_dbg


/*
 Broadcast the low 8-bits from input mask "k" to all 64-bit elements of "dst".
*/
static inline __m128i _mm_broadcastmb_epi64_dbg(__mmask8 k)
{
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)k);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_broadcastmb_epi64
#define _mm_broadcastmb_epi64 _mm_broadcastmb_epi64_dbg


/*
 Broadcast the low 16-bits from input mask "k" to all 32-bit elements of "dst".
*/
static inline __m256i _mm256_broadcastmw_epi32_dbg(__mmask16 k)
{
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)k);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcastmw_epi32
#define _mm256_broadcastmw_epi32 _mm256_broadcastmw_epi32_dbg


/*
 Broadcast the low 16-bits from input mask "k" to all 32-bit elements of "dst".
*/
static inline __m128i _mm_broadcastmw_epi32_dbg(__mmask16 k)
{
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)k);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_broadcastmw_epi32
#define _mm_broadcastmw_epi32 _mm_broadcastmw_epi32_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_broadcastq_epi64_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_broadcastq_epi64
#define _mm256_mask_broadcastq_epi64 _mm256_mask_broadcastq_epi64_dbg


/*
 Broadcast 64-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_set1_epi64_dbg(__m256i src, __mmask8 k, int64_t a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_set1_epi64
#define _mm256_mask_set1_epi64 _mm256_mask_set1_epi64_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_broadcastq_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_broadcastq_epi64
#define _mm256_maskz_broadcastq_epi64 _mm256_maskz_broadcastq_epi64_dbg


/*
 Broadcast 64-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_set1_epi64_dbg(__mmask8 k, int64_t a)
{
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_set1_epi64
#define _mm256_maskz_set1_epi64 _mm256_maskz_set1_epi64_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_broadcastq_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_broadcastq_epi64
#define _mm_mask_broadcastq_epi64 _mm_mask_broadcastq_epi64_dbg


/*
 Broadcast 64-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_set1_epi64_dbg(__m128i src, __mmask8 k, int64_t a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_set1_epi64
#define _mm_mask_set1_epi64 _mm_mask_set1_epi64_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_broadcastq_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_broadcastq_epi64
#define _mm_maskz_broadcastq_epi64 _mm_maskz_broadcastq_epi64_dbg


/*
 Broadcast 64-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_set1_epi64_dbg(__mmask8 k, int64_t a)
{
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_set1_epi64
#define _mm_maskz_set1_epi64 _mm_maskz_set1_epi64_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_broadcastw_epi16_dbg(__m256i src, __mmask16 k, __m128i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_broadcastw_epi16
#define _mm256_mask_broadcastw_epi16 _mm256_mask_broadcastw_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_set1_epi16_dbg(__m256i src, __mmask16 k, short a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_set1_epi16
#define _mm256_mask_set1_epi16 _mm256_mask_set1_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_broadcastw_epi16_dbg(__mmask16 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_broadcastw_epi16
#define _mm256_maskz_broadcastw_epi16 _mm256_maskz_broadcastw_epi16_dbg


/*
 Broadcast 16-bit integer "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_set1_epi16_dbg(__mmask16 k, short a)
{
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_set1_epi16
#define _mm256_maskz_set1_epi16 _mm256_maskz_set1_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst".
*/
static inline __m512i _mm512_broadcastw_epi16_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_broadcastw_epi16
#define _mm512_broadcastw_epi16 _mm512_broadcastw_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_broadcastw_epi16_dbg(__m512i src, __mmask32 k, __m128i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_broadcastw_epi16
#define _mm512_mask_broadcastw_epi16 _mm512_mask_broadcastw_epi16_dbg


/*
 Broadcast 16-bit integer "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_set1_epi16_dbg(__m512i src, __mmask32 k, short a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_set1_epi16
#define _mm512_mask_set1_epi16 _mm512_mask_set1_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_broadcastw_epi16_dbg(__mmask32 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_broadcastw_epi16
#define _mm512_maskz_broadcastw_epi16 _mm512_maskz_broadcastw_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_set1_epi16_dbg(__mmask32 k, short a)
{
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_set1_epi16
#define _mm512_maskz_set1_epi16 _mm512_maskz_set1_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_broadcastw_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_broadcastw_epi16
#define _mm_mask_broadcastw_epi16 _mm_mask_broadcastw_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_set1_epi16_dbg(__m128i src, __mmask8 k, short a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_set1_epi16
#define _mm_mask_set1_epi16 _mm_mask_set1_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_broadcastw_epi16_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[0];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_broadcastw_epi16
#define _mm_maskz_broadcastw_epi16 _mm_maskz_broadcastw_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_set1_epi16_dbg(__mmask8 k, short a)
{
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_set1_epi16
#define _mm_maskz_set1_epi16 _mm_maskz_set1_epi16_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpeq_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epi8_mask
#define _mm256_cmpeq_epi8_mask _mm256_cmpeq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpge_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epi8_mask
#define _mm256_cmpge_epi8_mask _mm256_cmpge_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpgt_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epi8_mask
#define _mm256_cmpgt_epi8_mask _mm256_cmpgt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmple_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epi8_mask
#define _mm256_cmple_epi8_mask _mm256_cmple_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmplt_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epi8_mask
#define _mm256_cmplt_epi8_mask _mm256_cmplt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpneq_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epi8_mask
#define _mm256_cmpneq_epi8_mask _mm256_cmpneq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpeq_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epi8_mask
#define _mm256_mask_cmpeq_epi8_mask _mm256_mask_cmpeq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpge_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epi8_mask
#define _mm256_mask_cmpge_epi8_mask _mm256_mask_cmpge_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpgt_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epi8_mask
#define _mm256_mask_cmpgt_epi8_mask _mm256_mask_cmpgt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmple_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epi8_mask
#define _mm256_mask_cmple_epi8_mask _mm256_mask_cmple_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmplt_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epi8_mask
#define _mm256_mask_cmplt_epi8_mask _mm256_mask_cmplt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpneq_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epi8_mask
#define _mm256_mask_cmpneq_epi8_mask _mm256_mask_cmpneq_epi8_mask_dbg

/*
 Set each bit of mask register "k" based on the most significant bit of the corresponding packed 8-bit integer in "a"..
*/
static inline __mmask64 _mm512_movepi8_mask_dbg(__m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] & 0x80 ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_movepi8_mask
#define _mm512_movepi8_mask _mm512_movepi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpeq_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_epi8_mask
#define _mm512_cmpeq_epi8_mask _mm512_cmpeq_epi8_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask16 _mm512_cmpeq_epi32_mask_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_epi32_mask
#define _mm512_cmpeq_epi32_mask _mm512_cmpeq_epi32_mask_dbg
/*
 Compare packed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm512_cmpge_epi32_mask_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpge_epi32_mask
#define _mm512_cmpge_epi32_mask _mm512_cmpge_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm512_cmpgt_epi32_mask_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpgt_epi32_mask
#define _mm512_cmpgt_epi32_mask _mm512_cmpgt_epi32_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpge_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpge_epi8_mask
#define _mm512_cmpge_epi8_mask _mm512_cmpge_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpgt_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpgt_epi8_mask
#define _mm512_cmpgt_epi8_mask _mm512_cmpgt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmple_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmple_epi8_mask
#define _mm512_cmple_epi8_mask _mm512_cmple_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmplt_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmplt_epi8_mask
#define _mm512_cmplt_epi8_mask _mm512_cmplt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpneq_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpneq_epi8_mask
#define _mm512_cmpneq_epi8_mask _mm512_cmpneq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpeq_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpeq_epi8_mask
#define _mm512_mask_cmpeq_epi8_mask _mm512_mask_cmpeq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpge_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpge_epi8_mask
#define _mm512_mask_cmpge_epi8_mask _mm512_mask_cmpge_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpgt_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpgt_epi8_mask
#define _mm512_mask_cmpgt_epi8_mask _mm512_mask_cmpgt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmple_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmple_epi8_mask
#define _mm512_mask_cmple_epi8_mask _mm512_mask_cmple_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmplt_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmplt_epi8_mask
#define _mm512_mask_cmplt_epi8_mask _mm512_mask_cmplt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpneq_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpneq_epi8_mask
#define _mm512_mask_cmpneq_epi8_mask _mm512_mask_cmpneq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpeq_epi8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epi8_mask
#define _mm_cmpeq_epi8_mask _mm_cmpeq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpge_epi8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epi8_mask
#define _mm_cmpge_epi8_mask _mm_cmpge_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpgt_epi8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epi8_mask
#define _mm_cmpgt_epi8_mask _mm_cmpgt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmple_epi8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epi8_mask
#define _mm_cmple_epi8_mask _mm_cmple_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmplt_epi8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epi8_mask
#define _mm_cmplt_epi8_mask _mm_cmplt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpneq_epi8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epi8_mask
#define _mm_cmpneq_epi8_mask _mm_cmpneq_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpeq_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epi8_mask
#define _mm_mask_cmpeq_epi8_mask _mm_mask_cmpeq_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpge_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epi8_mask
#define _mm_mask_cmpge_epi8_mask _mm_mask_cmpge_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpgt_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epi8_mask
#define _mm_mask_cmpgt_epi8_mask _mm_mask_cmpgt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmple_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epi8_mask
#define _mm_mask_cmple_epi8_mask _mm_mask_cmple_epi8_mask_dbg

/*
 Compare packed 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmplt_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epi8_mask
#define _mm_mask_cmplt_epi8_mask _mm_mask_cmplt_epi8_mask_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpneq_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epi8_mask
#define _mm_mask_cmpneq_epi8_mask _mm_mask_cmpneq_epi8_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpeq_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epi32_mask
#define _mm256_cmpeq_epi32_mask _mm256_cmpeq_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpge_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epi32_mask
#define _mm256_cmpge_epi32_mask _mm256_cmpge_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpgt_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epi32_mask
#define _mm256_cmpgt_epi32_mask _mm256_cmpgt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmple_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epi32_mask
#define _mm256_cmple_epi32_mask _mm256_cmple_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmplt_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epi32_mask
#define _mm256_cmplt_epi32_mask _mm256_cmplt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpneq_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epi32_mask
#define _mm256_cmpneq_epi32_mask _mm256_cmpneq_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpeq_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epi32_mask
#define _mm256_mask_cmpeq_epi32_mask _mm256_mask_cmpeq_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpge_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epi32_mask
#define _mm256_mask_cmpge_epi32_mask _mm256_mask_cmpge_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpgt_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epi32_mask
#define _mm256_mask_cmpgt_epi32_mask _mm256_mask_cmpgt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmple_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epi32_mask
#define _mm256_mask_cmple_epi32_mask _mm256_mask_cmple_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmplt_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epi32_mask
#define _mm256_mask_cmplt_epi32_mask _mm256_mask_cmplt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpneq_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epi32_mask
#define _mm256_mask_cmpneq_epi32_mask _mm256_mask_cmpneq_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpeq_epi32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epi32_mask
#define _mm_cmpeq_epi32_mask _mm_cmpeq_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpge_epi32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epi32_mask
#define _mm_cmpge_epi32_mask _mm_cmpge_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpgt_epi32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epi32_mask
#define _mm_cmpgt_epi32_mask _mm_cmpgt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmple_epi32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epi32_mask
#define _mm_cmple_epi32_mask _mm_cmple_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmplt_epi32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epi32_mask
#define _mm_cmplt_epi32_mask _mm_cmplt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpneq_epi32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epi32_mask
#define _mm_cmpneq_epi32_mask _mm_cmpneq_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpeq_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epi32_mask
#define _mm_mask_cmpeq_epi32_mask _mm_mask_cmpeq_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpge_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epi32_mask
#define _mm_mask_cmpge_epi32_mask _mm_mask_cmpge_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpgt_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epi32_mask
#define _mm_mask_cmpgt_epi32_mask _mm_mask_cmpgt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmple_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epi32_mask
#define _mm_mask_cmple_epi32_mask _mm_mask_cmple_epi32_mask_dbg

/*
 Compare packed 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmplt_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epi32_mask
#define _mm_mask_cmplt_epi32_mask _mm_mask_cmplt_epi32_mask_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpneq_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epi32_mask
#define _mm_mask_cmpneq_epi32_mask _mm_mask_cmpneq_epi32_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpeq_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epi64_mask
#define _mm256_cmpeq_epi64_mask _mm256_cmpeq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpge_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epi64_mask
#define _mm256_cmpge_epi64_mask _mm256_cmpge_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpgt_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epi64_mask
#define _mm256_cmpgt_epi64_mask _mm256_cmpgt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmple_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epi64_mask
#define _mm256_cmple_epi64_mask _mm256_cmple_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmplt_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epi64_mask
#define _mm256_cmplt_epi64_mask _mm256_cmplt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpneq_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epi64_mask
#define _mm256_cmpneq_epi64_mask _mm256_cmpneq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpeq_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epi64_mask
#define _mm256_mask_cmpeq_epi64_mask _mm256_mask_cmpeq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpge_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epi64_mask
#define _mm256_mask_cmpge_epi64_mask _mm256_mask_cmpge_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpgt_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epi64_mask
#define _mm256_mask_cmpgt_epi64_mask _mm256_mask_cmpgt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmple_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epi64_mask
#define _mm256_mask_cmple_epi64_mask _mm256_mask_cmple_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmplt_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epi64_mask
#define _mm256_mask_cmplt_epi64_mask _mm256_mask_cmplt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpneq_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epi64_mask
#define _mm256_mask_cmpneq_epi64_mask _mm256_mask_cmpneq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpeq_epi64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epi64_mask
#define _mm_cmpeq_epi64_mask _mm_cmpeq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpge_epi64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epi64_mask
#define _mm_cmpge_epi64_mask _mm_cmpge_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpgt_epi64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epi64_mask
#define _mm_cmpgt_epi64_mask _mm_cmpgt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmple_epi64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epi64_mask
#define _mm_cmple_epi64_mask _mm_cmple_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmplt_epi64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epi64_mask
#define _mm_cmplt_epi64_mask _mm_cmplt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpneq_epi64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epi64_mask
#define _mm_cmpneq_epi64_mask _mm_cmpneq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpeq_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epi64_mask
#define _mm_mask_cmpeq_epi64_mask _mm_mask_cmpeq_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpge_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epi64_mask
#define _mm_mask_cmpge_epi64_mask _mm_mask_cmpge_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpgt_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epi64_mask
#define _mm_mask_cmpgt_epi64_mask _mm_mask_cmpgt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmple_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epi64_mask
#define _mm_mask_cmple_epi64_mask _mm_mask_cmple_epi64_mask_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmplt_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epi64_mask
#define _mm_mask_cmplt_epi64_mask _mm_mask_cmplt_epi64_mask_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpneq_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epi64_mask
#define _mm_mask_cmpneq_epi64_mask _mm_mask_cmpneq_epi64_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpeq_epu8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epu8_mask
#define _mm256_cmpeq_epu8_mask _mm256_cmpeq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpge_epu8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epu8_mask
#define _mm256_cmpge_epu8_mask _mm256_cmpge_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpgt_epu8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epu8_mask
#define _mm256_cmpgt_epu8_mask _mm256_cmpgt_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmple_epu8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epu8_mask
#define _mm256_cmple_epu8_mask _mm256_cmple_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmplt_epu8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epu8_mask
#define _mm256_cmplt_epu8_mask _mm256_cmplt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm256_cmpneq_epu8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epu8_mask
#define _mm256_cmpneq_epu8_mask _mm256_cmpneq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpeq_epu8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epu8_mask
#define _mm256_mask_cmpeq_epu8_mask _mm256_mask_cmpeq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpge_epu8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epu8_mask
#define _mm256_mask_cmpge_epu8_mask _mm256_mask_cmpge_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpgt_epu8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epu8_mask
#define _mm256_mask_cmpgt_epu8_mask _mm256_mask_cmpgt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmple_epu8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epu8_mask
#define _mm256_mask_cmple_epu8_mask _mm256_mask_cmple_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmplt_epu8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epu8_mask
#define _mm256_mask_cmplt_epu8_mask _mm256_mask_cmplt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm256_mask_cmpneq_epu8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epu8_mask
#define _mm256_mask_cmpneq_epu8_mask _mm256_mask_cmpneq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpeq_epu8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_epu8_mask
#define _mm512_cmpeq_epu8_mask _mm512_cmpeq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpge_epu8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpge_epu8_mask
#define _mm512_cmpge_epu8_mask _mm512_cmpge_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpgt_epu8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpgt_epu8_mask
#define _mm512_cmpgt_epu8_mask _mm512_cmpgt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmple_epu8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmple_epu8_mask
#define _mm512_cmple_epu8_mask _mm512_cmple_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmplt_epu8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmplt_epu8_mask
#define _mm512_cmplt_epu8_mask _mm512_cmplt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask64 _mm512_cmpneq_epu8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_cmpneq_epu8_mask
#define _mm512_cmpneq_epu8_mask _mm512_cmpneq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpeq_epu8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpeq_epu8_mask
#define _mm512_mask_cmpeq_epu8_mask _mm512_mask_cmpeq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpge_epu8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpge_epu8_mask
#define _mm512_mask_cmpge_epu8_mask _mm512_mask_cmpge_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpgt_epu8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpgt_epu8_mask
#define _mm512_mask_cmpgt_epu8_mask _mm512_mask_cmpgt_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmple_epu8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmple_epu8_mask
#define _mm512_mask_cmple_epu8_mask _mm512_mask_cmple_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmplt_epu8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmplt_epu8_mask
#define _mm512_mask_cmplt_epu8_mask _mm512_mask_cmplt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask64 _mm512_mask_cmpneq_epu8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpneq_epu8_mask
#define _mm512_mask_cmpneq_epu8_mask _mm512_mask_cmpneq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpeq_epu8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epu8_mask
#define _mm_cmpeq_epu8_mask _mm_cmpeq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpge_epu8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epu8_mask
#define _mm_cmpge_epu8_mask _mm_cmpge_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpgt_epu8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epu8_mask
#define _mm_cmpgt_epu8_mask _mm_cmpgt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmple_epu8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epu8_mask
#define _mm_cmple_epu8_mask _mm_cmple_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmplt_epu8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epu8_mask
#define _mm_cmplt_epu8_mask _mm_cmplt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm_cmpneq_epu8_mask_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epu8_mask
#define _mm_cmpneq_epu8_mask _mm_cmpneq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpeq_epu8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epu8_mask
#define _mm_mask_cmpeq_epu8_mask _mm_mask_cmpeq_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpge_epu8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epu8_mask
#define _mm_mask_cmpge_epu8_mask _mm_mask_cmpge_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpgt_epu8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epu8_mask
#define _mm_mask_cmpgt_epu8_mask _mm_mask_cmpgt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmple_epu8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epu8_mask
#define _mm_mask_cmple_epu8_mask _mm_mask_cmple_epu8_mask_dbg

/*
 Compare packed unsigned 8-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmplt_epu8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epu8_mask
#define _mm_mask_cmplt_epu8_mask _mm_mask_cmplt_epu8_mask_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm_mask_cmpneq_epu8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epu8_mask
#define _mm_mask_cmpneq_epu8_mask _mm_mask_cmpneq_epu8_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpeq_epu32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epu32_mask
#define _mm256_cmpeq_epu32_mask _mm256_cmpeq_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpge_epu32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epu32_mask
#define _mm256_cmpge_epu32_mask _mm256_cmpge_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpgt_epu32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epu32_mask
#define _mm256_cmpgt_epu32_mask _mm256_cmpgt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmple_epu32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epu32_mask
#define _mm256_cmple_epu32_mask _mm256_cmple_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmplt_epu32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epu32_mask
#define _mm256_cmplt_epu32_mask _mm256_cmplt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpneq_epu32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epu32_mask
#define _mm256_cmpneq_epu32_mask _mm256_cmpneq_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpeq_epu32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epu32_mask
#define _mm256_mask_cmpeq_epu32_mask _mm256_mask_cmpeq_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpge_epu32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epu32_mask
#define _mm256_mask_cmpge_epu32_mask _mm256_mask_cmpge_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpgt_epu32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epu32_mask
#define _mm256_mask_cmpgt_epu32_mask _mm256_mask_cmpgt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmple_epu32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epu32_mask
#define _mm256_mask_cmple_epu32_mask _mm256_mask_cmple_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmplt_epu32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epu32_mask
#define _mm256_mask_cmplt_epu32_mask _mm256_mask_cmplt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpneq_epu32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epu32_mask
#define _mm256_mask_cmpneq_epu32_mask _mm256_mask_cmpneq_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpeq_epu32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epu32_mask
#define _mm_cmpeq_epu32_mask _mm_cmpeq_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpge_epu32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epu32_mask
#define _mm_cmpge_epu32_mask _mm_cmpge_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpgt_epu32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epu32_mask
#define _mm_cmpgt_epu32_mask _mm_cmpgt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmple_epu32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epu32_mask
#define _mm_cmple_epu32_mask _mm_cmple_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmplt_epu32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epu32_mask
#define _mm_cmplt_epu32_mask _mm_cmplt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpneq_epu32_mask_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epu32_mask
#define _mm_cmpneq_epu32_mask _mm_cmpneq_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpeq_epu32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epu32_mask
#define _mm_mask_cmpeq_epu32_mask _mm_mask_cmpeq_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpge_epu32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epu32_mask
#define _mm_mask_cmpge_epu32_mask _mm_mask_cmpge_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpgt_epu32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epu32_mask
#define _mm_mask_cmpgt_epu32_mask _mm_mask_cmpgt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmple_epu32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epu32_mask
#define _mm_mask_cmple_epu32_mask _mm_mask_cmple_epu32_mask_dbg

/*
 Compare packed unsigned 32-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmplt_epu32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epu32_mask
#define _mm_mask_cmplt_epu32_mask _mm_mask_cmplt_epu32_mask_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpneq_epu32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epu32_mask
#define _mm_mask_cmpneq_epu32_mask _mm_mask_cmpneq_epu32_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpeq_epu64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epu64_mask
#define _mm256_cmpeq_epu64_mask _mm256_cmpeq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpge_epu64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epu64_mask
#define _mm256_cmpge_epu64_mask _mm256_cmpge_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpgt_epu64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epu64_mask
#define _mm256_cmpgt_epu64_mask _mm256_cmpgt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmple_epu64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epu64_mask
#define _mm256_cmple_epu64_mask _mm256_cmple_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmplt_epu64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epu64_mask
#define _mm256_cmplt_epu64_mask _mm256_cmplt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm256_cmpneq_epu64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epu64_mask
#define _mm256_cmpneq_epu64_mask _mm256_cmpneq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpeq_epu64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epu64_mask
#define _mm256_mask_cmpeq_epu64_mask _mm256_mask_cmpeq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpge_epu64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epu64_mask
#define _mm256_mask_cmpge_epu64_mask _mm256_mask_cmpge_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpgt_epu64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epu64_mask
#define _mm256_mask_cmpgt_epu64_mask _mm256_mask_cmpgt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmple_epu64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epu64_mask
#define _mm256_mask_cmple_epu64_mask _mm256_mask_cmple_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmplt_epu64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epu64_mask
#define _mm256_mask_cmplt_epu64_mask _mm256_mask_cmplt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm256_mask_cmpneq_epu64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epu64_mask
#define _mm256_mask_cmpneq_epu64_mask _mm256_mask_cmpneq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpeq_epu64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epu64_mask
#define _mm_cmpeq_epu64_mask _mm_cmpeq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpge_epu64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epu64_mask
#define _mm_cmpge_epu64_mask _mm_cmpge_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpgt_epu64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epu64_mask
#define _mm_cmpgt_epu64_mask _mm_cmpgt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmple_epu64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epu64_mask
#define _mm_cmple_epu64_mask _mm_cmple_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmplt_epu64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epu64_mask
#define _mm_cmplt_epu64_mask _mm_cmplt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpneq_epu64_mask_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epu64_mask
#define _mm_cmpneq_epu64_mask _mm_cmpneq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpeq_epu64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epu64_mask
#define _mm_mask_cmpeq_epu64_mask _mm_mask_cmpeq_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpge_epu64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epu64_mask
#define _mm_mask_cmpge_epu64_mask _mm_mask_cmpge_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpgt_epu64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epu64_mask
#define _mm_mask_cmpgt_epu64_mask _mm_mask_cmpgt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmple_epu64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epu64_mask
#define _mm_mask_cmple_epu64_mask _mm_mask_cmple_epu64_mask_dbg

/*
 Compare packed unsigned 64-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmplt_epu64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epu64_mask
#define _mm_mask_cmplt_epu64_mask _mm_mask_cmplt_epu64_mask_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpneq_epu64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epu64_mask
#define _mm_mask_cmpneq_epu64_mask _mm_mask_cmpneq_epu64_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpeq_epu16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epu16_mask
#define _mm256_cmpeq_epu16_mask _mm256_cmpeq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpge_epu16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epu16_mask
#define _mm256_cmpge_epu16_mask _mm256_cmpge_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpgt_epu16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epu16_mask
#define _mm256_cmpgt_epu16_mask _mm256_cmpgt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmple_epu16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epu16_mask
#define _mm256_cmple_epu16_mask _mm256_cmple_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmplt_epu16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epu16_mask
#define _mm256_cmplt_epu16_mask _mm256_cmplt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpneq_epu16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epu16_mask
#define _mm256_cmpneq_epu16_mask _mm256_cmpneq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpeq_epu16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epu16_mask
#define _mm256_mask_cmpeq_epu16_mask _mm256_mask_cmpeq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpge_epu16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epu16_mask
#define _mm256_mask_cmpge_epu16_mask _mm256_mask_cmpge_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpgt_epu16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epu16_mask
#define _mm256_mask_cmpgt_epu16_mask _mm256_mask_cmpgt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmple_epu16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epu16_mask
#define _mm256_mask_cmple_epu16_mask _mm256_mask_cmple_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmplt_epu16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epu16_mask
#define _mm256_mask_cmplt_epu16_mask _mm256_mask_cmplt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpneq_epu16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epu16_mask
#define _mm256_mask_cmpneq_epu16_mask _mm256_mask_cmpneq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpeq_epu16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_epu16_mask
#define _mm512_cmpeq_epu16_mask _mm512_cmpeq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpge_epu16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpge_epu16_mask
#define _mm512_cmpge_epu16_mask _mm512_cmpge_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpgt_epu16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpgt_epu16_mask
#define _mm512_cmpgt_epu16_mask _mm512_cmpgt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmple_epu16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmple_epu16_mask
#define _mm512_cmple_epu16_mask _mm512_cmple_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmplt_epu16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmplt_epu16_mask
#define _mm512_cmplt_epu16_mask _mm512_cmplt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpneq_epu16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpneq_epu16_mask
#define _mm512_cmpneq_epu16_mask _mm512_cmpneq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpeq_epu16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpeq_epu16_mask
#define _mm512_mask_cmpeq_epu16_mask _mm512_mask_cmpeq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpge_epu16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpge_epu16_mask
#define _mm512_mask_cmpge_epu16_mask _mm512_mask_cmpge_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpgt_epu16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpgt_epu16_mask
#define _mm512_mask_cmpgt_epu16_mask _mm512_mask_cmpgt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmple_epu16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmple_epu16_mask
#define _mm512_mask_cmple_epu16_mask _mm512_mask_cmple_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmplt_epu16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmplt_epu16_mask
#define _mm512_mask_cmplt_epu16_mask _mm512_mask_cmplt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpneq_epu16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpneq_epu16_mask
#define _mm512_mask_cmpneq_epu16_mask _mm512_mask_cmpneq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpeq_epu16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epu16_mask
#define _mm_cmpeq_epu16_mask _mm_cmpeq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpge_epu16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epu16_mask
#define _mm_cmpge_epu16_mask _mm_cmpge_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpgt_epu16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epu16_mask
#define _mm_cmpgt_epu16_mask _mm_cmpgt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmple_epu16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epu16_mask
#define _mm_cmple_epu16_mask _mm_cmple_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmplt_epu16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epu16_mask
#define _mm_cmplt_epu16_mask _mm_cmplt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpneq_epu16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epu16_mask
#define _mm_cmpneq_epu16_mask _mm_cmpneq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpeq_epu16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epu16_mask
#define _mm_mask_cmpeq_epu16_mask _mm_mask_cmpeq_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpge_epu16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epu16_mask
#define _mm_mask_cmpge_epu16_mask _mm_mask_cmpge_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpgt_epu16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epu16_mask
#define _mm_mask_cmpgt_epu16_mask _mm_mask_cmpgt_epu16_mask_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmple_epu16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epu16_mask
#define _mm_mask_cmple_epu16_mask _mm_mask_cmple_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmplt_epu16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epu16_mask
#define _mm_mask_cmplt_epu16_mask _mm_mask_cmplt_epu16_mask_dbg

/*
 Compare packed unsigned 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpneq_epu16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epu16_mask
#define _mm_mask_cmpneq_epu16_mask _mm_mask_cmpneq_epu16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpeq_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpeq_epi16_mask
#define _mm256_cmpeq_epi16_mask _mm256_cmpeq_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpge_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpge_epi16_mask
#define _mm256_cmpge_epi16_mask _mm256_cmpge_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpgt_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpgt_epi16_mask
#define _mm256_cmpgt_epi16_mask _mm256_cmpgt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmple_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmple_epi16_mask
#define _mm256_cmple_epi16_mask _mm256_cmple_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmplt_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmplt_epi16_mask
#define _mm256_cmplt_epi16_mask _mm256_cmplt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask16 _mm256_cmpneq_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_cmpneq_epi16_mask
#define _mm256_cmpneq_epi16_mask _mm256_cmpneq_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpeq_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpeq_epi16_mask
#define _mm256_mask_cmpeq_epi16_mask _mm256_mask_cmpeq_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpge_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpge_epi16_mask
#define _mm256_mask_cmpge_epi16_mask _mm256_mask_cmpge_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpgt_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpgt_epi16_mask
#define _mm256_mask_cmpgt_epi16_mask _mm256_mask_cmpgt_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmple_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmple_epi16_mask
#define _mm256_mask_cmple_epi16_mask _mm256_mask_cmple_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmplt_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmplt_epi16_mask
#define _mm256_mask_cmplt_epi16_mask _mm256_mask_cmplt_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask16 _mm256_mask_cmpneq_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_cmpneq_epi16_mask
#define _mm256_mask_cmpneq_epi16_mask _mm256_mask_cmpneq_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpeq_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_epi16_mask
#define _mm512_cmpeq_epi16_mask _mm512_cmpeq_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpge_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpge_epi16_mask
#define _mm512_cmpge_epi16_mask _mm512_cmpge_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpgt_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpgt_epi16_mask
#define _mm512_cmpgt_epi16_mask _mm512_cmpgt_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmple_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmple_epi16_mask
#define _mm512_cmple_epi16_mask _mm512_cmple_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmplt_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmplt_epi16_mask
#define _mm512_cmplt_epi16_mask _mm512_cmplt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask32 _mm512_cmpneq_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpneq_epi16_mask
#define _mm512_cmpneq_epi16_mask _mm512_cmpneq_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpeq_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpeq_epi16_mask
#define _mm512_mask_cmpeq_epi16_mask _mm512_mask_cmpeq_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpge_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpge_epi16_mask
#define _mm512_mask_cmpge_epi16_mask _mm512_mask_cmpge_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpgt_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpgt_epi16_mask
#define _mm512_mask_cmpgt_epi16_mask _mm512_mask_cmpgt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmple_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmple_epi16_mask
#define _mm512_mask_cmple_epi16_mask _mm512_mask_cmple_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmplt_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmplt_epi16_mask
#define _mm512_mask_cmplt_epi16_mask _mm512_mask_cmplt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask32 _mm512_mask_cmpneq_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_cmpneq_epi16_mask
#define _mm512_mask_cmpneq_epi16_mask _mm512_mask_cmpneq_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpeq_epi16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpeq_epi16_mask
#define _mm_cmpeq_epi16_mask _mm_cmpeq_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpge_epi16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpge_epi16_mask
#define _mm_cmpge_epi16_mask _mm_cmpge_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpgt_epi16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpgt_epi16_mask
#define _mm_cmpgt_epi16_mask _mm_cmpgt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmple_epi16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmple_epi16_mask
#define _mm_cmple_epi16_mask _mm_cmple_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmplt_epi16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmplt_epi16_mask
#define _mm_cmplt_epi16_mask _mm_cmplt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k".
*/
static inline __mmask8 _mm_cmpneq_epi16_mask_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm_cmpneq_epi16_mask
#define _mm_cmpneq_epi16_mask _mm_cmpneq_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for equality, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpeq_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] == b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpeq_epi16_mask
#define _mm_mask_cmpeq_epi16_mask _mm_mask_cmpeq_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for greater-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpge_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] >= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpge_epi16_mask
#define _mm_mask_cmpge_epi16_mask _mm_mask_cmpge_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for greater-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpgt_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] > b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpgt_epi16_mask
#define _mm_mask_cmpgt_epi16_mask _mm_mask_cmpgt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for less-than-or-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmple_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] <= b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmple_epi16_mask
#define _mm_mask_cmple_epi16_mask _mm_mask_cmple_epi16_mask_dbg

/*
 Compare packed 16-bit integers in "a" and "b" for less-than, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmplt_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] < b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmplt_epi16_mask
#define _mm_mask_cmplt_epi16_mask _mm_mask_cmplt_epi16_mask_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for not-equal, and store the results in mask vector "k" using zeromask "k1" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __mmask8 _mm_mask_cmpneq_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (( a_vec[j] != b_vec[j] ) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_cmpneq_epi16_mask
#define _mm_mask_cmpneq_epi16_mask _mm_mask_cmpneq_epi16_mask_dbg


/*
 Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_expand_epi32_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_expand_epi32
#define _mm256_mask_expand_epi32 _mm256_mask_expand_epi32_dbg


/*
 Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_expand_epi32_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_expand_epi32
#define _mm256_maskz_expand_epi32 _mm256_maskz_expand_epi32_dbg


/*
 Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_expand_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_expand_epi32
#define _mm_mask_expand_epi32 _mm_mask_expand_epi32_dbg


/*
 Load contiguous active 32-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_expand_epi32_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/32];
      m = m + 32;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_expand_epi32
#define _mm_maskz_expand_epi32 _mm_maskz_expand_epi32_dbg


/*
 Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_expand_epi64_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_expand_epi64
#define _mm256_mask_expand_epi64 _mm256_mask_expand_epi64_dbg


/*
 Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_expand_epi64_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_expand_epi64
#define _mm256_maskz_expand_epi64 _mm256_maskz_expand_epi64_dbg


/*
 Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_expand_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_expand_epi64
#define _mm_mask_expand_epi64 _mm_mask_expand_epi64_dbg


/*
 Load contiguous active 64-bit integers from "a" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_expand_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[(m)/64];
      m = m + 64;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_expand_epi64
#define _mm_maskz_expand_epi64 _mm_maskz_expand_epi64_dbg


/*
 Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_mask_maddubs_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_maddubs_epi16
#define _mm256_mask_maddubs_epi16 _mm256_mask_maddubs_epi16_dbg


/*
 Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_maddubs_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_maddubs_epi16
#define _mm256_maskz_maddubs_epi16 _mm256_maskz_maddubs_epi16_dbg


/*
 Vertically multiply each unsigned 8-bit integer from "a" with the corresponding signed 8-bit integer from "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst".
	
*/
static inline __m512i _mm512_maddubs_epi16_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maddubs_epi16
#define _mm512_maddubs_epi16 _mm512_maddubs_epi16_dbg


/*
 Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_mask_maddubs_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_maddubs_epi16
#define _mm512_mask_maddubs_epi16 _mm512_mask_maddubs_epi16_dbg


/*
 Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_maddubs_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_maddubs_epi16
#define _mm512_maskz_maddubs_epi16 _mm512_maskz_maddubs_epi16_dbg


/*
 Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_mask_maddubs_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_maddubs_epi16
#define _mm_mask_maddubs_epi16 _mm_mask_maddubs_epi16_dbg


/*
 Multiply packed unsigned 8-bit integers in "a" by packed signed 8-bit integers in "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_maddubs_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_maddubs_epi16
#define _mm_maskz_maddubs_epi16 _mm_maskz_maddubs_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_mask_madd_epi16_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_madd_epi16
#define _mm256_mask_madd_epi16 _mm256_mask_madd_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_madd_epi16_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_madd_epi16
#define _mm256_maskz_madd_epi16 _mm256_maskz_madd_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst".
	
*/
static inline __m512i _mm512_madd_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_madd_epi16
#define _mm512_madd_epi16 _mm512_madd_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_mask_madd_epi16_dbg(__m512i src, __mmask16 k, __m512i a, __m512i b)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_madd_epi16
#define _mm512_mask_madd_epi16 _mm512_mask_madd_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_madd_epi16_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_madd_epi16
#define _mm512_maskz_madd_epi16 _mm512_maskz_madd_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_mask_madd_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_madd_epi16
#define _mm_mask_madd_epi16 _mm_mask_madd_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_madd_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_madd_epi16
#define _mm_maskz_madd_epi16 _mm_maskz_madd_epi16_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epi8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epi8
#define _mm256_mask_max_epi8 _mm256_mask_max_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_max_epi8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epi8
#define _mm256_maskz_max_epi8 _mm256_maskz_max_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_max_epi8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_max_epi8
#define _mm512_mask_max_epi8 _mm512_mask_max_epi8_dbg

/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_max_epi8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epi8
#define _mm512_maskz_max_epi8 _mm512_maskz_max_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst". 
*/
static inline __m512i _mm512_max_epi8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_max_epi8
#define _mm512_max_epi8 _mm512_max_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epi8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epi8
#define _mm_mask_max_epi8 _mm_mask_max_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_max_epi8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epi8
#define _mm_maskz_max_epi8 _mm_maskz_max_epi8_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epi32
#define _mm256_mask_max_epi32 _mm256_mask_max_epi32_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256i _mm256_maskz_max_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epi32
#define _mm256_maskz_max_epi32 _mm256_maskz_max_epi32_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epi32
#define _mm_mask_max_epi32 _mm_mask_max_epi32_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128i _mm_maskz_max_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epi32
#define _mm_maskz_max_epi32 _mm_maskz_max_epi32_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epi64
#define _mm256_mask_max_epi64 _mm256_mask_max_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_max_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epi64
#define _mm256_maskz_max_epi64 _mm256_maskz_max_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m256i _mm256_max_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epi64
#define _mm256_max_epi64 _mm256_max_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epi64
#define _mm_mask_max_epi64 _mm_mask_max_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_max_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epi64
#define _mm_maskz_max_epi64 _mm_maskz_max_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m128i _mm_max_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_max_epi64
#define _mm_max_epi64 _mm_max_epi64_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epi16
#define _mm256_mask_max_epi16 _mm256_mask_max_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_max_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epi16
#define _mm256_maskz_max_epi16 _mm256_maskz_max_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_max_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_max_epi16
#define _mm512_mask_max_epi16 _mm512_mask_max_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_max_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epi16
#define _mm512_maskz_max_epi16 _mm512_maskz_max_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst".
	
*/
static inline __m512i _mm512_max_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_max_epi16
#define _mm512_max_epi16 _mm512_max_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epi16
#define _mm_mask_max_epi16 _mm_mask_max_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_max_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epi16
#define _mm_maskz_max_epi16 _mm_maskz_max_epi16_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epu8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epu8
#define _mm256_mask_max_epu8 _mm256_mask_max_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_max_epu8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epu8
#define _mm256_maskz_max_epu8 _mm256_maskz_max_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_max_epu8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_max_epu8
#define _mm512_mask_max_epu8 _mm512_mask_max_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_max_epu8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epu8
#define _mm512_maskz_max_epu8 _mm512_maskz_max_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst".
	
*/
static inline __m512i _mm512_max_epu8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_max_epu8
#define _mm512_max_epu8 _mm512_max_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epu8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epu8
#define _mm_mask_max_epu8 _mm_mask_max_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_max_epu8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epu8
#define _mm_maskz_max_epu8 _mm_maskz_max_epu8_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epu32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epu32
#define _mm256_mask_max_epu32 _mm256_mask_max_epu32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256i _mm256_maskz_max_epu32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epu32
#define _mm256_maskz_max_epu32 _mm256_maskz_max_epu32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epu32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epu32
#define _mm_mask_max_epu32 _mm_mask_max_epu32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128i _mm_maskz_max_epu32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epu32
#define _mm_maskz_max_epu32 _mm_maskz_max_epu32_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epu64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epu64
#define _mm256_mask_max_epu64 _mm256_mask_max_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256i _mm256_maskz_max_epu64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epu64
#define _mm256_maskz_max_epu64 _mm256_maskz_max_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst". 
*/
static inline __m256i _mm256_max_epu64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epu64
#define _mm256_max_epu64 _mm256_max_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epu64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epu64
#define _mm_mask_max_epu64 _mm_mask_max_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128i _mm_maskz_max_epu64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epu64
#define _mm_maskz_max_epu64 _mm_maskz_max_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed maximum values in "dst". 
*/
static inline __m128i _mm_max_epu64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_max_epu64
#define _mm_max_epu64 _mm_max_epu64_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_max_epu16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_max_epu16
#define _mm256_mask_max_epu16 _mm256_mask_max_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_max_epu16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_max_epu16
#define _mm256_maskz_max_epu16 _mm256_maskz_max_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_max_epu16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_max_epu16
#define _mm512_mask_max_epu16 _mm512_mask_max_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_max_epu16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_max_epu16
#define _mm512_maskz_max_epu16 _mm512_maskz_max_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst".
	
*/
static inline __m512i _mm512_max_epu16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_max_epu16
#define _mm512_max_epu16 _mm512_max_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_max_epu16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_max_epu16
#define _mm_mask_max_epu16 _mm_mask_max_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_max_epu16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] > b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_max_epu16
#define _mm_maskz_max_epu16 _mm_maskz_max_epu16_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epi8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epi8
#define _mm256_mask_min_epi8 _mm256_mask_min_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_min_epi8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epi8
#define _mm256_maskz_min_epi8 _mm256_maskz_min_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_min_epi8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_min_epi8
#define _mm512_mask_min_epi8 _mm512_mask_min_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_min_epi8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epi8
#define _mm512_maskz_min_epi8 _mm512_maskz_min_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m512i _mm512_min_epi8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_min_epi8
#define _mm512_min_epi8 _mm512_min_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epi8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epi8
#define _mm_mask_min_epi8 _mm_mask_min_epi8_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_min_epi8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epi8
#define _mm_maskz_min_epi8 _mm_maskz_min_epi8_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epi32
#define _mm256_mask_min_epi32 _mm256_mask_min_epi32_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256i _mm256_maskz_min_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epi32
#define _mm256_maskz_min_epi32 _mm256_maskz_min_epi32_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epi32
#define _mm_mask_min_epi32 _mm_mask_min_epi32_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128i _mm_maskz_min_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epi32
#define _mm_maskz_min_epi32 _mm_maskz_min_epi32_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epi64
#define _mm256_mask_min_epi64 _mm256_mask_min_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256i _mm256_maskz_min_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epi64
#define _mm256_maskz_min_epi64 _mm256_maskz_min_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m256i _mm256_min_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epi64
#define _mm256_min_epi64 _mm256_min_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epi64
#define _mm_mask_min_epi64 _mm_mask_min_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128i _mm_maskz_min_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epi64
#define _mm_maskz_min_epi64 _mm_maskz_min_epi64_dbg


/*
 Compare packed 64-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m128i _mm_min_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_min_epi64
#define _mm_min_epi64 _mm_min_epi64_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epi16
#define _mm256_mask_min_epi16 _mm256_mask_min_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_min_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epi16
#define _mm256_maskz_min_epi16 _mm256_maskz_min_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_min_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_min_epi16
#define _mm512_mask_min_epi16 _mm512_mask_min_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_min_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epi16
#define _mm512_maskz_min_epi16 _mm512_maskz_min_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m512i _mm512_min_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_min_epi16
#define _mm512_min_epi16 _mm512_min_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epi16
#define _mm_mask_min_epi16 _mm_mask_min_epi16_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_min_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epi16
#define _mm_maskz_min_epi16 _mm_maskz_min_epi16_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epu8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epu8
#define _mm256_mask_min_epu8 _mm256_mask_min_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_min_epu8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epu8
#define _mm256_maskz_min_epu8 _mm256_maskz_min_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_min_epu8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_min_epu8
#define _mm512_mask_min_epu8 _mm512_mask_min_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_min_epu8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epu8
#define _mm512_maskz_min_epu8 _mm512_maskz_min_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m512i _mm512_min_epu8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_min_epu8
#define _mm512_min_epu8 _mm512_min_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epu8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epu8
#define _mm_mask_min_epu8 _mm_mask_min_epu8_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_min_epu8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epu8
#define _mm_maskz_min_epu8 _mm_maskz_min_epu8_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epu32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epu32
#define _mm256_mask_min_epu32 _mm256_mask_min_epu32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256i _mm256_maskz_min_epu32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epu32
#define _mm256_maskz_min_epu32 _mm256_maskz_min_epu32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epu32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epu32
#define _mm_mask_min_epu32 _mm_mask_min_epu32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128i _mm_maskz_min_epu32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epu32
#define _mm_maskz_min_epu32 _mm_maskz_min_epu32_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epu64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epu64
#define _mm256_mask_min_epu64 _mm256_mask_min_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m256i _mm256_maskz_min_epu64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epu64
#define _mm256_maskz_min_epu64 _mm256_maskz_min_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m256i _mm256_min_epu64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epu64
#define _mm256_min_epu64 _mm256_min_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epu64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epu64
#define _mm_mask_min_epu64 _mm_mask_min_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
   
*/
static inline __m128i _mm_maskz_min_epu64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epu64
#define _mm_maskz_min_epu64 _mm_maskz_min_epu64_dbg


/*
 Compare packed unsigned 64-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m128i _mm_min_epu64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_min_epu64
#define _mm_min_epu64 _mm_min_epu64_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_min_epu16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_min_epu16
#define _mm256_mask_min_epu16 _mm256_mask_min_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_min_epu16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_min_epu16
#define _mm256_maskz_min_epu16 _mm256_maskz_min_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_min_epu16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_min_epu16
#define _mm512_mask_min_epu16 _mm512_mask_min_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_min_epu16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_min_epu16
#define _mm512_maskz_min_epu16 _mm512_maskz_min_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m512i _mm512_min_epu16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_min_epu16
#define _mm512_min_epu16 _mm512_min_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_min_epu16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_min_epu16
#define _mm_mask_min_epu16 _mm_mask_min_epu16_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_min_epu16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (a_vec[j] < b_vec[j]) {
        dst_vec[j] = a_vec[j];
      } else {
        dst_vec[j] = b_vec[j];
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_min_epu16
#define _mm_maskz_min_epu16 _mm_maskz_min_epu16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtepi32_epi8_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtepi32_epi8
#define _mm256_cvtepi32_epi8 _mm256_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtepi32_epi8_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtepi32_epi8
#define _mm256_mask_cvtepi32_epi8 _mm256_mask_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtepi32_epi8_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtepi32_epi8
#define _mm256_maskz_cvtepi32_epi8 _mm256_maskz_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi32_epi8_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi32_epi8
#define _mm_cvtepi32_epi8 _mm_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi32_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi32_epi8
#define _mm_mask_cvtepi32_epi8 _mm_mask_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi32_epi8_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi32_epi8
#define _mm_maskz_cvtepi32_epi8 _mm_maskz_cvtepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtepi32_epi16_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtepi32_epi16
#define _mm256_cvtepi32_epi16 _mm256_cvtepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtepi32_epi16_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtepi32_epi16
#define _mm256_mask_cvtepi32_epi16 _mm256_mask_cvtepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtepi32_epi16_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtepi32_epi16
#define _mm256_maskz_cvtepi32_epi16 _mm256_maskz_cvtepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi32_epi16_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi32_epi16
#define _mm_cvtepi32_epi16 _mm_cvtepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi32_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi32_epi16
#define _mm_mask_cvtepi32_epi16 _mm_mask_cvtepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi32_epi16_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi32_epi16
#define _mm_maskz_cvtepi32_epi16 _mm_maskz_cvtepi32_epi16_dbg

/*
 Set each packed 8-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m256i _mm256_movm_epi8_dbg(__mmask32 k)
{
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = 0xFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_movm_epi8
#define _mm256_movm_epi8 _mm256_movm_epi8_dbg


/*
 Set each packed 8-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m512i _mm512_movm_epi8_dbg(__mmask64 k)
{
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = 0xFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_movm_epi8
#define _mm512_movm_epi8 _mm512_movm_epi8_dbg


/*
 Set each packed 8-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m128i _mm_movm_epi8_dbg(__mmask16 k)
{
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = 0xFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_movm_epi8
#define _mm_movm_epi8 _mm_movm_epi8_dbg


/*
 Set each packed 32-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m256i _mm256_movm_epi32_dbg(__mmask8 k)
{
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = 0xFFFFFFFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_movm_epi32
#define _mm256_movm_epi32 _mm256_movm_epi32_dbg


/*
 Set each packed 32-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m512i _mm512_movm_epi32_dbg(__mmask16 k)
{
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = 0xFFFFFFFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_movm_epi32
#define _mm512_movm_epi32 _mm512_movm_epi32_dbg


/*
 Set each packed 32-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m128i _mm_movm_epi32_dbg(__mmask8 k)
{
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = 0xFFFFFFFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_movm_epi32
#define _mm_movm_epi32 _mm_movm_epi32_dbg

/*
 Set each packed 64-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m256i _mm256_movm_epi64_dbg(__mmask8 k)
{
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = 0xFFFFFFFFFFFFFFFFULL;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_movm_epi64
#define _mm256_movm_epi64 _mm256_movm_epi64_dbg

/*
 Set each packed 64-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m512i _mm512_movm_epi64_dbg(__mmask8 k)
{
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = 0xFFFFFFFFFFFFFFFFULL;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_movm_epi64
#define _mm512_movm_epi64 _mm512_movm_epi64_dbg

/*
 Set each packed 64-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m128i _mm_movm_epi64_dbg(__mmask8 k)
{
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = 0xFFFFFFFFFFFFFFFFULL;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_movm_epi64
#define _mm_movm_epi64 _mm_movm_epi64_dbg

/*
 Set each packed 16-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m256i _mm256_movm_epi16_dbg(__mmask16 k)
{
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = 0xFFFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_movm_epi16
#define _mm256_movm_epi16 _mm256_movm_epi16_dbg


/*
 Set each packed 16-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m512i _mm512_movm_epi16_dbg(__mmask32 k)
{
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = 0xFFFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_movm_epi16
#define _mm512_movm_epi16 _mm512_movm_epi16_dbg


/*
 Set each packed 16-bit integer in "dst" to all ones or all zeros based on the value of the corresponding bit in "k".
*/
static inline __m128i _mm_movm_epi16_dbg(__mmask8 k)
{
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = 0xFFFF;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_movm_epi16
#define _mm_movm_epi16 _mm_movm_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtepi64_epi8_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtepi64_epi8
#define _mm256_cvtepi64_epi8 _mm256_cvtepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtepi64_epi8_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtepi64_epi8
#define _mm256_mask_cvtepi64_epi8 _mm256_mask_cvtepi64_epi8_dbg

/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtepi64_epi8_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtepi64_epi8
#define _mm256_maskz_cvtepi64_epi8 _mm256_maskz_cvtepi64_epi8_dbg

/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi64_epi8_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi64_epi8
#define _mm_cvtepi64_epi8 _mm_cvtepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi64_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi64_epi8
#define _mm_mask_cvtepi64_epi8 _mm_mask_cvtepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi64_epi8_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi64_epi8
#define _mm_maskz_cvtepi64_epi8 _mm_maskz_cvtepi64_epi8_dbg

/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtepi64_epi32_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtepi64_epi32
#define _mm256_cvtepi64_epi32 _mm256_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtepi64_epi32_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtepi64_epi32
#define _mm256_mask_cvtepi64_epi32 _mm256_mask_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtepi64_epi32_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtepi64_epi32
#define _mm256_maskz_cvtepi64_epi32 _mm256_maskz_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi64_epi32_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi64_epi32
#define _mm_cvtepi64_epi32 _mm_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi64_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi64_epi32
#define _mm_mask_cvtepi64_epi32 _mm_mask_cvtepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi64_epi32_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi64_epi32
#define _mm_maskz_cvtepi64_epi32 _mm_maskz_cvtepi64_epi32_dbg

/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtepi64_epi16_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtepi64_epi16
#define _mm256_cvtepi64_epi16 _mm256_cvtepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtepi64_epi16_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtepi64_epi16
#define _mm256_mask_cvtepi64_epi16 _mm256_mask_cvtepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtepi64_epi16_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtepi64_epi16
#define _mm256_maskz_cvtepi64_epi16 _mm256_maskz_cvtepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi64_epi16_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi64_epi16
#define _mm_cvtepi64_epi16 _mm_cvtepi64_epi16_dbg

/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi64_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi64_epi16
#define _mm_mask_cvtepi64_epi16 _mm_mask_cvtepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi64_epi16_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi64_epi16
#define _mm_maskz_cvtepi64_epi16 _mm_maskz_cvtepi64_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtsepi32_epi8_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtsepi32_epi8
#define _mm256_cvtsepi32_epi8 _mm256_cvtsepi32_epi8_dbg

/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtsepi32_epi8_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtsepi32_epi8
#define _mm256_mask_cvtsepi32_epi8 _mm256_mask_cvtsepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtsepi32_epi8_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtsepi32_epi8
#define _mm256_maskz_cvtsepi32_epi8 _mm256_maskz_cvtsepi32_epi8_dbg

/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtsepi32_epi8_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtsepi32_epi8
#define _mm_cvtsepi32_epi8 _mm_cvtsepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtsepi32_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtsepi32_epi8
#define _mm_mask_cvtsepi32_epi8 _mm_mask_cvtsepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtsepi32_epi8_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtsepi32_epi8
#define _mm_maskz_cvtsepi32_epi8 _mm_maskz_cvtsepi32_epi8_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtsepi32_epi16_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtsepi32_epi16
#define _mm256_cvtsepi32_epi16 _mm256_cvtsepi32_epi16_dbg

/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtsepi32_epi16_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtsepi32_epi16
#define _mm256_mask_cvtsepi32_epi16 _mm256_mask_cvtsepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtsepi32_epi16_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtsepi32_epi16
#define _mm256_maskz_cvtsepi32_epi16 _mm256_maskz_cvtsepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtsepi32_epi16_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtsepi32_epi16
#define _mm_cvtsepi32_epi16 _mm_cvtsepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtsepi32_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtsepi32_epi16
#define _mm_mask_cvtsepi32_epi16 _mm_mask_cvtsepi32_epi16_dbg


/*
 Convert packed 32-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtsepi32_epi16_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtsepi32_epi16
#define _mm_maskz_cvtsepi32_epi16 _mm_maskz_cvtsepi32_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtsepi64_epi8_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtsepi64_epi8
#define _mm256_cvtsepi64_epi8 _mm256_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtsepi64_epi8_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtsepi64_epi8
#define _mm256_mask_cvtsepi64_epi8 _mm256_mask_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtsepi64_epi8_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtsepi64_epi8
#define _mm256_maskz_cvtsepi64_epi8 _mm256_maskz_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtsepi64_epi8_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtsepi64_epi8
#define _mm_cvtsepi64_epi8 _mm_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtsepi64_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtsepi64_epi8
#define _mm_mask_cvtsepi64_epi8 _mm_mask_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtsepi64_epi8_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtsepi64_epi8
#define _mm_maskz_cvtsepi64_epi8 _mm_maskz_cvtsepi64_epi8_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtsepi64_epi32_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtsepi64_epi32
#define _mm256_cvtsepi64_epi32 _mm256_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtsepi64_epi32_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtsepi64_epi32
#define _mm256_mask_cvtsepi64_epi32 _mm256_mask_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtsepi64_epi32_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtsepi64_epi32
#define _mm256_maskz_cvtsepi64_epi32 _mm256_maskz_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtsepi64_epi32_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtsepi64_epi32
#define _mm_cvtsepi64_epi32 _mm_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtsepi64_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtsepi64_epi32
#define _mm_mask_cvtsepi64_epi32 _mm_mask_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 32-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtsepi64_epi32_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtsepi64_epi32
#define _mm_maskz_cvtsepi64_epi32 _mm_maskz_cvtsepi64_epi32_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtsepi64_epi16_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtsepi64_epi16
#define _mm256_cvtsepi64_epi16 _mm256_cvtsepi64_epi16_dbg

/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtsepi64_epi16_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtsepi64_epi16
#define _mm256_mask_cvtsepi64_epi16 _mm256_mask_cvtsepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtsepi64_epi16_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtsepi64_epi16
#define _mm256_maskz_cvtsepi64_epi16 _mm256_maskz_cvtsepi64_epi16_dbg

/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtsepi64_epi16_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtsepi64_epi16
#define _mm_cvtsepi64_epi16 _mm_cvtsepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtsepi64_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtsepi64_epi16
#define _mm_mask_cvtsepi64_epi16 _mm_mask_cvtsepi64_epi16_dbg


/*
 Convert packed 64-bit integers in "a" to packed 16-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtsepi64_epi16_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtsepi64_epi16
#define _mm_maskz_cvtsepi64_epi16 _mm_maskz_cvtsepi64_epi16_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst". 
*/
static inline __m128i _mm256_cvtsepi16_epi8_dbg(__m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtsepi16_epi8
#define _mm256_cvtsepi16_epi8 _mm256_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtsepi16_epi8_dbg(__m128i src, __mmask16 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtsepi16_epi8
#define _mm256_mask_cvtsepi16_epi8 _mm256_mask_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtsepi16_epi8_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtsepi16_epi8
#define _mm256_maskz_cvtsepi16_epi8 _mm256_maskz_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst". 
*/
static inline __m256i _mm512_cvtsepi16_epi8_dbg(__m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtsepi16_epi8
#define _mm512_cvtsepi16_epi8 _mm512_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtsepi16_epi8_dbg(__m256i src, __mmask32 k, __m512i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtsepi16_epi8
#define _mm512_mask_cvtsepi16_epi8 _mm512_mask_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtsepi16_epi8_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtsepi16_epi8
#define _mm512_maskz_cvtsepi16_epi8 _mm512_maskz_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst". 
*/
static inline __m128i _mm_cvtsepi16_epi8_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtsepi16_epi8
#define _mm_cvtsepi16_epi8 _mm_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtsepi16_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtsepi16_epi8
#define _mm_mask_cvtsepi16_epi8 _mm_mask_cvtsepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with signed saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtsepi16_epi8_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtsepi16_epi8
#define _mm_maskz_cvtsepi16_epi8 _mm_maskz_cvtsepi16_epi8_dbg


/*
 Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepi8_epi32_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepi8_epi32
#define _mm256_mask_cvtepi8_epi32 _mm256_mask_cvtepi8_epi32_dbg

/*
 Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtepi8_epi32_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi8_epi32
#define _mm256_maskz_cvtepi8_epi32 _mm256_maskz_cvtepi8_epi32_dbg

/*
 Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi8_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi8_epi32
#define _mm_mask_cvtepi8_epi32 _mm_mask_cvtepi8_epi32_dbg


/*
 Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi8_epi32_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi8_epi32
#define _mm_maskz_cvtepi8_epi32 _mm_maskz_cvtepi8_epi32_dbg

/*
 Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepi8_epi64_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepi8_epi64
#define _mm256_mask_cvtepi8_epi64 _mm256_mask_cvtepi8_epi64_dbg

/*
 Sign extend packed 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtepi8_epi64_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi8_epi64
#define _mm256_maskz_cvtepi8_epi64 _mm256_maskz_cvtepi8_epi64_dbg


/*
 Sign extend packed 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi8_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi8_epi64
#define _mm_mask_cvtepi8_epi64 _mm_mask_cvtepi8_epi64_dbg


/*
 Sign extend packed 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi8_epi64_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi8_epi64
#define _mm_maskz_cvtepi8_epi64 _mm_maskz_cvtepi8_epi64_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepi8_epi16_dbg(__m256i src, __mmask16 k, __m128i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepi8_epi16
#define _mm256_mask_cvtepi8_epi16 _mm256_mask_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_cvtepi8_epi16_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi8_epi16
#define _mm256_maskz_cvtepi8_epi16 _mm256_maskz_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepi8_epi16_dbg(__m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepi8_epi16
#define _mm512_cvtepi8_epi16 _mm512_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepi8_epi16_dbg(__m512i src, __mmask32 k, __m256i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepi8_epi16
#define _mm512_mask_cvtepi8_epi16 _mm512_mask_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_cvtepi8_epi16_dbg(__mmask32 k, __m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi8_epi16
#define _mm512_maskz_cvtepi8_epi16 _mm512_maskz_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi8_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi8_epi16
#define _mm_mask_cvtepi8_epi16 _mm_mask_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_cvtepi8_epi16_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi8_epi16
#define _mm_maskz_cvtepi8_epi16 _mm_maskz_cvtepi8_epi16_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepi32_epi64_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepi32_epi64
#define _mm256_mask_cvtepi32_epi64 _mm256_mask_cvtepi32_epi64_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtepi32_epi64_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi32_epi64
#define _mm256_maskz_cvtepi32_epi64 _mm256_maskz_cvtepi32_epi64_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi32_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi32_epi64
#define _mm_mask_cvtepi32_epi64 _mm_mask_cvtepi32_epi64_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtepi32_epi64_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi32_epi64
#define _mm_maskz_cvtepi32_epi64 _mm_maskz_cvtepi32_epi64_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepi16_epi32_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepi16_epi32
#define _mm256_mask_cvtepi16_epi32 _mm256_mask_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtepi16_epi32_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi16_epi32
#define _mm256_maskz_cvtepi16_epi32 _mm256_maskz_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi16_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi16_epi32
#define _mm_mask_cvtepi16_epi32 _mm_mask_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtepi16_epi32_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi16_epi32
#define _mm_maskz_cvtepi16_epi32 _mm_maskz_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepi16_epi64_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepi16_epi64
#define _mm256_mask_cvtepi16_epi64 _mm256_mask_cvtepi16_epi64_dbg


/*
 Sign extend packed 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtepi16_epi64_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepi16_epi64
#define _mm256_maskz_cvtepi16_epi64 _mm256_maskz_cvtepi16_epi64_dbg


/*
 Sign extend packed 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi16_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi16_epi64
#define _mm_mask_cvtepi16_epi64 _mm_mask_cvtepi16_epi64_dbg


/*
 Sign extend packed 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi16_epi64_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = SignExtend(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi16_epi64
#define _mm_maskz_cvtepi16_epi64 _mm_maskz_cvtepi16_epi64_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtusepi32_epi8_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtusepi32_epi8
#define _mm256_cvtusepi32_epi8 _mm256_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtusepi32_epi8_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtusepi32_epi8
#define _mm256_mask_cvtusepi32_epi8 _mm256_mask_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtusepi32_epi8_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtusepi32_epi8
#define _mm256_maskz_cvtusepi32_epi8 _mm256_maskz_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtusepi32_epi8_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtusepi32_epi8
#define _mm_cvtusepi32_epi8 _mm_cvtusepi32_epi8_dbg

/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtusepi32_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtusepi32_epi8
#define _mm_mask_cvtusepi32_epi8 _mm_mask_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtusepi32_epi8_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtusepi32_epi8
#define _mm_maskz_cvtusepi32_epi8 _mm_maskz_cvtusepi32_epi8_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtusepi32_epi16_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtusepi32_epi16
#define _mm256_cvtusepi32_epi16 _mm256_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtusepi32_epi16_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtusepi32_epi16
#define _mm256_mask_cvtusepi32_epi16 _mm256_mask_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtusepi32_epi16_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtusepi32_epi16
#define _mm256_maskz_cvtusepi32_epi16 _mm256_maskz_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtusepi32_epi16_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtusepi32_epi16
#define _mm_cvtusepi32_epi16 _mm_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtusepi32_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtusepi32_epi16
#define _mm_mask_cvtusepi32_epi16 _mm_mask_cvtusepi32_epi16_dbg


/*
 Convert packed unsigned 32-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtusepi32_epi16_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt32_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtusepi32_epi16
#define _mm_maskz_cvtusepi32_epi16 _mm_maskz_cvtusepi32_epi16_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtusepi64_epi8_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtusepi64_epi8
#define _mm256_cvtusepi64_epi8 _mm256_cvtusepi64_epi8_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtusepi64_epi8_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtusepi64_epi8
#define _mm256_mask_cvtusepi64_epi8 _mm256_mask_cvtusepi64_epi8_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtusepi64_epi8_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtusepi64_epi8
#define _mm256_maskz_cvtusepi64_epi8 _mm256_maskz_cvtusepi64_epi8_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtusepi64_epi8_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtusepi64_epi8
#define _mm_cvtusepi64_epi8 _mm_cvtusepi64_epi8_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtusepi64_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtusepi64_epi8
#define _mm_mask_cvtusepi64_epi8 _mm_mask_cvtusepi64_epi8_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtusepi64_epi8_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtusepi64_epi8
#define _mm_maskz_cvtusepi64_epi8 _mm_maskz_cvtusepi64_epi8_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtusepi64_epi32_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtusepi64_epi32
#define _mm256_cvtusepi64_epi32 _mm256_cvtusepi64_epi32_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtusepi64_epi32_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtusepi64_epi32
#define _mm256_mask_cvtusepi64_epi32 _mm256_mask_cvtusepi64_epi32_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtusepi64_epi32_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtusepi64_epi32
#define _mm256_maskz_cvtusepi64_epi32 _mm256_maskz_cvtusepi64_epi32_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtusepi64_epi32_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtusepi64_epi32
#define _mm_cvtusepi64_epi32 _mm_cvtusepi64_epi32_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtusepi64_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtusepi64_epi32
#define _mm_mask_cvtusepi64_epi32 _mm_mask_cvtusepi64_epi32_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 32-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtusepi64_epi32_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int32(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 3; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtusepi64_epi32
#define _mm_maskz_cvtusepi64_epi32 _mm_maskz_cvtusepi64_epi32_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm256_cvtusepi64_epi16_dbg(__m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtusepi64_epi16
#define _mm256_cvtusepi64_epi16 _mm256_cvtusepi64_epi16_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtusepi64_epi16_dbg(__m128i src, __mmask8 k, __m256i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtusepi64_epi16
#define _mm256_mask_cvtusepi64_epi16 _mm256_mask_cvtusepi64_epi16_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtusepi64_epi16_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 4; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtusepi64_epi16
#define _mm256_maskz_cvtusepi64_epi16 _mm256_maskz_cvtusepi64_epi16_dbg

/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst".
*/
static inline __m128i _mm_cvtusepi64_epi16_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtusepi64_epi16
#define _mm_cvtusepi64_epi16 _mm_cvtusepi64_epi16_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtusepi64_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtusepi64_epi16
#define _mm_mask_cvtusepi64_epi16 _mm_mask_cvtusepi64_epi16_dbg


/*
 Convert packed unsigned 64-bit integers in "a" to packed unsigned 16-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtusepi64_epi16_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt64_To_Int16(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 2; j <= 7; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtusepi64_epi16
#define _mm_maskz_cvtusepi64_epi16 _mm_maskz_cvtusepi64_epi16_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst". 
*/
static inline __m128i _mm256_cvtusepi16_epi8_dbg(__m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtusepi16_epi8
#define _mm256_cvtusepi16_epi8 _mm256_cvtusepi16_epi8_dbg

/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtusepi16_epi8_dbg(__m128i src, __mmask16 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtusepi16_epi8
#define _mm256_mask_cvtusepi16_epi8 _mm256_mask_cvtusepi16_epi8_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtusepi16_epi8_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtusepi16_epi8
#define _mm256_maskz_cvtusepi16_epi8 _mm256_maskz_cvtusepi16_epi8_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst". 
*/
static inline __m256i _mm512_cvtusepi16_epi8_dbg(__m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtusepi16_epi8
#define _mm512_cvtusepi16_epi8 _mm512_cvtusepi16_epi8_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtusepi16_epi8_dbg(__m256i src, __mmask32 k, __m512i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtusepi16_epi8
#define _mm512_mask_cvtusepi16_epi8 _mm512_mask_cvtusepi16_epi8_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtusepi16_epi8_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtusepi16_epi8
#define _mm512_maskz_cvtusepi16_epi8 _mm512_maskz_cvtusepi16_epi8_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst". 
*/
static inline __m128i _mm_cvtusepi16_epi8_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtusepi16_epi8
#define _mm_cvtusepi16_epi8 _mm_cvtusepi16_epi8_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtusepi16_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtusepi16_epi8
#define _mm_mask_cvtusepi16_epi8 _mm_mask_cvtusepi16_epi8_dbg


/*
 Convert packed unsigned 16-bit integers in "a" to packed unsigned 8-bit integers with unsigned saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtusepi16_epi8_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_UnsignedInt16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtusepi16_epi8
#define _mm_maskz_cvtusepi16_epi8 _mm_maskz_cvtusepi16_epi8_dbg

/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst". 
*/
static inline __m128i _mm256_cvtepi16_epi8_dbg(__m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtepi16_epi8
#define _mm256_cvtepi16_epi8 _mm256_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_mask_cvtepi16_epi8_dbg(__m128i src, __mmask16 k, __m256i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_mask_cvtepi16_epi8
#define _mm256_mask_cvtepi16_epi8 _mm256_mask_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm256_maskz_cvtepi16_epi8_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_maskz_cvtepi16_epi8
#define _mm256_maskz_cvtepi16_epi8 _mm256_maskz_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst". 
*/
static inline __m256i _mm512_cvtepi16_epi8_dbg(__m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_cvtepi16_epi8
#define _mm512_cvtepi16_epi8 _mm512_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_mask_cvtepi16_epi8_dbg(__m256i src, __mmask32 k, __m512i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_mask_cvtepi16_epi8
#define _mm512_mask_cvtepi16_epi8 _mm512_mask_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm512_maskz_cvtepi16_epi8_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm512_maskz_cvtepi16_epi8
#define _mm512_maskz_cvtepi16_epi8 _mm512_maskz_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst". 
*/
static inline __m128i _mm_cvtepi16_epi8_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi16_epi8
#define _mm_cvtepi16_epi8 _mm_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepi16_epi8_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepi16_epi8
#define _mm_mask_cvtepi16_epi8 _mm_mask_cvtepi16_epi8_dbg


/*
 Convert packed 16-bit integers in "a" to packed 8-bit integers with truncation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepi16_epi8_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Truncate_Int16_To_Int8(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  for (int j = 8; j <= 15; j++) dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepi16_epi8
#define _mm_maskz_cvtepi16_epi8 _mm_maskz_cvtepi16_epi8_dbg

/*
 Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepu8_epi32_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepu8_epi32
#define _mm256_mask_cvtepu8_epi32 _mm256_mask_cvtepu8_epi32_dbg


/*
 Zero extend packed unsigned 8-bit integers in the low 8 bytes of "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtepu8_epi32_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu8_epi32
#define _mm256_maskz_cvtepu8_epi32 _mm256_maskz_cvtepu8_epi32_dbg

/*
 Zero extend packed unsigned 8-bit integers in the low 4 bytes of "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepu8_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepu8_epi32
#define _mm_mask_cvtepu8_epi32 _mm_mask_cvtepu8_epi32_dbg


/*
 Zero extend packed unsigned 8-bit integers in th elow 4 bytes of "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepu8_epi32_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepu8_epi32
#define _mm_maskz_cvtepu8_epi32 _mm_maskz_cvtepu8_epi32_dbg


/*
 Zero extend packed unsigned 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepu8_epi64_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepu8_epi64
#define _mm256_mask_cvtepu8_epi64 _mm256_mask_cvtepu8_epi64_dbg


/*
 Zero extend packed unsigned 8-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtepu8_epi64_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu8_epi64
#define _mm256_maskz_cvtepu8_epi64 _mm256_maskz_cvtepu8_epi64_dbg


/*
 Zero extend packed unsigned 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepu8_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepu8_epi64
#define _mm_mask_cvtepu8_epi64 _mm_mask_cvtepu8_epi64_dbg


/*
 Zero extend packed unsigned 8-bit integers in the low 2 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepu8_epi64_dbg(__mmask8 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepu8_epi64
#define _mm_maskz_cvtepu8_epi64 _mm_maskz_cvtepu8_epi64_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepu8_epi16_dbg(__m256i src, __mmask16 k, __m128i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepu8_epi16
#define _mm256_mask_cvtepu8_epi16 _mm256_mask_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_cvtepu8_epi16_dbg(__mmask16 k, __m128i a)
{
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu8_epi16
#define _mm256_maskz_cvtepu8_epi16 _mm256_maskz_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".
*/
static inline __m512i _mm512_cvtepu8_epi16_dbg(__m256i a)
{
  uint8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_cvtepu8_epi16
#define _mm512_cvtepu8_epi16 _mm512_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_cvtepu8_epi16_dbg(__m512i src, __mmask32 k, __m256i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  uint8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_cvtepu8_epi16
#define _mm512_mask_cvtepu8_epi16 _mm512_mask_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512i _mm512_maskz_cvtepu8_epi16_dbg(__mmask32 k, __m256i a)
{
  uint8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_cvtepu8_epi16
#define _mm512_maskz_cvtepu8_epi16 _mm512_maskz_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepu8_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepu8_epi16
#define _mm_mask_cvtepu8_epi16 _mm_mask_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_cvtepu8_epi16_dbg(__mmask8 k, __m128i a)
{
  uint8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepu8_epi16
#define _mm_maskz_cvtepu8_epi16 _mm_maskz_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepu32_epi64_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  uint32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepu32_epi64
#define _mm256_mask_cvtepu32_epi64 _mm256_mask_cvtepu32_epi64_dbg


/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtepu32_epi64_dbg(__mmask8 k, __m128i a)
{
  uint32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu32_epi64
#define _mm256_maskz_cvtepu32_epi64 _mm256_maskz_cvtepu32_epi64_dbg

/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepu32_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  uint32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepu32_epi64
#define _mm_mask_cvtepu32_epi64 _mm_mask_cvtepu32_epi64_dbg

/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtepu32_epi64_dbg(__mmask8 k, __m128i a)
{
  uint32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepu32_epi64
#define _mm_maskz_cvtepu32_epi64 _mm_maskz_cvtepu32_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepu16_epi32_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepu16_epi32
#define _mm256_mask_cvtepu16_epi32 _mm256_mask_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_cvtepu16_epi32_dbg(__mmask8 k, __m128i a)
{
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu16_epi32
#define _mm256_maskz_cvtepu16_epi32 _mm256_maskz_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepu16_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepu16_epi32
#define _mm_mask_cvtepu16_epi32 _mm_mask_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_cvtepu16_epi32_dbg(__mmask8 k, __m128i a)
{
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepu16_epi32
#define _mm_maskz_cvtepu16_epi32 _mm_maskz_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_cvtepu16_epi64_dbg(__m256i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_cvtepu16_epi64
#define _mm256_mask_cvtepu16_epi64 _mm256_mask_cvtepu16_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_cvtepu16_epi64_dbg(__mmask8 k, __m128i a)
{
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_cvtepu16_epi64
#define _mm256_maskz_cvtepu16_epi64 _mm256_maskz_cvtepu16_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_cvtepu16_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_cvtepu16_epi64
#define _mm_mask_cvtepu16_epi64 _mm_mask_cvtepu16_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in the low 4 bytes of "a" to packed 64-bit integers, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_cvtepu16_epi64_dbg(__mmask8 k, __m128i a)
{
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_cvtepu16_epi64
#define _mm_maskz_cvtepu16_epi64 _mm_maskz_cvtepu16_epi64_dbg

/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mul_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mul_epi32
#define _mm256_mask_mul_epi32 _mm256_mask_mul_epi32_dbg

/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mul_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mul_epi32
#define _mm256_maskz_mul_epi32 _mm256_maskz_mul_epi32_dbg

/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mul_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mul_epi32
#define _mm_mask_mul_epi32 _mm_mask_mul_epi32_dbg

/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mul_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mul_epi32
#define _mm_maskz_mul_epi32 _mm_maskz_mul_epi32_dbg

/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mulhrs_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  uint32_t tmp;
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
      dst_vec[j] = (tmp & 0x1fffe) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mulhrs_epi16
#define _mm256_mask_mulhrs_epi16 _mm256_mask_mulhrs_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mulhrs_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  uint32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
      dst_vec[j] = (tmp & 0x1fffe) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mulhrs_epi16
#define _mm256_maskz_mulhrs_epi16 _mm256_maskz_mulhrs_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mulhrs_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  uint32_t tmp;
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
      dst_vec[j] = (tmp & 0x1fffe) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mulhrs_epi16
#define _mm512_mask_mulhrs_epi16 _mm512_mask_mulhrs_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mulhrs_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  uint32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
      dst_vec[j] = (tmp & 0x1fffe) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mulhrs_epi16
#define _mm512_maskz_mulhrs_epi16 _mm512_maskz_mulhrs_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst". 
*/
static inline __m512i _mm512_mulhrs_epi16_dbg(__m512i a, __m512i b)
{
  uint32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
    dst_vec[j] = (tmp & 0x1fffe) >> 1;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mulhrs_epi16
#define _mm512_mulhrs_epi16 _mm512_mulhrs_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mulhrs_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  uint32_t tmp;
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
      dst_vec[j] = (tmp & 0x1fffe) >> 1;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mulhrs_epi16
#define _mm_mask_mulhrs_epi16 _mm_mask_mulhrs_epi16_dbg


/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mulhrs_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  uint32_t tmp;
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
      dst_vec[j] = (tmp & 0x1fffe) >> 1;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mulhrs_epi16
#define _mm_maskz_mulhrs_epi16 _mm_maskz_mulhrs_epi16_dbg


/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mulhi_epu16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  uint32_t tmp;
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mulhi_epu16
#define _mm256_mask_mulhi_epu16 _mm256_mask_mulhi_epu16_dbg


/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mulhi_epu16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  uint32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mulhi_epu16
#define _mm256_maskz_mulhi_epu16 _mm256_maskz_mulhi_epu16_dbg


/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mulhi_epu16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  uint32_t tmp;
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mulhi_epu16
#define _mm512_mask_mulhi_epu16 _mm512_mask_mulhi_epu16_dbg


/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mulhi_epu16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  uint32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mulhi_epu16
#define _mm512_maskz_mulhi_epu16 _mm512_maskz_mulhi_epu16_dbg


/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst". 
*/
static inline __m512i _mm512_mulhi_epu16_dbg(__m512i a, __m512i b)
{
  uint32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = (tmp & 0xffff0000) >> 16;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mulhi_epu16
#define _mm512_mulhi_epu16 _mm512_mulhi_epu16_dbg


/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mulhi_epu16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  uint32_t tmp;
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mulhi_epu16
#define _mm_mask_mulhi_epu16 _mm_mask_mulhi_epu16_dbg


/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mulhi_epu16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  uint32_t tmp;
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mulhi_epu16
#define _mm_maskz_mulhi_epu16 _mm_maskz_mulhi_epu16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mulhi_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int32_t tmp;
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mulhi_epi16
#define _mm256_mask_mulhi_epi16 _mm256_mask_mulhi_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mulhi_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mulhi_epi16
#define _mm256_maskz_mulhi_epi16 _mm256_maskz_mulhi_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mulhi_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int32_t tmp;
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mulhi_epi16
#define _mm512_mask_mulhi_epi16 _mm512_mask_mulhi_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mulhi_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mulhi_epi16
#define _mm512_maskz_mulhi_epi16 _mm512_maskz_mulhi_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst". 
*/
static inline __m512i _mm512_mulhi_epi16_dbg(__m512i a, __m512i b)
{
  int32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = (tmp & 0xffff0000) >> 16;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mulhi_epi16
#define _mm512_mulhi_epi16 _mm512_mulhi_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mulhi_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t tmp;
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mulhi_epi16
#define _mm_mask_mulhi_epi16 _mm_mask_mulhi_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mulhi_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t tmp;
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff0000) >> 16;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mulhi_epi16
#define _mm_maskz_mulhi_epi16 _mm_maskz_mulhi_epi16_dbg


/*
 Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mullo_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t tmp;
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = tmp & 0xffffffff;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mullo_epi32
#define _mm256_mask_mullo_epi32 _mm256_mask_mullo_epi32_dbg


/*
 Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mullo_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t tmp;
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = tmp & 0xffffffff;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mullo_epi32
#define _mm256_maskz_mullo_epi32 _mm256_maskz_mullo_epi32_dbg


/*
 Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mullo_epi32_dbg(__mmask16 k, __m512i a, __m512i b)
{
  int64_t tmp;
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = tmp & 0xffffffff;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mullo_epi32
#define _mm512_maskz_mullo_epi32 _mm512_maskz_mullo_epi32_dbg


/*
 Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mullo_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t tmp;
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = tmp & 0xffffffff;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mullo_epi32
#define _mm_mask_mullo_epi32 _mm_mask_mullo_epi32_dbg


/*
 Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mullo_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t tmp;
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = tmp & 0xffffffff;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mullo_epi32
#define _mm_maskz_mullo_epi32 _mm_maskz_mullo_epi32_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mullo_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
      tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
      tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
      dst_vec[j] = tmp5;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mullo_epi64
#define _mm256_mask_mullo_epi64 _mm256_mask_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mullo_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
      tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
      tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
      dst_vec[j] = tmp5;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mullo_epi64
#define _mm256_maskz_mullo_epi64 _mm256_maskz_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst". 
*/
static inline __m256i _mm256_mullo_epi64_dbg(__m256i a, __m256i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
    tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
    tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
    dst_vec[j] = tmp5;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mullo_epi64
#define _mm256_mullo_epi64 _mm256_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mullo_epi64_dbg(__m512i src, __mmask8 k, __m512i a, __m512i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
      tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
      tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
      dst_vec[j] = tmp5;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mullo_epi64
#define _mm512_mask_mullo_epi64 _mm512_mask_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mullo_epi64_dbg(__mmask8 k, __m512i a, __m512i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
      tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
      tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
      dst_vec[j] = tmp5;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mullo_epi64
#define _mm512_maskz_mullo_epi64 _mm512_maskz_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst". 
*/
static inline __m512i _mm512_mullo_epi64_dbg(__m512i a, __m512i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
    tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
    tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
    dst_vec[j] = tmp5;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mullo_epi64
#define _mm512_mullo_epi64 _mm512_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mullo_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
      tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
      tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
      dst_vec[j] = tmp5;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mullo_epi64
#define _mm_mask_mullo_epi64 _mm_mask_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mullo_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
      tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
      tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
      dst_vec[j] = tmp5;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mullo_epi64
#define _mm_maskz_mullo_epi64 _mm_maskz_mullo_epi64_dbg

/*
 Multiply the packed 64-bit integers in "a" and "b", producing intermediate 128-bit integers, and store the low 64 bits of the intermediate integers in "dst". 
*/
static inline __m128i _mm_mullo_epi64_dbg(__m128i a, __m128i b)
{
  int64_t tmp1, tmp2, tmp3, tmp4, tmp5;
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    tmp1 = a_vec[j] >> 32; tmp2 = a_vec[j] & 0xFFFFFFFFUL;
    tmp3 = b_vec[j] >> 32; tmp4 = b_vec[j] & 0xFFFFFFFFUL;
    tmp5 = (tmp2 * tmp4 + ((tmp1 * tmp4 + tmp2 * tmp3) << 32)) & 0xFFFFFFFFFFFFFFFFULL;
    dst_vec[j] = tmp5;
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mullo_epi64
#define _mm_mullo_epi64 _mm_mullo_epi64_dbg

/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mullo_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int32_t tmp;
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff) >> 0;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mullo_epi16
#define _mm256_mask_mullo_epi16 _mm256_mask_mullo_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mullo_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff) >> 0;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mullo_epi16
#define _mm256_maskz_mullo_epi16 _mm256_maskz_mullo_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_mullo_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int32_t tmp;
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff) >> 0;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mullo_epi16
#define _mm512_mask_mullo_epi16 _mm512_mask_mullo_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_mullo_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff) >> 0;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_mullo_epi16
#define _mm512_maskz_mullo_epi16 _mm512_maskz_mullo_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst". 
*/
static inline __m512i _mm512_mullo_epi16_dbg(__m512i a, __m512i b)
{
  int32_t tmp;
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = (tmp & 0xffff) >> 0;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mullo_epi16
#define _mm512_mullo_epi16 _mm512_mullo_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mullo_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t tmp;
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff) >> 0;
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mullo_epi16
#define _mm_mask_mullo_epi16 _mm_mask_mullo_epi16_dbg


/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mullo_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t tmp;
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      tmp = a_vec[j] * b_vec[j];
      dst_vec[j] = (tmp & 0xffff) >> 0;
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mullo_epi16
#define _mm_maskz_mullo_epi16 _mm_maskz_mullo_epi16_dbg

/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_mul_epu32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  uint32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  uint32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_mul_epu32
#define _mm256_mask_mul_epu32 _mm256_mask_mul_epu32_dbg

/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_mul_epu32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  uint32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  uint32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_mul_epu32
#define _mm256_maskz_mul_epu32 _mm256_maskz_mul_epu32_dbg

/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_mul_epu32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  uint32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  uint32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_mul_epu32
#define _mm_mask_mul_epu32 _mm_mask_mul_epu32_dbg

/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_mul_epu32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  uint32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  uint32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j*2] * b_vec[j*2];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_mul_epu32
#define _mm_maskz_mul_epu32 _mm_maskz_mul_epu32_dbg

/*
 Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_or_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_or_epi32
#define _mm256_mask_or_epi32 _mm256_mask_or_epi32_dbg


/*
 Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_or_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_or_epi32
#define _mm256_maskz_or_epi32 _mm256_maskz_or_epi32_dbg


/*
 Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_or_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_or_epi32
#define _mm_mask_or_epi32 _mm_mask_or_epi32_dbg


/*
 Compute the bitwise OR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_or_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_or_epi32
#define _mm_maskz_or_epi32 _mm_maskz_or_epi32_dbg


/*
 Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_or_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_or_epi64
#define _mm256_mask_or_epi64 _mm256_mask_or_epi64_dbg


/*
 Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_or_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_or_epi64
#define _mm256_maskz_or_epi64 _mm256_maskz_or_epi64_dbg


/*
 Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_or_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_or_epi64
#define _mm_mask_or_epi64 _mm_mask_or_epi64_dbg


/*
 Compute the bitwise OR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_or_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] | b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_or_epi64
#define _mm_maskz_or_epi64 _mm_maskz_or_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_rol_epi32_dbg(__m256i src, __mmask8 k, __m256i a, const int imm8)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_rol_epi32
#define _mm256_mask_rol_epi32 _mm256_mask_rol_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_rol_epi32_dbg(__mmask8 k, __m256i a, const int imm8)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_rol_epi32
#define _mm256_maskz_rol_epi32 _mm256_maskz_rol_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m256i _mm256_rol_epi32_dbg(__m256i a, const int imm8)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_rol_epi32
#define _mm256_rol_epi32 _mm256_rol_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_rol_epi32_dbg(__m128i src, __mmask8 k, __m128i a, const int imm8)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_rol_epi32
#define _mm_mask_rol_epi32 _mm_mask_rol_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_rol_epi32_dbg(__mmask8 k, __m128i a, const int imm8)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_rol_epi32
#define _mm_maskz_rol_epi32 _mm_maskz_rol_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m128i _mm_rol_epi32_dbg(__m128i a, int imm8)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_rol_epi32
#define _mm_rol_epi32 _mm_rol_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_rol_epi64_dbg(__m256i src, __mmask8 k, __m256i a, const int imm8)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_rol_epi64
#define _mm256_mask_rol_epi64 _mm256_mask_rol_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_rol_epi64_dbg(__mmask8 k, __m256i a, const int imm8)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_rol_epi64
#define _mm256_maskz_rol_epi64 _mm256_maskz_rol_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m256i _mm256_rol_epi64_dbg(__m256i a, const int imm8)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_rol_epi64
#define _mm256_rol_epi64 _mm256_rol_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_rol_epi64_dbg(__m128i src, __mmask8 k, __m128i a, const int imm8)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_rol_epi64
#define _mm_mask_rol_epi64 _mm_mask_rol_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_rol_epi64_dbg(__mmask8 k, __m128i a, const int imm8)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_rol_epi64
#define _mm_maskz_rol_epi64 _mm_maskz_rol_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m128i _mm_rol_epi64_dbg(__m128i a, const int imm8)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_rol_epi64
#define _mm_rol_epi64 _mm_rol_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_rolv_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_rolv_epi32
#define _mm256_mask_rolv_epi32 _mm256_mask_rolv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_rolv_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_rolv_epi32
#define _mm256_maskz_rolv_epi32 _mm256_maskz_rolv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m256i _mm256_rolv_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_rolv_epi32
#define _mm256_rolv_epi32 _mm256_rolv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_rolv_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_rolv_epi32
#define _mm_mask_rolv_epi32 _mm_mask_rolv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_rolv_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_rolv_epi32
#define _mm_maskz_rolv_epi32 _mm_maskz_rolv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m128i _mm_rolv_epi32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = LEFT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_rolv_epi32
#define _mm_rolv_epi32 _mm_rolv_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_rolv_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_rolv_epi64
#define _mm256_mask_rolv_epi64 _mm256_mask_rolv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_rolv_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_rolv_epi64
#define _mm256_maskz_rolv_epi64 _mm256_maskz_rolv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m256i _mm256_rolv_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_rolv_epi64
#define _mm256_rolv_epi64 _mm256_rolv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_rolv_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_rolv_epi64
#define _mm_mask_rolv_epi64 _mm_mask_rolv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_rolv_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_rolv_epi64
#define _mm_maskz_rolv_epi64 _mm_maskz_rolv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the left by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m128i _mm_rolv_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = LEFT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_rolv_epi64
#define _mm_rolv_epi64 _mm_rolv_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_ror_epi32_dbg(__m256i src, __mmask8 k, __m256i a, const int imm8)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_ror_epi32
#define _mm256_mask_ror_epi32 _mm256_mask_ror_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_ror_epi32_dbg(__mmask8 k, __m256i a, const int imm8)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_ror_epi32
#define _mm256_maskz_ror_epi32 _mm256_maskz_ror_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m256i _mm256_ror_epi32_dbg(__m256i a, const int imm8)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_ror_epi32
#define _mm256_ror_epi32 _mm256_ror_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_ror_epi32_dbg(__m128i src, __mmask8 k, __m128i a, const int imm8)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_ror_epi32
#define _mm_mask_ror_epi32 _mm_mask_ror_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_ror_epi32_dbg(__mmask8 k, __m128i a, const int imm8)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_ror_epi32
#define _mm_maskz_ror_epi32 _mm_maskz_ror_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m128i _mm_ror_epi32_dbg(__m128i a, const int imm8)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_ror_epi32
#define _mm_ror_epi32 _mm_ror_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_ror_epi64_dbg(__m256i src, __mmask8 k, __m256i a, const int imm8)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_ror_epi64
#define _mm256_mask_ror_epi64 _mm256_mask_ror_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_ror_epi64_dbg(__mmask8 k, __m256i a, const int imm8)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_ror_epi64
#define _mm256_maskz_ror_epi64 _mm256_maskz_ror_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m256i _mm256_ror_epi64_dbg(__m256i a, const int imm8)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_ror_epi64
#define _mm256_ror_epi64 _mm256_ror_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_ror_epi64_dbg(__m128i src, __mmask8 k, __m128i a, const int imm8)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_ror_epi64
#define _mm_mask_ror_epi64 _mm_mask_ror_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_ror_epi64_dbg(__mmask8 k, __m128i a, const int imm8)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_ror_epi64
#define _mm_maskz_ror_epi64 _mm_maskz_ror_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in "imm8", and store the results in "dst". 
*/
static inline __m128i _mm_ror_epi64_dbg(__m128i a, const int imm8)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_ror_epi64
#define _mm_ror_epi64 _mm_ror_epi64_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_rorv_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_rorv_epi32
#define _mm256_mask_rorv_epi32 _mm256_mask_rorv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_rorv_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_rorv_epi32
#define _mm256_maskz_rorv_epi32 _mm256_maskz_rorv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m256i _mm256_rorv_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_rorv_epi32
#define _mm256_rorv_epi32 _mm256_rorv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_rorv_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_rorv_epi32
#define _mm_mask_rorv_epi32 _mm_mask_rorv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_rorv_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_rorv_epi32
#define _mm_maskz_rorv_epi32 _mm_maskz_rorv_epi32_dbg


/*
 Rotate the bits in each packed 32-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m128i _mm_rorv_epi32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = RIGHT_ROTATE_DWORDS(a_vec[j], b_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_rorv_epi32
#define _mm_rorv_epi32 _mm_rorv_epi32_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_rorv_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_rorv_epi64
#define _mm256_mask_rorv_epi64 _mm256_mask_rorv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_rorv_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_rorv_epi64
#define _mm256_maskz_rorv_epi64 _mm256_maskz_rorv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m256i _mm256_rorv_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_rorv_epi64
#define _mm256_rorv_epi64 _mm256_rorv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_rorv_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_rorv_epi64
#define _mm_mask_rorv_epi64 _mm_mask_rorv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_rorv_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_rorv_epi64
#define _mm_maskz_rorv_epi64 _mm_maskz_rorv_epi64_dbg


/*
 Rotate the bits in each packed 64-bit integer in "a" to the right by the number of bits specified in the corresponding element of "b", and store the results in "dst". 
*/
static inline __m128i _mm_rorv_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = RIGHT_ROTATE_QWORDS(a_vec[j], b_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_rorv_epi64
#define _mm_rorv_epi64 _mm_rorv_epi64_dbg


/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_sll_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m128i count)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sll_epi32
#define _mm256_mask_sll_epi32 _mm256_mask_sll_epi32_dbg



/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_sll_epi32_dbg(__mmask8 k, __m256i a, __m128i count)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sll_epi32
#define _mm256_maskz_sll_epi32 _mm256_maskz_sll_epi32_dbg



/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_sll_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sll_epi32
#define _mm_mask_sll_epi32 _mm_mask_sll_epi32_dbg



/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_sll_epi32_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sll_epi32
#define _mm_maskz_sll_epi32 _mm_maskz_sll_epi32_dbg



/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_sll_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m128i count)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sll_epi64
#define _mm256_mask_sll_epi64 _mm256_mask_sll_epi64_dbg



/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_sll_epi64_dbg(__mmask8 k, __m256i a, __m128i count)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sll_epi64
#define _mm256_maskz_sll_epi64 _mm256_maskz_sll_epi64_dbg



/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_sll_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sll_epi64
#define _mm_mask_sll_epi64 _mm_mask_sll_epi64_dbg



/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_sll_epi64_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sll_epi64
#define _mm_maskz_sll_epi64 _mm_maskz_sll_epi64_dbg



/*
 Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_sllv_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i count)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t count_vec[8];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sllv_epi32
#define _mm256_mask_sllv_epi32 _mm256_mask_sllv_epi32_dbg


/*
 Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_sllv_epi32_dbg(__mmask8 k, __m256i a, __m256i count)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t count_vec[8];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sllv_epi32
#define _mm256_maskz_sllv_epi32 _mm256_maskz_sllv_epi32_dbg


/*
 Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_sllv_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t count_vec[4];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sllv_epi32
#define _mm_mask_sllv_epi32 _mm_mask_sllv_epi32_dbg


/*
 Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_sllv_epi32_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t count_vec[4];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sllv_epi32
#define _mm_maskz_sllv_epi32 _mm_maskz_sllv_epi32_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_sllv_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i count)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[4];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sllv_epi64
#define _mm256_mask_sllv_epi64 _mm256_mask_sllv_epi64_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_sllv_epi64_dbg(__mmask8 k, __m256i a, __m256i count)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[4];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sllv_epi64
#define _mm256_maskz_sllv_epi64 _mm256_maskz_sllv_epi64_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_sllv_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sllv_epi64
#define _mm_mask_sllv_epi64 _mm_mask_sllv_epi64_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_sllv_epi64_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sllv_epi64
#define _mm_maskz_sllv_epi64 _mm_maskz_sllv_epi64_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_sllv_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i count)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t count_vec[16];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sllv_epi16
#define _mm256_mask_sllv_epi16 _mm256_mask_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_sllv_epi16_dbg(__mmask16 k, __m256i a, __m256i count)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t count_vec[16];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sllv_epi16
#define _mm256_maskz_sllv_epi16 _mm256_maskz_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_sllv_epi16_dbg(__m256i a, __m256i count)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t count_vec[16];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sllv_epi16
#define _mm256_sllv_epi16 _mm256_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_sllv_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i count)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t count_vec[32];
  _mm512_storeu_si512((void*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sllv_epi16
#define _mm512_mask_sllv_epi16 _mm512_mask_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_sllv_epi16_dbg(__mmask32 k, __m512i a, __m512i count)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t count_vec[32];
  _mm512_storeu_si512((void*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sllv_epi16
#define _mm512_maskz_sllv_epi16 _mm512_maskz_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_sllv_epi16_dbg(__m512i a, __m512i count)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t count_vec[32];
  _mm512_storeu_si512((void*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sllv_epi16
#define _mm512_sllv_epi16 _mm512_sllv_epi16_dbg

/*
 Shift packed 32-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst".
*/
static inline __m512i _mm512_slli_epi32_dbg(__m512i a, unsigned int imm8)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if ((imm8 & 0xff) > 31) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend(a_vec[j] << (imm8 & 0xff));
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_slli_epi32
#define _mm512_slli_epi32 _mm512_slli_epi32_dbg

/*
 Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst".
*/
static inline __m512i _mm512_slli_epi64_dbg(__m512i a, unsigned int imm8)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if ((imm8 & 0xff)> 63) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend(a_vec[j] << (imm8 & 0xff));
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_slli_epi64
#define _mm512_slli_epi64 _mm512_slli_epi64_dbg

/*
 Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst".
*/
static inline __m512i _mm512_slli_epi16_dbg(__m512i a, unsigned int imm8)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if ((imm8 & 0xff)> 15) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend(a_vec[j] << (imm8 & 0xff));
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_slli_epi16
#define _mm512_slli_epi16 _mm512_slli_epi16_dbg

/*
 Shift packed 32-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst".
*/
static inline __m512i _mm512_srli_epi32_dbg(__m512i a, unsigned int imm8)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if ((imm8 & 0xff)> 31) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend(a_vec[j] >> (imm8 & 0xff));
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_srli_epi32
#define _mm512_srli_epi32 _mm512_srli_epi32_dbg

/*
 Shift packed 64-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst".
*/
static inline __m512i _mm512_srli_epi64_dbg(__m512i a, unsigned int imm8)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if ((imm8 & 0xff) > 63) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend(a_vec[j] >> (imm8 & 0xff));
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_srli_epi64
#define _mm512_srli_epi64 _mm512_srli_epi64_dbg

/*
 Shift packed 16-bit integers in "a" right by "imm8" while shifting in zeros, and store the results in "dst".
*/
static inline __m512i _mm512_srli_epi16_dbg(__m512i a, unsigned int imm8)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if ((imm8 & 0xff) > 15) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend(a_vec[j] >> (imm8 & 0xff));
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_srli_epi16
#define _mm512_srli_epi16 _mm512_srli_epi16_dbg

/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_sllv_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t count_vec[8];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sllv_epi16
#define _mm_mask_sllv_epi16 _mm_mask_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_sllv_epi16_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t count_vec[8];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sllv_epi16
#define _mm_maskz_sllv_epi16 _mm_maskz_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m128i _mm_sllv_epi16_dbg(__m128i a, __m128i count)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t count_vec[8];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = (count_vec[j] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_sllv_epi16
#define _mm_sllv_epi16 _mm_sllv_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_sll_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m128i count)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sll_epi16
#define _mm256_mask_sll_epi16 _mm256_mask_sll_epi16_dbg



/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_sll_epi16_dbg(__mmask16 k, __m256i a, __m128i count)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sll_epi16
#define _mm256_maskz_sll_epi16 _mm256_maskz_sll_epi16_dbg



/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_sll_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m128i count)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sll_epi16
#define _mm512_mask_sll_epi16 _mm512_mask_sll_epi16_dbg



/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_sll_epi16_dbg(__mmask32 k, __m512i a, __m128i count)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sll_epi16
#define _mm512_maskz_sll_epi16 _mm512_maskz_sll_epi16_dbg



/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_sll_epi16_dbg(__m512i a, __m128i count)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (count_vec[0] > 15) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sll_epi16
#define _mm512_sll_epi16 _mm512_sll_epi16_dbg



/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_sll_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sll_epi16
#define _mm_mask_sll_epi16 _mm_mask_sll_epi16_dbg



/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_sll_epi16_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sll_epi16
#define _mm_maskz_sll_epi16 _mm_maskz_sll_epi16_dbg



/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_srl_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m128i count)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_srl_epi32
#define _mm256_mask_srl_epi32 _mm256_mask_srl_epi32_dbg


/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_srl_epi32_dbg(__mmask8 k, __m256i a, __m128i count)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_srl_epi32
#define _mm256_maskz_srl_epi32 _mm256_maskz_srl_epi32_dbg


/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_srl_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_srl_epi32
#define _mm_mask_srl_epi32 _mm_mask_srl_epi32_dbg


/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_srl_epi32_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 31) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_srl_epi32
#define _mm_maskz_srl_epi32 _mm_maskz_srl_epi32_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_srl_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m128i count)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_srl_epi64
#define _mm256_mask_srl_epi64 _mm256_mask_srl_epi64_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_srl_epi64_dbg(__mmask8 k, __m256i a, __m128i count)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_srl_epi64
#define _mm256_maskz_srl_epi64 _mm256_maskz_srl_epi64_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_srl_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_srl_epi64
#define _mm_mask_srl_epi64 _mm_mask_srl_epi64_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_srl_epi64_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 63) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_srl_epi64
#define _mm_maskz_srl_epi64 _mm_maskz_srl_epi64_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_srl_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m128i count)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_srl_epi16
#define _mm256_mask_srl_epi16 _mm256_mask_srl_epi16_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_maskz_srl_epi16_dbg(__mmask16 k, __m256i a, __m128i count)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_srl_epi16
#define _mm256_maskz_srl_epi16 _mm256_maskz_srl_epi16_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_mask_srl_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m128i count)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_srl_epi16
#define _mm512_mask_srl_epi16 _mm512_mask_srl_epi16_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512i _mm512_maskz_srl_epi16_dbg(__mmask32 k, __m512i a, __m128i count)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_srl_epi16
#define _mm512_maskz_srl_epi16 _mm512_maskz_srl_epi16_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m512i _mm512_srl_epi16_dbg(__m512i a, __m128i count)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (count_vec[0] > 15) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_srl_epi16
#define _mm512_srl_epi16 _mm512_srl_epi16_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_srl_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i count)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_srl_epi16
#define _mm_mask_srl_epi16 _mm_mask_srl_epi16_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_maskz_srl_epi16_dbg(__mmask8 k, __m128i a, __m128i count)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      if (count_vec[0] > 15) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
      }
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_srl_epi16
#define _mm_maskz_srl_epi16 _mm_maskz_srl_epi16_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_sub_epi8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sub_epi8
#define _mm256_mask_sub_epi8 _mm256_mask_sub_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_sub_epi8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sub_epi8
#define _mm256_maskz_sub_epi8 _mm256_maskz_sub_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_sub_epi8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sub_epi8
#define _mm512_mask_sub_epi8 _mm512_mask_sub_epi8_dbg

/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_sub_epi8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sub_epi8
#define _mm512_maskz_sub_epi8 _mm512_maskz_sub_epi8_dbg

/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst".
*/
static inline __m512i _mm512_sub_epi8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sub_epi8
#define _mm512_sub_epi8 _mm512_sub_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_sub_epi8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sub_epi8
#define _mm_mask_sub_epi8 _mm_mask_sub_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_sub_epi8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sub_epi8
#define _mm_maskz_sub_epi8 _mm_maskz_sub_epi8_dbg


/*
 Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_sub_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sub_epi32
#define _mm256_mask_sub_epi32 _mm256_mask_sub_epi32_dbg


/*
 Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_sub_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sub_epi32
#define _mm256_maskz_sub_epi32 _mm256_maskz_sub_epi32_dbg


/*
 Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_sub_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sub_epi32
#define _mm_mask_sub_epi32 _mm_mask_sub_epi32_dbg


/*
 Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_sub_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sub_epi32
#define _mm_maskz_sub_epi32 _mm_maskz_sub_epi32_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_sub_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sub_epi64
#define _mm256_mask_sub_epi64 _mm256_mask_sub_epi64_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_sub_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sub_epi64
#define _mm256_maskz_sub_epi64 _mm256_maskz_sub_epi64_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_sub_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sub_epi64
#define _mm_mask_sub_epi64 _mm_mask_sub_epi64_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_sub_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sub_epi64
#define _mm_maskz_sub_epi64 _mm_maskz_sub_epi64_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_subs_epi8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_subs_epi8
#define _mm256_mask_subs_epi8 _mm256_mask_subs_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_subs_epi8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_subs_epi8
#define _mm256_maskz_subs_epi8 _mm256_maskz_subs_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_subs_epi8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_subs_epi8
#define _mm512_mask_subs_epi8 _mm512_mask_subs_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_subs_epi8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_subs_epi8
#define _mm512_maskz_subs_epi8 _mm512_maskz_subs_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_subs_epi8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_subs_epi8
#define _mm512_subs_epi8 _mm512_subs_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_subs_epi8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_subs_epi8
#define _mm_mask_subs_epi8 _mm_mask_subs_epi8_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_subs_epi8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_subs_epi8
#define _mm_maskz_subs_epi8 _mm_maskz_subs_epi8_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_subs_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_subs_epi16
#define _mm256_mask_subs_epi16 _mm256_mask_subs_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_subs_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_subs_epi16
#define _mm256_maskz_subs_epi16 _mm256_maskz_subs_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_subs_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_subs_epi16
#define _mm512_mask_subs_epi16 _mm512_mask_subs_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_subs_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_subs_epi16
#define _mm512_maskz_subs_epi16 _mm512_maskz_subs_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_subs_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_subs_epi16
#define _mm512_subs_epi16 _mm512_subs_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_subs_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_subs_epi16
#define _mm_mask_subs_epi16 _mm_mask_subs_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_subs_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_subs_epi16
#define _mm_maskz_subs_epi16 _mm_maskz_subs_epi16_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_subs_epu8_dbg(__m256i src, __mmask32 k, __m256i a, __m256i b)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_subs_epu8
#define _mm256_mask_subs_epu8 _mm256_mask_subs_epu8_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_subs_epu8_dbg(__mmask32 k, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_subs_epu8
#define _mm256_maskz_subs_epu8 _mm256_maskz_subs_epu8_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_subs_epu8_dbg(__m512i src, __mmask64 k, __m512i a, __m512i b)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_subs_epu8
#define _mm512_mask_subs_epu8 _mm512_mask_subs_epu8_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_subs_epu8_dbg(__mmask64 k, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    if (k & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_subs_epu8
#define _mm512_maskz_subs_epu8 _mm512_maskz_subs_epu8_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_subs_epu8_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  int8_t dst_vec[64];
  for (int j = 0; j <= 63; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_subs_epu8
#define _mm512_subs_epu8 _mm512_subs_epu8_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_subs_epu8_dbg(__m128i src, __mmask16 k, __m128i a, __m128i b)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_subs_epu8
#define _mm_mask_subs_epu8 _mm_mask_subs_epu8_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_subs_epu8_dbg(__mmask16 k, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_subs_epu8
#define _mm_maskz_subs_epu8 _mm_maskz_subs_epu8_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_subs_epu16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_subs_epu16
#define _mm256_mask_subs_epu16 _mm256_mask_subs_epu16_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_subs_epu16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_subs_epu16
#define _mm256_maskz_subs_epu16 _mm256_maskz_subs_epu16_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_subs_epu16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_subs_epu16
#define _mm512_mask_subs_epu16 _mm512_mask_subs_epu16_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_subs_epu16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_subs_epu16
#define _mm512_maskz_subs_epu16 _mm512_maskz_subs_epu16_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m512i _mm512_subs_epu16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_subs_epu16
#define _mm512_subs_epu16 _mm512_subs_epu16_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_subs_epu16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_subs_epu16
#define _mm_mask_subs_epu16 _mm_mask_subs_epu16_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_subs_epu16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_subs_epu16
#define _mm_maskz_subs_epu16 _mm_maskz_subs_epu16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_mask_sub_epi16_dbg(__m256i src, __mmask16 k, __m256i a, __m256i b)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_sub_epi16
#define _mm256_mask_sub_epi16 _mm256_mask_sub_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256i _mm256_maskz_sub_epi16_dbg(__mmask16 k, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_sub_epi16
#define _mm256_maskz_sub_epi16 _mm256_maskz_sub_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_sub_epi16_dbg(__m512i src, __mmask32 k, __m512i a, __m512i b)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_sub_epi16
#define _mm512_mask_sub_epi16 _mm512_mask_sub_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_sub_epi16_dbg(__mmask32 k, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_sub_epi16
#define _mm512_maskz_sub_epi16 _mm512_maskz_sub_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst".
*/
static inline __m512i _mm512_sub_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sub_epi16
#define _mm512_sub_epi16 _mm512_sub_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128i _mm_mask_sub_epi16_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_sub_epi16
#define _mm_mask_sub_epi16 _mm_mask_sub_epi16_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128i _mm_maskz_sub_epi16_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_sub_epi16
#define _mm_maskz_sub_epi16 _mm_maskz_sub_epi16_dbg


/*
 Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask32 _mm256_mask_test_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_test_epi8_mask
#define _mm256_mask_test_epi8_mask _mm256_mask_test_epi8_mask_dbg

/*
 Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" if the intermediate value is non-zero.
*/
static inline __mmask32 _mm256_test_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_test_epi8_mask
#define _mm256_test_epi8_mask _mm256_test_epi8_mask_dbg

/*
 Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask64 _mm512_mask_test_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_test_epi8_mask
#define _mm512_mask_test_epi8_mask _mm512_mask_test_epi8_mask_dbg

/*
 Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" if the intermediate value is non-zero.
*/
static inline __mmask64 _mm512_test_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (((a_vec[j] & b_vec[j]) != 0) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_test_epi8_mask
#define _mm512_test_epi8_mask _mm512_test_epi8_mask_dbg


/*
 Compute the bitwise AND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask16 _mm_mask_test_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_test_epi8_mask
#define _mm_mask_test_epi8_mask _mm_mask_test_epi8_mask_dbg

/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask8 _mm256_mask_test_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_test_epi32_mask
#define _mm256_mask_test_epi32_mask _mm256_mask_test_epi32_mask_dbg

/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" if the intermediate value is non-zero.
*/
static inline __mmask8 _mm256_test_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_test_epi32_mask
#define _mm256_test_epi32_mask _mm256_test_epi32_mask_dbg


/*
 Compute the bitwise AND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask8 _mm_mask_test_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_test_epi32_mask
#define _mm_mask_test_epi32_mask _mm_mask_test_epi32_mask_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask8 _mm256_mask_test_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_test_epi64_mask
#define _mm256_mask_test_epi64_mask _mm256_mask_test_epi64_mask_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" if the intermediate value is non-zero.
*/
static inline __mmask8 _mm256_test_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_test_epi64_mask
#define _mm256_test_epi64_mask _mm256_test_epi64_mask_dbg


/*
 Compute the bitwise AND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask8 _mm_mask_test_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_test_epi64_mask
#define _mm_mask_test_epi64_mask _mm_mask_test_epi64_mask_dbg

/*
 Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask16 _mm256_mask_test_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_test_epi16_mask
#define _mm256_mask_test_epi16_mask _mm256_mask_test_epi16_mask_dbg

/*
 Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" if the intermediate value is non-zero.
*/
static inline __mmask16 _mm256_test_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_test_epi16_mask
#define _mm256_test_epi16_mask _mm256_test_epi16_mask_dbg

/*
 Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask32 _mm512_mask_test_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_test_epi16_mask
#define _mm512_mask_test_epi16_mask _mm512_mask_test_epi16_mask_dbg

/*
 Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" if the intermediate value is non-zero.
*/
static inline __mmask32 _mm512_test_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_test_epi16_mask
#define _mm512_test_epi16_mask _mm512_test_epi16_mask_dbg

/*
 Compute the bitwise AND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is non-zero.
*/
static inline __mmask8 _mm_mask_test_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) != 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_test_epi16_mask
#define _mm_mask_test_epi16_mask _mm_mask_test_epi16_mask_dbg

/*
 Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask32 _mm256_mask_testn_epi8_mask_dbg(__mmask32 k1, __m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_testn_epi8_mask
#define _mm256_mask_testn_epi8_mask _mm256_mask_testn_epi8_mask_dbg

/*
 Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask32 _mm256_testn_epi8_mask_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_testn_epi8_mask
#define _mm256_testn_epi8_mask _mm256_testn_epi8_mask_dbg

/*
 Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask64 _mm512_mask_testn_epi8_mask_dbg(__mmask64 k1, __m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    if (k1 & ((1ULL << j) & 0xFFFFFFFFFFFFFFFFULL)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1ULL : 0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_testn_epi8_mask
#define _mm512_mask_testn_epi8_mask _mm512_mask_testn_epi8_mask_dbg

/*
 Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask64 _mm512_testn_epi8_mask_dbg(__m512i a, __m512i b)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t b_vec[64];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask64 k = 0;
  for (int j = 0; j <= 63; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1ULL : 0) << j;
  }
  return k;
}

#undef _mm512_testn_epi8_mask
#define _mm512_testn_epi8_mask _mm512_testn_epi8_mask_dbg

/*
 Compute the bitwise NAND of packed 8-bit integers in "a" and "b", producing intermediate 8-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask16 _mm_mask_testn_epi8_mask_dbg(__mmask16 k1, __m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_testn_epi8_mask
#define _mm_mask_testn_epi8_mask _mm_mask_testn_epi8_mask_dbg

/*
 Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask8 _mm256_mask_testn_epi32_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_testn_epi32_mask
#define _mm256_mask_testn_epi32_mask _mm256_mask_testn_epi32_mask_dbg


/*
 Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask8 _mm256_testn_epi32_mask_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_testn_epi32_mask
#define _mm256_testn_epi32_mask _mm256_testn_epi32_mask_dbg


/*
 Compute the bitwise NAND of packed 32-bit integers in "a" and "b", producing intermediate 32-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask8 _mm_mask_testn_epi32_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_testn_epi32_mask
#define _mm_mask_testn_epi32_mask _mm_mask_testn_epi32_mask_dbg


/*
 Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask8 _mm256_mask_testn_epi64_mask_dbg(__mmask8 k1, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_testn_epi64_mask
#define _mm256_mask_testn_epi64_mask _mm256_mask_testn_epi64_mask_dbg


/*
 Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask8 _mm256_testn_epi64_mask_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 3; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_testn_epi64_mask
#define _mm256_testn_epi64_mask _mm256_testn_epi64_mask_dbg


/*
 Compute the bitwise NAND of packed 64-bit integers in "a" and "b", producing intermediate 64-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask8 _mm_mask_testn_epi64_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 1; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_testn_epi64_mask
#define _mm_mask_testn_epi64_mask _mm_mask_testn_epi64_mask_dbg


/*
 Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask16 _mm256_mask_testn_epi16_mask_dbg(__mmask16 k1, __m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    if (k1 & ((1 << j) & 0xffff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm256_mask_testn_epi16_mask
#define _mm256_mask_testn_epi16_mask _mm256_mask_testn_epi16_mask_dbg

/*
 Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask16 _mm256_testn_epi16_mask_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm256_testn_epi16_mask
#define _mm256_testn_epi16_mask _mm256_testn_epi16_mask_dbg


/*
 Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask32 _mm512_mask_testn_epi16_mask_dbg(__mmask32 k1, __m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    if (k1 & ((1 << j) & 0xffffffff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm512_mask_testn_epi16_mask
#define _mm512_mask_testn_epi16_mask _mm512_mask_testn_epi16_mask_dbg

/*
 Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" if the intermediate value is zero.
*/
static inline __mmask32 _mm512_testn_epi16_mask_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  __mmask32 k = 0;
  for (int j = 0; j <= 31; j++) {
    k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_testn_epi16_mask
#define _mm512_testn_epi16_mask _mm512_testn_epi16_mask_dbg


/*
 Compute the bitwise NAND of packed 16-bit integers in "a" and "b", producing intermediate 16-bit values, and set the corresponding bit in result mask "k" (subject to writemask "k") if the intermediate value is zero.
*/
static inline __mmask8 _mm_mask_testn_epi16_mask_dbg(__mmask8 k1, __m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    if (k1 & ((1 << j) & 0xff)) {
      k |= (((a_vec[j] & b_vec[j]) == 0) ? 1 : 0) << j;
    } else {
      k |= (0) << j;
    }
  }
  return k;
}

#undef _mm_mask_testn_epi16_mask
#define _mm_mask_testn_epi16_mask _mm_mask_testn_epi16_mask_dbg


/*
 Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_xor_epi32_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_xor_epi32
#define _mm256_mask_xor_epi32 _mm256_mask_xor_epi32_dbg


/*
 Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_xor_epi32_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_xor_epi32
#define _mm256_maskz_xor_epi32 _mm256_maskz_xor_epi32_dbg


/*
 Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_xor_epi32_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_xor_epi32
#define _mm_mask_xor_epi32 _mm_mask_xor_epi32_dbg


/*
 Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_xor_epi32_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_xor_epi32
#define _mm_maskz_xor_epi32 _mm_maskz_xor_epi32_dbg


/*
 Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256i _mm256_mask_xor_epi64_dbg(__m256i src, __mmask8 k, __m256i a, __m256i b)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_xor_epi64
#define _mm256_mask_xor_epi64 _mm256_mask_xor_epi64_dbg


/*
 Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m256i _mm256_maskz_xor_epi64_dbg(__mmask8 k, __m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_xor_epi64
#define _mm256_maskz_xor_epi64 _mm256_maskz_xor_epi64_dbg


/*
 Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128i _mm_mask_xor_epi64_dbg(__m128i src, __mmask8 k, __m128i a, __m128i b)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_xor_epi64
#define _mm_mask_xor_epi64 _mm_mask_xor_epi64_dbg


/*
 Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m128i _mm_maskz_xor_epi64_dbg(__mmask8 k, __m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] ^ b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_xor_epi64
#define _mm_maskz_xor_epi64 _mm_maskz_xor_epi64_dbg


/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	
*/
static inline __m256d _mm256_mask_range_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b, int imm8)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_range_pd
#define _mm256_mask_range_pd _mm256_mask_range_pd_dbg


/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	
*/
static inline __m256d _mm256_maskz_range_pd_dbg(__mmask8 k, __m256d a, __m256d b, int imm8)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_range_pd
#define _mm256_maskz_range_pd _mm256_maskz_range_pd_dbg


/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	
*/
static inline __m256d _mm256_range_pd_dbg(__m256d a, __m256d b, int imm8)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_range_pd
#define _mm256_range_pd _mm256_range_pd_dbg


/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	
*/
static inline __m512d _mm512_mask_range_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b, int imm8)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_range_pd
#define _mm512_mask_range_pd _mm512_mask_range_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/	
static inline __m512d _mm512_mask_range_round_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b, int imm8, int rounding)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_range_round_pd
#define _mm512_mask_range_round_pd _mm512_mask_range_round_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/	
static inline __m512d _mm512_maskz_range_pd_dbg(__mmask8 k, __m512d a, __m512d b, int imm8)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_range_pd
#define _mm512_maskz_range_pd _mm512_maskz_range_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_range_round_pd_dbg(__mmask8 k, __m512d a, __m512d b, int imm8, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_range_round_pd
#define _mm512_maskz_range_round_pd _mm512_maskz_range_round_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/	
static inline __m512d _mm512_range_pd_dbg(__m512d a, __m512d b, int imm8)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_range_pd
#define _mm512_range_pd _mm512_range_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_range_round_pd_dbg(__m512d a, __m512d b, int imm8, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_range_round_pd
#define _mm512_range_round_pd _mm512_range_round_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128d _mm_mask_range_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int imm8)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_range_pd
#define _mm_mask_range_pd _mm_mask_range_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128d _mm_maskz_range_pd_dbg(__mmask8 k, __m128d a, __m128d b, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_range_pd
#define _mm_maskz_range_pd _mm_maskz_range_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128d _mm_range_pd_dbg(__m128d a, __m128d b, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_range_pd
#define _mm_range_pd _mm_range_pd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m256 _mm256_mask_range_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b, int imm8)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_range_ps
#define _mm256_mask_range_ps _mm256_mask_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m256 _mm256_maskz_range_ps_dbg(__mmask8 k, __m256 a, __m256 b, int imm8)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_range_ps
#define _mm256_maskz_range_ps _mm256_maskz_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m256 _mm256_range_ps_dbg(__m256 a, __m256 b, int imm8)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_range_ps
#define _mm256_range_ps _mm256_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m512 _mm512_mask_range_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b, int imm8)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_range_ps
#define _mm512_mask_range_ps _mm512_mask_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask_range_round_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b, int imm8, int rounding)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_range_round_ps
#define _mm512_mask_range_round_ps _mm512_mask_range_round_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m512 _mm512_maskz_range_ps_dbg(__mmask16 k, __m512 a, __m512 b, int imm8)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_range_ps
#define _mm512_maskz_range_ps _mm512_maskz_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_range_round_ps_dbg(__mmask16 k, __m512 a, __m512 b, int imm8, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_range_round_ps
#define _mm512_maskz_range_round_ps _mm512_maskz_range_round_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m512 _mm512_range_ps_dbg(__m512 a, __m512 b, int imm8)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}


#undef _mm512_range_ps
#define _mm512_range_ps _mm512_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_range_round_ps_dbg(__m512 a, __m512 b, int imm8, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_range_round_ps
#define _mm512_range_round_ps _mm512_range_round_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128 _mm_mask_range_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int imm8)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_range_ps
#define _mm_mask_range_ps _mm_mask_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128 _mm_maskz_range_ps_dbg(__mmask8 k, __m128 a, __m128 b, int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = RANGE32(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_range_ps
#define _mm_maskz_range_ps _mm_maskz_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128 _mm_range_ps_dbg(__m128 a, __m128 b, int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = RANGE(a_vec[j], b_vec[j], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_range_ps
#define _mm_range_ps _mm_range_ps_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask_range_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int imm8, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_range_round_sd
#define _mm_mask_range_round_sd _mm_mask_range_round_sd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/	
static inline __m128d _mm_mask_range_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int imm8)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_range_sd
#define _mm_mask_range_sd _mm_mask_range_sd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_maskz_range_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int imm8, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_range_round_sd
#define _mm_maskz_range_round_sd _mm_maskz_range_round_sd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128d _mm_maskz_range_sd_dbg(__mmask8 k, __m128d a, __m128d b, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_range_sd
#define _mm_maskz_range_sd _mm_maskz_range_sd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower double-precision (64-bit) floating-point element in "a" and "b", store the result in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_range_round_sd_dbg(__m128d a, __m128d b, int imm8, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = RANGE(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_range_round_sd
#define _mm_range_round_sd _mm_range_round_sd_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask_range_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int imm8, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE32(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_range_round_ss
#define _mm_mask_range_round_ss _mm_mask_range_round_ss_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128 _mm_mask_range_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int imm8)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE32(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_range_ss
#define _mm_mask_range_ss _mm_mask_range_ss_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_maskz_range_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int imm8, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE32(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_range_round_ss
#define _mm_maskz_range_round_ss _mm_maskz_range_round_ss_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
*/
static inline __m128 _mm_maskz_range_ss_dbg(__mmask8 k, __m128 a, __m128 b, int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = RANGE32(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = a_vec[1];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_range_ss
#define _mm_maskz_range_ss _mm_maskz_range_ss_dbg

/*
 Calculate the max, min, absolute max, or absolute min (depending on control in "imm8") for the lower single-precision (32-bit) floating-point element in "a" and "b", store the result in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	imm8[1:0] specifies the operation control: 00 = min, 01 = max, 10 = absolute max, 11 = absolute min.
	imm8[3:2] specifies the sign control: 00 = sign from a, 01 = sign from compare result, 10 = clear sign bit, 11 = set sign bit.
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_range_round_ss_dbg(__m128 a, __m128 b, int imm8, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = RANGE32(a_vec[0], b_vec[0], (imm8 & 0x3) >> 0, (imm8 & 0xc) >> 2);
  dst_vec[1] = a_vec[1];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_range_round_ss
#define _mm_range_round_ss _mm_range_round_ss_dbg

/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256d _mm256_mask_rcp14_pd_dbg(__m256d src, __mmask8 k, __m256d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_rcp14_pd
#define _mm256_mask_rcp14_pd _mm256_mask_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256d _mm256_maskz_rcp14_pd_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_rcp14_pd
#define _mm256_maskz_rcp14_pd _mm256_maskz_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256d _mm256_rcp14_pd_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_rcp14_pd
#define _mm256_rcp14_pd _mm256_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_mask_rcp14_pd_dbg(__m128d src, __mmask8 k, __m128d a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_rcp14_pd
#define _mm_mask_rcp14_pd _mm_mask_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_maskz_rcp14_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_rcp14_pd
#define _mm_maskz_rcp14_pd _mm_maskz_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_rcp14_pd_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_rcp14_pd
#define _mm_rcp14_pd _mm_rcp14_pd_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256 _mm256_mask_rcp14_ps_dbg(__m256 src, __mmask8 k, __m256 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_rcp14_ps
#define _mm256_mask_rcp14_ps _mm256_mask_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256 _mm256_maskz_rcp14_ps_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_rcp14_ps
#define _mm256_maskz_rcp14_ps _mm256_maskz_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256 _mm256_rcp14_ps_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_rcp14_ps
#define _mm256_rcp14_ps _mm256_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_mask_rcp14_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_rcp14_ps
#define _mm_mask_rcp14_ps _mm_mask_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_maskz_rcp14_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_rcp14_ps
#define _mm_maskz_rcp14_ps _mm_maskz_rcp14_ps_dbg


/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_rcp14_ps_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_rcp14_ps
#define _mm_rcp14_ps _mm_rcp14_ps_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_reduce_pd_dbg(__m256d src, __mmask8 k, __m256d a, int imm8)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_reduce_pd
#define _mm256_mask_reduce_pd _mm256_mask_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_maskz_reduce_pd_dbg(__mmask8 k, __m256d a, int imm8)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_reduce_pd
#define _mm256_maskz_reduce_pd _mm256_maskz_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst". 
*/
static inline __m256d _mm256_reduce_pd_dbg(__m256d a, int imm8)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_reduce_pd
#define _mm256_reduce_pd _mm256_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_reduce_pd_dbg(__m512d src, __mmask8 k, __m512d a, int imm8)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_reduce_pd
#define _mm512_mask_reduce_pd _mm512_mask_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_mask_reduce_round_pd_dbg(__m512d src, __mmask8 k, __m512d a, int imm8, int rounding)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_reduce_round_pd
#define _mm512_mask_reduce_round_pd _mm512_mask_reduce_round_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_maskz_reduce_pd_dbg(__mmask8 k, __m512d a, int imm8)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_reduce_pd
#define _mm512_maskz_reduce_pd _mm512_maskz_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_maskz_reduce_round_pd_dbg(__mmask8 k, __m512d a, int imm8, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_reduce_round_pd
#define _mm512_maskz_reduce_round_pd _mm512_maskz_reduce_round_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst". 
*/
static inline __m512d _mm512_reduce_pd_dbg(__m512d a, int imm8)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_reduce_pd
#define _mm512_reduce_pd _mm512_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512d _mm512_reduce_round_pd_dbg(__m512d a, int imm8, int rounding)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_reduce_round_pd
#define _mm512_reduce_round_pd _mm512_reduce_round_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_reduce_pd_dbg(__m128d src, __mmask8 k, __m128d a, int imm8)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_reduce_pd
#define _mm_mask_reduce_pd _mm_mask_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_maskz_reduce_pd_dbg(__mmask8 k, __m128d a, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_reduce_pd
#define _mm_maskz_reduce_pd _mm_maskz_reduce_pd_dbg


/*
 Extract the reduced argument of packed double-precision (64-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst". 
*/
static inline __m128d _mm_reduce_pd_dbg(__m128d a, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ReduceArgumentPD(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_reduce_pd
#define _mm_reduce_pd _mm_reduce_pd_dbg


/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_reduce_ps_dbg(__m256 src, __mmask8 k, __m256 a, int imm8)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_reduce_ps
#define _mm256_mask_reduce_ps _mm256_mask_reduce_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_reduce_ps_dbg(__mmask8 k, __m256 a, int imm8)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_reduce_ps
#define _mm256_maskz_reduce_ps _mm256_maskz_reduce_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst".
*/
static inline __m256 _mm256_reduce_ps_dbg(__m256 a, int imm8)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_reduce_ps
#define _mm256_reduce_ps _mm256_reduce_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_reduce_ps_dbg(__m512 src, __mmask16 k, __m512 a, int imm8)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_reduce_ps
#define _mm512_mask_reduce_ps _mm512_mask_reduce_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask_reduce_round_ps_dbg(__m512 src, __mmask16 k, __m512 a, int imm8, int rounding)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_reduce_round_ps
#define _mm512_mask_reduce_round_ps _mm512_mask_reduce_round_ps_dbg


/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_maskz_reduce_ps_dbg(__mmask16 k, __m512 a, int imm8)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_reduce_ps
#define _mm512_maskz_reduce_ps _mm512_maskz_reduce_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_maskz_reduce_round_ps_dbg(__mmask16 k, __m512 a, int imm8, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_reduce_round_ps
#define _mm512_maskz_reduce_round_ps _mm512_maskz_reduce_round_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst".
*/
static inline __m512 _mm512_reduce_ps_dbg(__m512 a, int imm8)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_reduce_ps
#define _mm512_reduce_ps _mm512_reduce_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_reduce_round_ps_dbg(__m512 a, int imm8, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_reduce_round_ps
#define _mm512_reduce_round_ps _mm512_reduce_round_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_reduce_ps_dbg(__m128 src, __mmask8 k, __m128 a, int imm8)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_reduce_ps
#define _mm_mask_reduce_ps _mm_mask_reduce_ps_dbg

/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_reduce_ps_dbg(__mmask8 k, __m128 a, int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_reduce_ps
#define _mm_maskz_reduce_ps _mm_maskz_reduce_ps_dbg


/*
 Extract the reduced argument of packed single-precision (32-bit) floating-point elements in "a" by the number of bits specified by "imm8", and store the results in "dst".
*/
static inline __m128 _mm_reduce_ps_dbg(__m128 a, int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ReduceArgumentPS(a_vec[j], (imm8 & 0xff) >> 0);
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_reduce_ps
#define _mm_reduce_ps _mm_reduce_ps_dbg

/*
 Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "b" to the upper element of "dst".
*/
static inline __m128d _mm_mask_reduce_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int imm8)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPD(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = b_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_reduce_sd
#define _mm_mask_reduce_sd _mm_mask_reduce_sd_dbg


/*
 Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper element from "b" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_mask_reduce_round_sd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b, int imm8, int rounding)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPD(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = b_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_reduce_round_sd
#define _mm_mask_reduce_round_sd _mm_mask_reduce_round_sd_dbg


/*
 Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "b" to the upper element of "dst".
*/
static inline __m128d _mm_maskz_reduce_sd_dbg(__mmask8 k, __m128d a, __m128d b, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPD(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = b_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_reduce_sd
#define _mm_maskz_reduce_sd _mm_maskz_reduce_sd_dbg


/*
 Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper element from "b" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_maskz_reduce_round_sd_dbg(__mmask8 k, __m128d a, __m128d b, int imm8, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPD(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = b_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_reduce_round_sd
#define _mm_maskz_reduce_round_sd _mm_maskz_reduce_round_sd_dbg


/*
 Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper element from "b" to the upper element of "dst".
*/
static inline __m128d _mm_reduce_sd_dbg(__m128d a, __m128d b, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = ReduceArgumentPD(a_vec[0], (imm8 & 0xff) >> 0);
  dst_vec[1] = b_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_reduce_sd
#define _mm_reduce_sd _mm_reduce_sd_dbg


/*
 Extract the reduced argument of the lower double-precision (64-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper element from "b" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128d _mm_reduce_round_sd_dbg(__m128d a, __m128d b, int imm8, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = ReduceArgumentPD(a_vec[0], (imm8 & 0xff) >> 0);
  dst_vec[1] = b_vec[1];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_reduce_round_sd
#define _mm_reduce_round_sd _mm_reduce_round_sd_dbg


/*
 Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
*/
static inline __m128 _mm_mask_reduce_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int imm8)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPS(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = b_vec[1];
  dst_vec[2] = b_vec[2];
  dst_vec[3] = b_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_reduce_ss
#define _mm_mask_reduce_ss _mm_mask_reduce_ss_dbg

/*
 Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using writemask "k" (the element is copied from "src" when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_mask_reduce_round_ss_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b, int imm8, int rounding)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPS(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = src_vec[0];
  }
  dst_vec[1] = b_vec[1];
  dst_vec[2] = b_vec[2];
  dst_vec[3] = b_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_reduce_round_ss
#define _mm_mask_reduce_round_ss _mm_mask_reduce_round_ss_dbg

/*
 Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
*/
static inline __m128 _mm_maskz_reduce_ss_dbg(__mmask8 k, __m128 a, __m128 b, int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPS(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = b_vec[1];
  dst_vec[2] = b_vec[2];
  dst_vec[3] = b_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_reduce_ss
#define _mm_maskz_reduce_ss _mm_maskz_reduce_ss_dbg

/*
 Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst" using zeromask "k" (the element is zeroed out when mask bit 0 is not set), and copy the upper 3 packed elements from "b" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_maskz_reduce_round_ss_dbg(__mmask8 k, __m128 a, __m128 b, int imm8, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  if (k & ((1 << 0) & 0xff)) {
    dst_vec[0] = ReduceArgumentPS(a_vec[0], (imm8 & 0xff) >> 0);
  } else {
    dst_vec[0] = 0;
  }
  dst_vec[1] = b_vec[1];
  dst_vec[2] = b_vec[2];
  dst_vec[3] = b_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_reduce_round_ss
#define _mm_maskz_reduce_round_ss _mm_maskz_reduce_round_ss_dbg

/*
 Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper 3 packed elements from "b" to the upper elements of "dst".
*/
static inline __m128 _mm_reduce_ss_dbg(__m128 a, __m128 b, int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = ReduceArgumentPS(a_vec[0], imm8 & 0xff);
  dst_vec[1] = b_vec[1];
  dst_vec[2] = b_vec[2];
  dst_vec[3] = b_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_reduce_ss
#define _mm_reduce_ss _mm_reduce_ss_dbg

/*
 Extract the reduced argument of the lower single-precision (32-bit) floating-point element in "a" by the number of bits specified by "imm8", store the result in the lower element of "dst", and copy the upper 3 packed elements from "b" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m128 _mm_reduce_round_ss_dbg(__m128 a, __m128 b, int imm8, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = ReduceArgumentPS(a_vec[0], imm8 & 0xff);
  dst_vec[1] = b_vec[1];
  dst_vec[2] = b_vec[2];
  dst_vec[3] = b_vec[3];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_reduce_round_ss
#define _mm_reduce_round_ss _mm_reduce_round_ss_dbg

/*
 Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256d _mm256_mask_rsqrt14_pd_dbg(__m256d src, __mmask8 k, __m256d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_rsqrt14_pd
#define _mm256_mask_rsqrt14_pd _mm256_mask_rsqrt14_pd_dbg


/*
 Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256d _mm256_maskz_rsqrt14_pd_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_rsqrt14_pd
#define _mm256_maskz_rsqrt14_pd _mm256_maskz_rsqrt14_pd_dbg


/*
 Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_mask_rsqrt14_pd_dbg(__m128d src, __mmask8 k, __m128d a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_rsqrt14_pd
#define _mm_mask_rsqrt14_pd _mm_mask_rsqrt14_pd_dbg


/*
 Compute the approximate reciprocal square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128d _mm_maskz_rsqrt14_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_rsqrt14_pd
#define _mm_maskz_rsqrt14_pd _mm_maskz_rsqrt14_pd_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256 _mm256_mask_rsqrt14_ps_dbg(__m256 src, __mmask8 k, __m256 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_rsqrt14_ps
#define _mm256_mask_rsqrt14_ps _mm256_mask_rsqrt14_ps_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m256 _mm256_maskz_rsqrt14_ps_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_rsqrt14_ps
#define _mm256_maskz_rsqrt14_ps _mm256_maskz_rsqrt14_ps_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_mask_rsqrt14_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_rsqrt14_ps
#define _mm_mask_rsqrt14_ps _mm_mask_rsqrt14_ps_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). The maximum relative error for this approximation is less than 2^-14.
*/
static inline __m128 _mm_maskz_rsqrt14_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_rsqrt14_ps
#define _mm_maskz_rsqrt14_ps _mm_maskz_rsqrt14_ps_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_mask_sqrt_pd_dbg(__m256d src, __mmask8 k, __m256d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_sqrt_pd
#define _mm256_mask_sqrt_pd _mm256_mask_sqrt_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_sqrt_pd_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_sqrt_pd
#define _mm256_maskz_sqrt_pd _mm256_maskz_sqrt_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128d _mm_mask_sqrt_pd_dbg(__m128d src, __mmask8 k, __m128d a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_sqrt_pd
#define _mm_mask_sqrt_pd _mm_mask_sqrt_pd_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_sqrt_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_sqrt_pd
#define _mm_maskz_sqrt_pd _mm_maskz_sqrt_pd_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_mask_sqrt_ps_dbg(__m256 src, __mmask8 k, __m256 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_sqrt_ps
#define _mm256_mask_sqrt_ps _mm256_mask_sqrt_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_sqrt_ps_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_sqrt_ps
#define _mm256_maskz_sqrt_ps _mm256_maskz_sqrt_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128 _mm_mask_sqrt_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_sqrt_ps
#define _mm_mask_sqrt_ps _mm_mask_sqrt_ps_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_sqrt_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = sqrt(a_vec[j]);
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_sqrt_ps
#define _mm_maskz_sqrt_ps _mm_maskz_sqrt_ps_dbg


/*
 Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_mask_sub_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_sub_pd
#define _mm256_mask_sub_pd _mm256_mask_sub_pd_dbg


/*
 Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_sub_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_sub_pd
#define _mm256_maskz_sub_pd _mm256_maskz_sub_pd_dbg


/*
 Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128d _mm_mask_sub_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_sub_pd
#define _mm_mask_sub_pd _mm_mask_sub_pd_dbg


/*
 Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_sub_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_sub_pd
#define _mm_maskz_sub_pd _mm_maskz_sub_pd_dbg


/*
 Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_mask_sub_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_sub_ps
#define _mm256_mask_sub_ps _mm256_mask_sub_ps_dbg


/*
 Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_sub_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_sub_ps
#define _mm256_maskz_sub_ps _mm256_maskz_sub_ps_dbg


/*
 Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m128 _mm_mask_sub_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_sub_ps
#define _mm_mask_sub_ps _mm_mask_sub_ps_dbg


/*
 Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_sub_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_sub_ps
#define _mm_maskz_sub_ps _mm_maskz_sub_ps_dbg


/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256d _mm256_mask_xor_pd_dbg(__m256d src, __mmask8 k, __m256d a, __m256d b)
{
  uint64_t src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_xor_pd
#define _mm256_mask_xor_pd _mm256_mask_xor_pd_dbg


/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256d _mm256_maskz_xor_pd_dbg(__mmask8 k, __m256d a, __m256d b)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_xor_pd
#define _mm256_maskz_xor_pd _mm256_maskz_xor_pd_dbg


/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512d _mm512_mask_xor_pd_dbg(__m512d src, __mmask8 k, __m512d a, __m512d b)
{
  uint64_t src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_xor_pd
#define _mm512_mask_xor_pd _mm512_mask_xor_pd_dbg


/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512d _mm512_maskz_xor_pd_dbg(__mmask8 k, __m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_maskz_xor_pd
#define _mm512_maskz_xor_pd _mm512_maskz_xor_pd_dbg


/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m512d _mm512_xor_pd_dbg(__m512d a, __m512d b)
{
  uint64_t a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  uint64_t b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  uint64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_xor_pd
#define _mm512_xor_pd _mm512_xor_pd_dbg

/*
 Compute the bitwise XOR of packed 32-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_xor_epi32_dbg(__m512i a, __m512i b)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_si512((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] ^ b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_xor_epi32
#define _mm512_xor_epi32 _mm512_xor_epi32_dbg

/*
 Compute the bitwise XOR of 512 bits (representing integer data) in "a" and "b", and store the result in "dst".
*/
static inline __m512i _mm512_xor_si512_dbg(__m512i a, __m512i b)
{
  return _mm512_xor_epi32(a, b);
}

#undef _mm512_xor_si512
#define _mm512_xor_si512 _mm512_xor_si512_dbg

/*
 Compute the bitwise XOR of packed 64-bit integers in "a" and "b", and store the results in "dst".

*/
static inline __m512i _mm512_xor_epi64_dbg(__m512i a, __m512i b)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t b_vec[8];
  _mm512_storeu_si512((void*)b_vec, b);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] ^ b_vec[j];
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_xor_epi64
#define _mm512_xor_epi64 _mm512_xor_epi64_dbg

/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128d _mm_mask_xor_pd_dbg(__m128d src, __mmask8 k, __m128d a, __m128d b)
{
  uint64_t src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_xor_pd
#define _mm_mask_xor_pd _mm_mask_xor_pd_dbg


/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128d _mm_maskz_xor_pd_dbg(__mmask8 k, __m128d a, __m128d b)
{
  uint64_t a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_xor_pd
#define _mm_maskz_xor_pd _mm_maskz_xor_pd_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m256 _mm256_mask_xor_ps_dbg(__m256 src, __mmask8 k, __m256 a, __m256 b)
{
  int32_t src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_xor_ps
#define _mm256_mask_xor_ps _mm256_mask_xor_ps_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m256 _mm256_maskz_xor_ps_dbg(__mmask8 k, __m256 a, __m256 b)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_xor_ps
#define _mm256_maskz_xor_ps _mm256_maskz_xor_ps_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m512 _mm512_mask_xor_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  int32_t src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_xor_ps
#define _mm512_mask_xor_ps _mm512_mask_xor_ps_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
	
*/
static inline __m512 _mm512_maskz_xor_ps_dbg(__mmask16 k, __m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_xor_ps
#define _mm512_maskz_xor_ps _mm512_maskz_xor_ps_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m512 _mm512_xor_ps_dbg(__m512 a, __m512 b)
{
  int32_t a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  int32_t b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_xor_ps
#define _mm512_xor_ps _mm512_xor_ps_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). 
*/
static inline __m128 _mm_mask_xor_ps_dbg(__m128 src, __mmask8 k, __m128 a, __m128 b)
{
  int32_t src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_xor_ps
#define _mm_mask_xor_ps _mm_mask_xor_ps_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m128 _mm_maskz_xor_ps_dbg(__mmask8 k, __m128 a, __m128 b)
{
  int32_t a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_xor_ps
#define _mm_maskz_xor_ps _mm_maskz_xor_ps_dbg


/*
 Compute the absolute value of packed 8-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m256i _mm256_abs_epi8_dbg(__m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = abs(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_abs_epi8
#define _mm256_abs_epi8 _mm256_abs_epi8_dbg


/*
 Compute the absolute value of packed 16-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m256i _mm256_abs_epi16_dbg(__m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = abs(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_abs_epi16
#define _mm256_abs_epi16 _mm256_abs_epi16_dbg


/*
 Compute the absolute value of packed 32-bit integers in "a", and store the unsigned results in "dst". 
*/
static inline __m256i _mm256_abs_epi32_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = abs(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_abs_epi32
#define _mm256_abs_epi32 _mm256_abs_epi32_dbg


/*
 Add packed 8-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m256i _mm256_add_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_add_epi8
#define _mm256_add_epi8 _mm256_add_epi8_dbg


/*
 Add packed 16-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m256i _mm256_add_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_add_epi16
#define _mm256_add_epi16 _mm256_add_epi16_dbg


/*
 Add packed 32-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m256i _mm256_add_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_add_epi32
#define _mm256_add_epi32 _mm256_add_epi32_dbg


/*
 Add packed 64-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m256i _mm256_add_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_add_epi64
#define _mm256_add_epi64 _mm256_add_epi64_dbg


/*
 Add packed 8-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_adds_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_Int8((int16_t) a_vec[j] + b_vec[j] );
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_adds_epi8
#define _mm256_adds_epi8 _mm256_adds_epi8_dbg


/*
 Add packed 16-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_adds_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_To_Int16( a_vec[j] + b_vec[j] );
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_adds_epi16
#define _mm256_adds_epi16 _mm256_adds_epi16_dbg


/*
 Add packed unsigned 8-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_adds_epu8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t) a_vec[j] + b_vec[j] );
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_adds_epu8
#define _mm256_adds_epu8 _mm256_adds_epu8_dbg


/*
 Add packed unsigned 16-bit integers in "a" and "b" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_adds_epu16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t) a_vec[j] + b_vec[j] );
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_adds_epu16
#define _mm256_adds_epu16 _mm256_adds_epu16_dbg


/*
 Compute the bitwise AND of 256 bits (representing integer data) in "a" and "b", and store the result in "dst".
*/
static inline __m256i _mm256_and_si256_dbg(__m256i a, __m256i b)
{
  __m256i a_vec[1];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  __m256i b_vec[1];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __m256i dst_vec[1];
  dst_vec[0] = (a_vec[0] & b_vec[0]);
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_and_si256
#define _mm256_and_si256 _mm256_and_si256_dbg


/*
 Average packed unsigned 8-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m256i _mm256_avg_epu8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_avg_epu8
#define _mm256_avg_epu8 _mm256_avg_epu8_dbg


/*
 Average packed unsigned 16-bit integers in "a" and "b", and store the results in "dst".
*/
static inline __m256i _mm256_avg_epu16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = (a_vec[j] + b_vec[j] + 1) >> 1;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_avg_epu16
#define _mm256_avg_epu16 _mm256_avg_epu16_dbg


/*
 Blend packed 16-bit integers from "a" and "b" within 128-bit lanes using control mask "imm8", and store the results in "dst".
*/
static inline __m256i _mm256_blend_epi16_dbg(__m256i a, __m256i b, const int imm8)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (imm8 & ((1 << 8) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_blend_epi16
#define _mm256_blend_epi16 _mm256_blend_epi16_dbg


/*
 Blend packed 32-bit integers from "a" and "b" using control mask "imm8", and store the results in "dst".
*/
static inline __m128i _mm_blend_epi32_dbg(__m128i a, __m128i b, const int imm8)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (imm8 & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_blend_epi32
#define _mm_blend_epi32 _mm_blend_epi32_dbg


/*
 Blend packed 32-bit integers from "a" and "b" using control mask "imm8", and store the results in "dst".
*/
static inline __m256i _mm256_blend_epi32_dbg(__m256i a, __m256i b, const int imm8)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (imm8 & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_blend_epi32
#define _mm256_blend_epi32 _mm256_blend_epi32_dbg


/*
 Blend packed 8-bit integers from "a" and "b" using "mask", and store the results in "dst".
*/
static inline __m256i _mm256_blendv_epi8_dbg(__m256i a, __m256i b, __m256i mask)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t mask_vec[32];
  _mm256_storeu_si256((__m256i*)mask_vec, mask);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (mask_vec[j]) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_blendv_epi8
#define _mm256_blendv_epi8 _mm256_blendv_epi8_dbg

/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst".
*/
static inline __m128i _mm_broadcastb_epi8_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_broadcastb_epi8
#define _mm_broadcastb_epi8 _mm_broadcastb_epi8_dbg


/*
 Broadcast the low packed 8-bit integer from "a" to all elements of "dst".
*/
static inline __m256i _mm256_broadcastb_epi8_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcastb_epi8
#define _mm256_broadcastb_epi8 _mm256_broadcastb_epi8_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst".
*/
static inline __m128i _mm_broadcastd_epi32_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_broadcastd_epi32
#define _mm_broadcastd_epi32 _mm_broadcastd_epi32_dbg


/*
 Broadcast the low packed 32-bit integer from "a" to all elements of "dst".
*/
static inline __m256i _mm256_broadcastd_epi32_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcastd_epi32
#define _mm256_broadcastd_epi32 _mm256_broadcastd_epi32_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst". 
*/
static inline __m128i _mm_broadcastq_epi64_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_broadcastq_epi64
#define _mm_broadcastq_epi64 _mm_broadcastq_epi64_dbg


/*
 Broadcast the low packed 64-bit integer from "a" to all elements of "dst". 
*/
static inline __m256i _mm256_broadcastq_epi64_dbg(__m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcastq_epi64
#define _mm256_broadcastq_epi64 _mm256_broadcastq_epi64_dbg


/*
 Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst".
*/
static inline __m128d _mm_broadcastsd_pd_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_broadcastsd_pd
#define _mm_broadcastsd_pd _mm_broadcastsd_pd_dbg


/*
 Broadcast the low double-precision (64-bit) floating-point element from "a" to all elements of "dst".
*/
static inline __m256d _mm256_broadcastsd_pd_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_broadcastsd_pd
#define _mm256_broadcastsd_pd _mm256_broadcastsd_pd_dbg


/*
 Broadcast 128 bits of integer data from "a" to all 128-bit lanes in "dst".
	
*/
static inline __m256i _mm_broadcastsi128_si256_dbg(__m128i a)
{
  __m128i a_vec[1];
  _mm_storeu_si128((__m128i*)a_vec, a);
  __m128i dst_vec[2];
  dst_vec[0] = a_vec[0];
  dst_vec[1] = a_vec[0];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm_broadcastsi128_si256
#define _mm_broadcastsi128_si256 _mm_broadcastsi128_si256_dbg


/*
 Broadcast 128 bits of integer data from "a" to all 128-bit lanes in "dst".
	
*/
static inline __m256i _mm256_broadcastsi128_si256_dbg(__m128i a)
{
  __m128i a_vec[1];
  _mm_storeu_si128((__m128i*)a_vec, a);
  __m128i dst_vec[2];
  dst_vec[0] = a_vec[0];
  dst_vec[1] = a_vec[0];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcastsi128_si256
#define _mm256_broadcastsi128_si256 _mm256_broadcastsi128_si256_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst".
*/
static inline __m128 _mm_broadcastss_ps_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_broadcastss_ps
#define _mm_broadcastss_ps _mm_broadcastss_ps_dbg


/*
 Broadcast the low single-precision (32-bit) floating-point element from "a" to all elements of "dst".
*/
static inline __m256 _mm256_broadcastss_ps_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_broadcastss_ps
#define _mm256_broadcastss_ps _mm256_broadcastss_ps_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst".
*/
static inline __m128i _mm_broadcastw_epi16_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_broadcastw_epi16
#define _mm_broadcastw_epi16 _mm_broadcastw_epi16_dbg


/*
 Broadcast the low packed 16-bit integer from "a" to all elements of "dst".
*/
static inline __m256i _mm256_broadcastw_epi16_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[0];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_broadcastw_epi16
#define _mm256_broadcastw_epi16 _mm256_broadcastw_epi16_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for equality, and store the results in "dst".
*/
static inline __m256i _mm256_cmpeq_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = ( a_vec[j] == b_vec[j] ) ? 0xFF : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpeq_epi8
#define _mm256_cmpeq_epi8 _mm256_cmpeq_epi8_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for equality, and store the results in "dst".
*/
static inline __m256i _mm256_cmpeq_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ( a_vec[j] == b_vec[j] ) ? 0xFFFF : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpeq_epi16
#define _mm256_cmpeq_epi16 _mm256_cmpeq_epi16_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for equality, and store the results in "dst".
*/
static inline __m256i _mm256_cmpeq_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ( a_vec[j] == b_vec[j] ) ? 0xFFFFFFFF : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpeq_epi32
#define _mm256_cmpeq_epi32 _mm256_cmpeq_epi32_dbg


/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in "dst".
*/
static inline __m256i _mm256_cmpeq_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ( a_vec[j] == b_vec[j] ) ? 0xFFFFFFFFFFFFFFFFULL : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpeq_epi64
#define _mm256_cmpeq_epi64 _mm256_cmpeq_epi64_dbg


/*
 Compare packed 8-bit integers in "a" and "b" for greater-than, and store the results in "dst".
*/
static inline __m256i _mm256_cmpgt_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = ( a_vec[j] > b_vec[j] ) ? 0xFF : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpgt_epi8
#define _mm256_cmpgt_epi8 _mm256_cmpgt_epi8_dbg


/*
 Compare packed 16-bit integers in "a" and "b" for greater-than, and store the results in "dst".
*/
static inline __m256i _mm256_cmpgt_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ( a_vec[j] > b_vec[j] ) ? 0xFFFF : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpgt_epi16
#define _mm256_cmpgt_epi16 _mm256_cmpgt_epi16_dbg


/*
 Compare packed 32-bit integers in "a" and "b" for greater-than, and store the results in "dst".
*/
static inline __m256i _mm256_cmpgt_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ( a_vec[j] > b_vec[j] ) ? 0xFFFFFFFF : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpgt_epi32
#define _mm256_cmpgt_epi32 _mm256_cmpgt_epi32_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in "dst".
*/
static inline __m256i _mm256_cmpgt_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ( a_vec[j] > b_vec[j] ) ? 0xFFFFFFFFFFFFFFFFULL : 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cmpgt_epi64
#define _mm256_cmpgt_epi64 _mm256_cmpgt_epi64_dbg

/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepi16_epi32_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepi16_epi32
#define _mm256_cvtepi16_epi32 _mm256_cvtepi16_epi32_dbg


/*
 Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepi16_epi64_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepi16_epi64
#define _mm256_cvtepi16_epi64 _mm256_cvtepi16_epi64_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepi32_epi64_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepi32_epi64
#define _mm256_cvtepi32_epi64 _mm256_cvtepi32_epi64_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepi8_epi16_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepi8_epi16
#define _mm256_cvtepi8_epi16 _mm256_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepi8_epi32_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepi8_epi32
#define _mm256_cvtepi8_epi32 _mm256_cvtepi8_epi32_dbg

/*
 Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepi8_epi64_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepi8_epi64
#define _mm256_cvtepi8_epi64 _mm256_cvtepi8_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepu16_epi32_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepu16_epi32
#define _mm256_cvtepu16_epi32 _mm256_cvtepu16_epi32_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepu16_epi64_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepu16_epi64
#define _mm256_cvtepu16_epi64 _mm256_cvtepu16_epi64_dbg


/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepu32_epi64_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepu32_epi64
#define _mm256_cvtepu32_epi64 _mm256_cvtepu32_epi64_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepu8_epi16_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepu8_epi16
#define _mm256_cvtepu8_epi16 _mm256_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepu8_epi32_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepu8_epi32
#define _mm256_cvtepu8_epi32 _mm256_cvtepu8_epi32_dbg

/*
 Zero extend packed unsigned 8-bit integers in the low 8 byte sof "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtepu8_epi64_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtepu8_epi64
#define _mm256_cvtepu8_epi64 _mm256_cvtepu8_epi64_dbg

/*
 Horizontally add adjacent pairs of 16-bit integers in "a" and "b", and pack the signed 16-bit results in "dst".
*/
static inline __m256i _mm256_hadd_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  dst_vec[0] = a_vec[1] + a_vec[0];
  dst_vec[1] = a_vec[3] + a_vec[2];
  dst_vec[2] = a_vec[5] + a_vec[4];
  dst_vec[3] = a_vec[7] + a_vec[6];
  dst_vec[4] = b_vec[1] + b_vec[0];
  dst_vec[5] = b_vec[3] + b_vec[2];
  dst_vec[6] = b_vec[5] + b_vec[4];
  dst_vec[7] = b_vec[7] + b_vec[6];
  dst_vec[8] = a_vec[9] + a_vec[8];
  dst_vec[9] = a_vec[11] + a_vec[10];
  dst_vec[10] = a_vec[13] + a_vec[12];
  dst_vec[11] = a_vec[15] + a_vec[14];
  dst_vec[12] = b_vec[7] + b_vec[8];
  dst_vec[13] = b_vec[9] + b_vec[10];
  dst_vec[14] = b_vec[11] + b_vec[12];
  dst_vec[15] = b_vec[13] + b_vec[14];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_hadd_epi16
#define _mm256_hadd_epi16 _mm256_hadd_epi16_dbg


/*
 Horizontally add adjacent pairs of 32-bit integers in "a" and "b", and pack the signed 32-bit results in "dst".
*/
static inline __m256i _mm256_hadd_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  dst_vec[0] = a_vec[1] + a_vec[0];
  dst_vec[1] = a_vec[3] + a_vec[2];
  dst_vec[2] = b_vec[1] + b_vec[0];
  dst_vec[3] = b_vec[3] + b_vec[2];
  dst_vec[4] = a_vec[5] + a_vec[4];
  dst_vec[5] = a_vec[7] + a_vec[6];
  dst_vec[6] = b_vec[5] + b_vec[4];
  dst_vec[7] = b_vec[7] + b_vec[6];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_hadd_epi32
#define _mm256_hadd_epi32 _mm256_hadd_epi32_dbg


/*
 Horizontally add adjacent pairs of 16-bit integers in "a" and "b" using saturation, and pack the signed 16-bit results in "dst".
*/
static inline __m256i _mm256_hadds_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  dst_vec[0] = Saturate_To_Int16(a_vec[1] + a_vec[0]);
  dst_vec[1] = Saturate_To_Int16(a_vec[3] + a_vec[2]);
  dst_vec[2] = Saturate_To_Int16(a_vec[5] + a_vec[4]);
  dst_vec[3] = Saturate_To_Int16(a_vec[7] + a_vec[6]);
  dst_vec[4] = Saturate_To_Int16(b_vec[1] + b_vec[0]);
  dst_vec[5] = Saturate_To_Int16(b_vec[3] + b_vec[2]);
  dst_vec[6] = Saturate_To_Int16(b_vec[5] + b_vec[4]);
  dst_vec[7] = Saturate_To_Int16(b_vec[7] + b_vec[6]);
  dst_vec[8] = Saturate_To_Int16(a_vec[9] + a_vec[8]);
  dst_vec[9] = Saturate_To_Int16(a_vec[11] + a_vec[10]);
  dst_vec[10] = Saturate_To_Int16(a_vec[13] + a_vec[12]);
  dst_vec[11] = Saturate_To_Int16(a_vec[15] + a_vec[14]);
  dst_vec[12] = Saturate_To_Int16(b_vec[8] + b_vec[9]);
  dst_vec[13] = Saturate_To_Int16(b_vec[11] + b_vec[10]);
  dst_vec[14] = Saturate_To_Int16(b_vec[13] + b_vec[12]);
  dst_vec[15] = Saturate_To_Int16(b_vec[15] + b_vec[14]);
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_hadds_epi16
#define _mm256_hadds_epi16 _mm256_hadds_epi16_dbg


/*
 Horizontally subtract adjacent pairs of 16-bit integers in "a" and "b", and pack the signed 16-bit results in "dst".
*/
static inline __m256i _mm256_hsub_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  dst_vec[0] = a_vec[0] - a_vec[1];
  dst_vec[1] = a_vec[2] - a_vec[3];
  dst_vec[2] = a_vec[4] - a_vec[5];
  dst_vec[3] = a_vec[6] - a_vec[7];
  dst_vec[4] = b_vec[0] - b_vec[1];
  dst_vec[5] = b_vec[2] - b_vec[3];
  dst_vec[6] = b_vec[4] - b_vec[5];
  dst_vec[7] = b_vec[6] - b_vec[7];
  dst_vec[8] = a_vec[8] - a_vec[9];
  dst_vec[9] = a_vec[10] - a_vec[11];
  dst_vec[10] = a_vec[12] - a_vec[13];
  dst_vec[11] = a_vec[14] - a_vec[15];
  dst_vec[12] = b_vec[8] - b_vec[9];
  dst_vec[13] = b_vec[10] - b_vec[11];
  dst_vec[14] = b_vec[12] - b_vec[13];
  dst_vec[15] = b_vec[14] - b_vec[15];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_hsub_epi16
#define _mm256_hsub_epi16 _mm256_hsub_epi16_dbg


/*
 Horizontally subtract adjacent pairs of 32-bit integers in "a" and "b", and pack the signed 32-bit results in "dst".
*/
static inline __m256i _mm256_hsub_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  dst_vec[0] = a_vec[0] - a_vec[1];
  dst_vec[1] = a_vec[2] - a_vec[3];
  dst_vec[2] = b_vec[0] - b_vec[1];
  dst_vec[3] = b_vec[2] - b_vec[3];
  dst_vec[4] = a_vec[4] - a_vec[5];
  dst_vec[5] = a_vec[6] - a_vec[7];
  dst_vec[6] = b_vec[4] - b_vec[5];
  dst_vec[7] = b_vec[6] - b_vec[7];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_hsub_epi32
#define _mm256_hsub_epi32 _mm256_hsub_epi32_dbg


/*
 Horizontally subtract adjacent pairs of 16-bit integers in "a" and "b" using saturation, and pack the signed 16-bit results in "dst".
*/
static inline __m256i _mm256_hsubs_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  dst_vec[0] = Saturate_To_Int16(a_vec[0] - a_vec[1]);
  dst_vec[1] = Saturate_To_Int16(a_vec[2] - a_vec[3]);
  dst_vec[2] = Saturate_To_Int16(a_vec[4] - a_vec[5]);
  dst_vec[3] = Saturate_To_Int16(a_vec[6] - a_vec[7]);
  dst_vec[4] = Saturate_To_Int16(b_vec[0] - b_vec[1]);
  dst_vec[5] = Saturate_To_Int16(b_vec[2] - b_vec[3]);
  dst_vec[6] = Saturate_To_Int16(b_vec[4] - b_vec[5]);
  dst_vec[7] = Saturate_To_Int16(b_vec[6] - b_vec[7]);
  dst_vec[8] = Saturate_To_Int16(a_vec[8] - a_vec[9]);
  dst_vec[9] = Saturate_To_Int16(a_vec[10] - a_vec[11]);
  dst_vec[10] = Saturate_To_Int16(a_vec[12] - a_vec[13]);
  dst_vec[11] = Saturate_To_Int16(a_vec[14] - a_vec[15]);
  dst_vec[12] = Saturate_To_Int16(b_vec[8] - b_vec[9]);
  dst_vec[13] = Saturate_To_Int16(b_vec[10] - b_vec[11]);
  dst_vec[14] = Saturate_To_Int16(b_vec[12] - b_vec[13]);
  dst_vec[15] = Saturate_To_Int16(b_vec[14] - b_vec[15]);
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_hsubs_epi16
#define _mm256_hsubs_epi16 _mm256_hsubs_epi16_dbg


/*
 Copy "a" to "dst", then insert 128 bits (composed of integer data) from "b" into "dst" at the location specified by "imm8".
*/
static inline __m256i _mm256_inserti128_si256_dbg(__m256i a, __m128i b, const int imm8)
{
  __m256i a_vec[1];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  __m128i b_vec[1];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __m128i dst_vec[2];
  _mm256_storeu_si256((__m256i*)&dst_vec[0], a_vec[0]);
  switch (imm8 & 0x3) {
    case 0:
    dst_vec[0] = b_vec[0];
break;
    case 1:
    dst_vec[1] = b_vec[0];
break;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_inserti128_si256
#define _mm256_inserti128_si256 _mm256_inserti128_si256_dbg

/*
 Copy "a" to "dst", then insert 256 bits (composed of 8 packed 32-bit integers) from "b" into "dst" at the location specified by "imm8".
*/
static inline __m512i _mm512_inserti32x8_dbg(__m512i a, __m256i b, const int imm8)
{
  __m512i a_vec[1];
  _mm512_storeu_si512((void*)a_vec, a);
  __m256i b_vec[1];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __m256i dst_vec[2];
  _mm512_storeu_si512((void*)&dst_vec[0], a_vec[0]);
  switch (imm8 & 0x3) {
    case 0:
    dst_vec[0] = b_vec[0];
break;
    case 1:
    dst_vec[1] = b_vec[0];
break;
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_inserti32x8
#define _mm512_inserti32x8 _mm512_inserti32x8_dbg

/*
 Multiply packed signed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Horizontally add adjacent pairs of intermediate 32-bit integers, and pack the results in "dst".
	
*/
static inline __m256i _mm256_madd_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_madd_epi16
#define _mm256_madd_epi16 _mm256_madd_epi16_dbg


/*
 Vertically multiply each unsigned 8-bit integer from "a" with the corresponding signed 8-bit integer from "b", producing intermediate signed 16-bit integers. Horizontally add adjacent pairs of intermediate signed 16-bit integers, and pack the saturated results in "dst".
	
*/
static inline __m256i _mm256_maddubs_epi16_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_To_Int16( a_vec[j*2]*b_vec[j*2] + a_vec[j*2+1]*b_vec[j*2+1] );
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maddubs_epi16
#define _mm256_maddubs_epi16 _mm256_maddubs_epi16_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst". 
*/
static inline __m256i _mm256_max_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epi8
#define _mm256_max_epi8 _mm256_max_epi8_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed maximum values in "dst".
	
*/
static inline __m256i _mm256_max_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epi16
#define _mm256_max_epi16 _mm256_max_epi16_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m256i _mm256_max_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epi32
#define _mm256_max_epi32 _mm256_max_epi32_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed maximum values in "dst".
	
*/
static inline __m256i _mm256_max_epu8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epu8
#define _mm256_max_epu8 _mm256_max_epu8_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst".
	
*/
static inline __m256i _mm256_max_epu16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epu16
#define _mm256_max_epu16 _mm256_max_epu16_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m256i _mm256_max_epu32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_max_epu32
#define _mm256_max_epu32 _mm256_max_epu32_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256i _mm256_min_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epi8
#define _mm256_min_epi8 _mm256_min_epi8_dbg


/*
 Compare packed 16-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256i _mm256_min_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epi16
#define _mm256_min_epi16 _mm256_min_epi16_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256i _mm256_min_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epi32
#define _mm256_min_epi32 _mm256_min_epi32_dbg


/*
 Compare packed unsigned 8-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256i _mm256_min_epu8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epu8
#define _mm256_min_epu8 _mm256_min_epu8_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256i _mm256_min_epu16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epu16
#define _mm256_min_epu16 _mm256_min_epu16_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256i _mm256_min_epu32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_min_epu32
#define _mm256_min_epu32 _mm256_min_epu32_dbg

/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst". 
*/
static inline __m256i _mm256_mul_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j*2] * b_vec[j*2];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mul_epi32
#define _mm256_mul_epi32 _mm256_mul_epi32_dbg

/*
 Multiply the low unsigned 32-bit integers from each packed 64-bit element in "a" and "b", and store the unsigned 64-bit results in "dst". 
*/
static inline __m256i _mm256_mul_epu32_dbg(__m256i a, __m256i b)
{
  uint32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  uint32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j*2] * b_vec[j*2];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mul_epu32
#define _mm256_mul_epu32 _mm256_mul_epu32_dbg

/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst". 
*/
static inline __m256i _mm256_mulhi_epi16_dbg(__m256i a, __m256i b)
{
  int32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = (tmp & 0xffff0000) >> 16;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mulhi_epi16
#define _mm256_mulhi_epi16 _mm256_mulhi_epi16_dbg

/*
 Multiply the packed unsigned 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the high 16 bits of the intermediate integers in "dst". 
*/
static inline __m256i _mm256_mulhi_epu16_dbg(__m256i a, __m256i b)
{
  uint32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = (tmp & 0xffff0000) >> 16;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mulhi_epu16
#define _mm256_mulhi_epu16 _mm256_mulhi_epu16_dbg

/*
 Multiply packed 16-bit integers in "a" and "b", producing intermediate signed 32-bit integers. Truncate each intermediate integer to the 18 most significant bits, round by adding 1, and store bits [16:1] to "dst". 
*/
static inline __m256i _mm256_mulhrs_epi16_dbg(__m256i a, __m256i b)
{
  int32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    tmp = ((a_vec[j] * b_vec[j]) >> 14) + 1;
    dst_vec[j] = (tmp & 0x1fffe) >> 1;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mulhrs_epi16
#define _mm256_mulhrs_epi16 _mm256_mulhrs_epi16_dbg

/*
 Multiply the packed 16-bit integers in "a" and "b", producing intermediate 32-bit integers, and store the low 16 bits of the intermediate integers in "dst". 
*/
static inline __m256i _mm256_mullo_epi16_dbg(__m256i a, __m256i b)
{
  int32_t tmp;
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = (tmp & 0xffff) >> 0;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mullo_epi16
#define _mm256_mullo_epi16 _mm256_mullo_epi16_dbg

/*
 Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in "dst". 
*/
static inline __m256i _mm256_mullo_epi32_dbg(__m256i a, __m256i b)
{
  int64_t tmp;
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = tmp & 0xffffffff;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mullo_epi32
#define _mm256_mullo_epi32 _mm256_mullo_epi32_dbg

/*
 Compute the bitwise OR of 256 bits (representing integer data) in "a" and "b", and store the result in "dst".
*/
static inline __m256i _mm256_or_si256_dbg(__m256i a, __m256i b)
{
  __m256i a_vec[1];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  __m256i b_vec[1];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __m256i dst_vec[1];
  dst_vec[0] = (a_vec[0] | b_vec[0]);
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_or_si256
#define _mm256_or_si256 _mm256_or_si256_dbg


/*
 Negate packed 8-bit integers in "a" when the corresponding signed 8-bit integer in "b" is negative, and store the results in "dst". Element in "dst" are zeroed out when the corresponding element in "b" is zero.
*/
static inline __m256i _mm256_sign_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (b_vec[j] < 0) {
      dst_vec[j] = NEG(a_vec[j]);
    } else {
      if (b_vec[j] == 0) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = a_vec[j];
      }
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sign_epi8
#define _mm256_sign_epi8 _mm256_sign_epi8_dbg


/*
 Negate packed 16-bit integers in "a" when the corresponding signed 16-bit integer in "b" is negative, and store the results in "dst". Element in "dst" are zeroed out when the corresponding element in "b" is zero.
*/
static inline __m256i _mm256_sign_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (b_vec[j] < 0) {
      dst_vec[j] = NEG(a_vec[j]);
    } else {
      if (b_vec[j] == 0) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = a_vec[j];
      }
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sign_epi16
#define _mm256_sign_epi16 _mm256_sign_epi16_dbg


/*
 Negate packed 32-bit integers in "a" when the corresponding signed 32-bit integer in "b" is negative, and store the results in "dst". Element in "dst" are zeroed out when the corresponding element in "b" is zero.
*/
static inline __m256i _mm256_sign_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (b_vec[j] < 0) {
      dst_vec[j] = NEG(a_vec[j]);
    } else {
      if (b_vec[j] == 0) {
        dst_vec[j] = 0;
      } else {
        dst_vec[j] = a_vec[j];
      }
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sign_epi32
#define _mm256_sign_epi32 _mm256_sign_epi32_dbg


/*
 Shift packed 16-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_sll_epi16_dbg(__m256i a, __m128i count)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (count_vec[0] > 15) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (count_vec[0] > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << count_vec[0]);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sll_epi16
#define _mm256_sll_epi16 _mm256_sll_epi16_dbg


/*
 Shift packed 16-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_slli_epi16_dbg(__m256i a, int imm8)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if ((imm8 & 0xff) >> 0 > 15) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (imm8 > 15) ? 0 : ZeroExtend((uint16_t)a_vec[j] << imm8);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_slli_epi16
#define _mm256_slli_epi16 _mm256_slli_epi16_dbg


/*
 Shift packed 32-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_sll_epi32_dbg(__m256i a, __m128i count)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (count_vec[0] > 31) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (count_vec[0] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[0]);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sll_epi32
#define _mm256_sll_epi32 _mm256_sll_epi32_dbg


/*
 Shift packed 32-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst".
*/
static inline __m256i _mm256_slli_epi32_dbg(__m256i a, int imm8)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if ((imm8 & 0xff) >> 0 > 31) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (imm8 > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << imm8);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_slli_epi32
#define _mm256_slli_epi32 _mm256_slli_epi32_dbg


/*
 Shift packed 64-bit integers in "a" left by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_sll_epi64_dbg(__m256i a, __m128i count)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (count_vec[0] > 63) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (count_vec[0] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[0]);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sll_epi64
#define _mm256_sll_epi64 _mm256_sll_epi64_dbg


/*
 Shift packed 64-bit integers in "a" left by "imm8" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_slli_epi64_dbg(__m256i a, int imm8)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if ((imm8 & 0xff) >> 0 > 63) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = (imm8 > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << imm8);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_slli_epi64
#define _mm256_slli_epi64 _mm256_slli_epi64_dbg


/*
 Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m128i _mm_sllv_epi32_dbg(__m128i a, __m128i count)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t count_vec[4];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = (count_vec[j] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_sllv_epi32
#define _mm_sllv_epi32 _mm_sllv_epi32_dbg


/*
 Shift packed 32-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_sllv_epi32_dbg(__m256i a, __m256i count)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t count_vec[8];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = (count_vec[j] > 31) ? 0 : ZeroExtend((uint32_t)a_vec[j] << count_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sllv_epi32
#define _mm256_sllv_epi32 _mm256_sllv_epi32_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m128i _mm_sllv_epi64_dbg(__m128i a, __m128i count)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_sllv_epi64
#define _mm_sllv_epi64 _mm_sllv_epi64_dbg


/*
 Shift packed 64-bit integers in "a" left by the amount specified by the corresponding element in "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_sllv_epi64_dbg(__m256i a, __m256i count)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[4];
  _mm256_storeu_si256((__m256i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = (count_vec[j] > 63) ? 0 : ZeroExtend((uint64_t)a_vec[j] << count_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sllv_epi64
#define _mm256_sllv_epi64 _mm256_sllv_epi64_dbg


/*
 Shift packed 16-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_srl_epi16_dbg(__m256i a, __m128i count)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (count_vec[0] > 15) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend((uint16_t)a_vec[j] >> count_vec[0]);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_srl_epi16
#define _mm256_srl_epi16 _mm256_srl_epi16_dbg


/*
 Shift packed 32-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_srl_epi32_dbg(__m256i a, __m128i count)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (count_vec[0] > 31) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend((uint32_t)a_vec[j] >> count_vec[0]);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_srl_epi32
#define _mm256_srl_epi32 _mm256_srl_epi32_dbg


/*
 Shift packed 64-bit integers in "a" right by "count" while shifting in zeros, and store the results in "dst". 
*/
static inline __m256i _mm256_srl_epi64_dbg(__m256i a, __m128i count)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (count_vec[0] > 63) {
      dst_vec[j] = 0;
    } else {
      dst_vec[j] = ZeroExtend((uint64_t)a_vec[j] >> count_vec[0]);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_srl_epi64
#define _mm256_srl_epi64 _mm256_srl_epi64_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_sub_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sub_epi8
#define _mm256_sub_epi8 _mm256_sub_epi8_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_sub_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sub_epi16
#define _mm256_sub_epi16 _mm256_sub_epi16_dbg


/*
 Subtract packed 32-bit integers in "b" from packed 32-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_sub_epi32_dbg(__m256i a, __m256i b)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sub_epi32
#define _mm256_sub_epi32 _mm256_sub_epi32_dbg


/*
 Subtract packed 64-bit integers in "b" from packed 64-bit integers in "a", and store the results in "dst".
*/
static inline __m256i _mm256_sub_epi64_dbg(__m256i a, __m256i b)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t b_vec[4];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_sub_epi64
#define _mm256_sub_epi64 _mm256_sub_epi64_dbg


/*
 Subtract packed 8-bit integers in "b" from packed 8-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_subs_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_Int8((int16_t)a_vec[j] - b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_subs_epi8
#define _mm256_subs_epi8 _mm256_subs_epi8_dbg


/*
 Subtract packed 16-bit integers in "b" from packed 16-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_subs_epi16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_To_Int16(a_vec[j] - b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_subs_epi16
#define _mm256_subs_epi16 _mm256_subs_epi16_dbg


/*
 Subtract packed unsigned 8-bit integers in "b" from packed unsigned 8-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_subs_epu8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt8((uint16_t)a_vec[j] - b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_subs_epu8
#define _mm256_subs_epu8 _mm256_subs_epu8_dbg


/*
 Subtract packed unsigned 16-bit integers in "b" from packed unsigned 16-bit integers in "a" using saturation, and store the results in "dst".
*/
static inline __m256i _mm256_subs_epu16_dbg(__m256i a, __m256i b)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t b_vec[16];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = Saturate_To_UnsignedInt16((uint32_t)a_vec[j] - b_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_subs_epu16
#define _mm256_subs_epu16 _mm256_subs_epu16_dbg


/*
 Compute the bitwise XOR of 256 bits (representing integer data) in "a" and "b", and store the result in "dst".
*/
static inline __m256i _mm256_xor_si256_dbg(__m256i a, __m256i b)
{
  __m256i a_vec[1];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  __m256i b_vec[1];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  __m256i dst_vec[1];
  dst_vec[0] = (a_vec[0] ^ b_vec[0]);
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_xor_si256
#define _mm256_xor_si256 _mm256_xor_si256_dbg


/*
 Copy the lower double-precision (64-bit) floating-point element of "a" to "dst".
*/
static inline double _mm256_cvtsd_f64_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst;
  dst = a_vec[0];
return dst;
}

#undef _mm256_cvtsd_f64
#define _mm256_cvtsd_f64 _mm256_cvtsd_f64_dbg


/*
 Add packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
*/
static inline __m256d _mm256_add_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_add_pd
#define _mm256_add_pd _mm256_add_pd_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
*/
static inline __m256 _mm256_add_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_add_ps
#define _mm256_add_ps _mm256_add_ps_dbg


/*
 Alternatively add and subtract packed double-precision (64-bit) floating-point elements in "a" to/from packed elements in "b", and store the results in "dst".
*/
static inline __m256d _mm256_addsub_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = a_vec[j] + b_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_addsub_pd
#define _mm256_addsub_pd _mm256_addsub_pd_dbg


/*
 Alternatively add and subtract packed single-precision (32-bit) floating-point elements in "a" to/from packed elements in "b", and store the results in "dst".
*/
static inline __m256 _mm256_addsub_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (j % 2 == 0) {
      dst_vec[j] = a_vec[j] - b_vec[j];
    } else {
      dst_vec[j] = a_vec[j] + b_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_addsub_ps
#define _mm256_addsub_ps _mm256_addsub_ps_dbg


/*
 Compute the bitwise AND of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
*/
static inline __m256d _mm256_and_pd_dbg(__m256d a, __m256d b)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ((uint64_t)a_vec[j] & (uint64_t)b_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_and_pd
#define _mm256_and_pd _mm256_and_pd_dbg


/*
 Compute the bitwise AND of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
*/
static inline __m256 _mm256_and_ps_dbg(__m256 a, __m256 b)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ((uint32_t)a_vec[j] & (uint32_t)b_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_and_ps
#define _mm256_and_ps _mm256_and_ps_dbg


/*
 Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using control mask "imm8", and store the results in "dst".
*/
static inline __m256d _mm256_blend_pd_dbg(__m256d a, __m256d b, const int imm8)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (imm8 & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_blend_pd
#define _mm256_blend_pd _mm256_blend_pd_dbg


/*
 Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using control mask "imm8", and store the results in "dst".
*/
static inline __m256 _mm256_blend_ps_dbg(__m256 a, __m256 b, const int imm8)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (imm8 & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_blend_ps
#define _mm256_blend_ps _mm256_blend_ps_dbg


/*
 Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using "mask", and store the results in "dst".
*/
static inline __m256d _mm256_blendv_pd_dbg(__m256d a, __m256d b, __m256d mask)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double mask_vec[4];
  _mm256_storeu_pd((double*)mask_vec, mask);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (mask_vec[j]) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_blendv_pd
#define _mm256_blendv_pd _mm256_blendv_pd_dbg


/*
 Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using "mask", and store the results in "dst".
*/
static inline __m256 _mm256_blendv_ps_dbg(__m256 a, __m256 b, __m256 mask)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t mask_vec[8];
  _mm256_storeu_ps((float*)mask_vec, mask);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (mask_vec[j]) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_blendv_ps
#define _mm256_blendv_ps _mm256_blendv_ps_dbg


/*
 Divide packed double-precision (64-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst".
*/
static inline __m256d _mm256_div_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] / b_vec[j];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_div_pd
#define _mm256_div_pd _mm256_div_pd_dbg


/*
 Divide packed single-precision (32-bit) floating-point elements in "a" by packed elements in "b", and store the results in "dst".
*/
static inline __m256 _mm256_div_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] / b_vec[j];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_div_ps
#define _mm256_div_ps _mm256_div_ps_dbg


/*
 Conditionally multiply the packed single-precision (32-bit) floating-point elements in "a" and "b" using the high 4 bits in "imm8", sum the four products, and conditionally store the sum in "dst" using the low 4 bits of "imm8".

static inline __m256 _mm256_dp_ps_dbg(__m256 a, __m256 b, const int imm8)
{
  __m128 a_vec[2];
  _mm256_storeu_ps((float*)a_vec, a);
  __m128 b_vec[2];
  _mm256_storeu_ps((float*)b_vec, b);
  __m128 dst_vec[2];
return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_dp_ps
#define _mm256_dp_ps _mm256_dp_ps_dbg
*/

/*
 Horizontally add adjacent pairs of double-precision (64-bit) floating-point elements in "a" and "b", and pack the results in "dst".
*/
static inline __m256d _mm256_hadd_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  dst_vec[0] = a_vec[1] + a_vec[0];
  dst_vec[1] = b_vec[1] + b_vec[0];
  dst_vec[2] = a_vec[3] + a_vec[2];
  dst_vec[3] = b_vec[3] + b_vec[2];
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_hadd_pd
#define _mm256_hadd_pd _mm256_hadd_pd_dbg


/*
 Horizontally add adjacent pairs of single-precision (32-bit) floating-point elements in "a" and "b", and pack the results in "dst".
*/
static inline __m256 _mm256_hadd_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  dst_vec[0] = a_vec[1] + a_vec[0];
  dst_vec[1] = a_vec[3] + a_vec[2];
  dst_vec[2] = b_vec[1] + b_vec[0];
  dst_vec[3] = b_vec[3] + b_vec[2];
  dst_vec[4] = a_vec[5] + a_vec[4];
  dst_vec[5] = a_vec[7] + a_vec[6];
  dst_vec[6] = b_vec[5] + b_vec[4];
  dst_vec[7] = b_vec[7] + b_vec[6];
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_hadd_ps
#define _mm256_hadd_ps _mm256_hadd_ps_dbg


/*
 Horizontally subtract adjacent pairs of double-precision (64-bit) floating-point elements in "a" and "b", and pack the results in "dst".
*/
static inline __m256d _mm256_hsub_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  dst_vec[0] = a_vec[0] - a_vec[1];
  dst_vec[1] = b_vec[0] - b_vec[1];
  dst_vec[2] = a_vec[2] - a_vec[3];
  dst_vec[3] = b_vec[2] - b_vec[3];
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_hsub_pd
#define _mm256_hsub_pd _mm256_hsub_pd_dbg


/*
 Horizontally add adjacent pairs of single-precision (32-bit) floating-point elements in "a" and "b", and pack the results in "dst".
*/
static inline __m256 _mm256_hsub_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  dst_vec[0] = a_vec[0] - a_vec[1];
  dst_vec[1] = a_vec[2] - a_vec[3];
  dst_vec[2] = b_vec[0] - b_vec[1];
  dst_vec[3] = b_vec[2] - b_vec[3];
  dst_vec[4] = a_vec[4] - a_vec[5];
  dst_vec[5] = a_vec[6] - a_vec[7];
  dst_vec[6] = b_vec[4] - b_vec[5];
  dst_vec[7] = b_vec[6] - b_vec[7];
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_hsub_ps
#define _mm256_hsub_ps _mm256_hsub_ps_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m256d _mm256_max_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = MAX(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_max_pd
#define _mm256_max_pd _mm256_max_pd_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m256 _mm256_max_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = MAX(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_max_ps
#define _mm256_max_ps _mm256_max_ps_dbg


/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256d _mm256_min_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = MIN(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_min_pd
#define _mm256_min_pd _mm256_min_pd_dbg


/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b", and store packed minimum values in "dst".
	
*/
static inline __m256 _mm256_min_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = MIN(a_vec[j], b_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_min_ps
#define _mm256_min_ps _mm256_min_ps_dbg


/*
 Multiply packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m256d _mm256_mul_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] * b_vec[j];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mul_pd
#define _mm256_mul_pd _mm256_mul_pd_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m256 _mm256_mul_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] * b_vec[j];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mul_ps
#define _mm256_mul_ps _mm256_mul_ps_dbg


/*
 Compute the bitwise OR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m256d _mm256_or_pd_dbg(__m256d a, __m256d b)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = (uint64_t)a_vec[j] | (uint64_t)b_vec[j];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_or_pd
#define _mm256_or_pd _mm256_or_pd_dbg


/*
 Compute the bitwise OR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m256 _mm256_or_ps_dbg(__m256 a, __m256 b)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = (uint32_t)a_vec[j] | (uint32_t)b_vec[j];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_or_ps
#define _mm256_or_ps _mm256_or_ps_dbg


/*
 Subtract packed double-precision (64-bit) floating-point elements in "b" from packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst".
*/
static inline __m256d _mm256_sub_pd_dbg(__m256d a, __m256d b)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_sub_pd
#define _mm256_sub_pd _mm256_sub_pd_dbg


/*
 Subtract packed single-precision (32-bit) floating-point elements in "b" from packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst".
*/
static inline __m256 _mm256_sub_ps_dbg(__m256 a, __m256 b)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a_vec[j] - b_vec[j];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_sub_ps
#define _mm256_sub_ps _mm256_sub_ps_dbg


/*
 Compute the bitwise XOR of packed double-precision (64-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m256d _mm256_xor_pd_dbg(__m256d a, __m256d b)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  uint64_t b_vec[4];
  _mm256_storeu_pd((double*)b_vec, b);
  uint64_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = (uint64_t)a_vec[j] ^ (uint64_t)b_vec[j];
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_xor_pd
#define _mm256_xor_pd _mm256_xor_pd_dbg


/*
 Compute the bitwise XOR of packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	
*/
static inline __m256 _mm256_xor_ps_dbg(__m256 a, __m256 b)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t b_vec[8];
  _mm256_storeu_ps((float*)b_vec, b);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = (uint32_t)a_vec[j] ^ (uint32_t)b_vec[j];
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_xor_ps
#define _mm256_xor_ps _mm256_xor_ps_dbg


/*
 Convert packed 32-bit integers in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256d _mm256_cvtepi32_pd_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_Int32_To_FP64(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_cvtepi32_pd
#define _mm256_cvtepi32_pd _mm256_cvtepi32_pd_dbg


/*
 Convert packed 32-bit integers in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256 _mm256_cvtepi32_ps_dbg(__m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_Int32_To_FP32(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_cvtepi32_ps
#define _mm256_cvtepi32_ps _mm256_cvtepi32_ps_dbg

/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed single-precision (32-bit) floating-point elements, and store the results in "dst".
*/
static inline __m128 _mm256_cvtpd_ps_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_FP32(a_vec[j]);
  }
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm256_cvtpd_ps
#define _mm256_cvtpd_ps _mm256_cvtpd_ps_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m256i _mm256_cvtps_epi32_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_Int32(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvtps_epi32
#define _mm256_cvtps_epi32 _mm256_cvtps_epi32_dbg


/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed double-precision (64-bit) floating-point elements, and store the results in "dst".
*/
static inline __m256d _mm256_cvtps_pd_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP32_To_FP64(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_cvtps_pd
#define _mm256_cvtps_pd _mm256_cvtps_pd_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m128i _mm256_cvttpd_epi32_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_Int32_Truncate(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvttpd_epi32
#define _mm256_cvttpd_epi32 _mm256_cvttpd_epi32_dbg


/*
 Convert packed double-precision (64-bit) floating-point elements in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm256_cvtpd_epi32_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = Convert_FP64_To_Int32(a_vec[j]);
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm256_cvtpd_epi32
#define _mm256_cvtpd_epi32 _mm256_cvtpd_epi32_dbg

/*
 Convert packed single-precision (32-bit) floating-point elements in "a" to packed 32-bit integers with truncation, and store the results in "dst".
*/
static inline __m256i _mm256_cvttps_epi32_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = Convert_FP32_To_Int32_Truncate(a_vec[j]);
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_cvttps_epi32
#define _mm256_cvttps_epi32 _mm256_cvttps_epi32_dbg


/*
 Shuffle single-precision (32-bit) floating-point elements in "a" using the control in "imm8", and store the results in "dst".
*/
static inline __m128 _mm_permute_ps_dbg(__m128 a, int imm8)
{
  __m128 a_vec[1];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  dst_vec[0] = _mm_extract_ps(a_vec[0], (imm8 & 0x3) >> 0);
  dst_vec[1] = _mm_extract_ps(a_vec[0], (imm8 & 0xc) >> 2);
  dst_vec[2] = _mm_extract_ps(a_vec[0], (imm8 & 0x30) >> 4);
  dst_vec[3] = _mm_extract_ps(a_vec[0], (imm8 & 0xc0) >> 6);
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_permute_ps
#define _mm_permute_ps _mm_permute_ps_dbg


/*
 Shuffle double-precision (64-bit) floating-point elements in "a" using the control in "imm8", and store the results in "dst".
*/
static inline __m128d _mm_permute_pd_dbg(__m128d a, int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  if ((imm8 & ((1 << 0) & 0xffffffff)) == 0) {
    dst_vec[0] = a_vec[0];
  }
  if ((imm8 & ((1 << 0) & 0xffffffff)) == 1) {
    dst_vec[0] = a_vec[1];
  }
  if ((imm8 & ((1 << 1) & 0xffffffff)) == 0) {
    dst_vec[1] = a_vec[0];
  }
  if ((imm8 & ((1 << 1) & 0xffffffff)) == 1) {
    dst_vec[1] = a_vec[1];
  }
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_permute_pd
#define _mm_permute_pd _mm_permute_pd_dbg

/*
 Shuffle 32-bit integers in "a" across lanes using the corresponding index in "idx", and store the results in "dst". Note that this intrinsic shuffles across 128-bit lanes.
*/
static inline __m512i _mm512_permutexvar_epi32_dbg(__m512i idx, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t idx_vec[16];
  _mm512_storeu_si512((void*)idx_vec, idx);

  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    int id = idx_vec[j] & 0xf;
    dst_vec[j] = a_vec[id];
  }
  
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_permutexvar_epi32
#define _mm512_permutexvar_epi32 _mm512_permutexvar_epi32_dbg

/*
 Copy "a" to "dst", then insert 128 bits (composed of 4 packed single-precision (32-bit) floating-point elements) from "b" into "dst" at the location specified by "imm8".
*/
static inline __m256 _mm256_insertf128_ps_dbg(__m256 a, __m128 b, int imm8)
{
  __m256 a_vec[1];
  _mm256_storeu_ps((float*)a_vec, a);
  __m128 b_vec[1];
  _mm_storeu_ps((float*)b_vec, b);
  __m128 dst_vec[2];
  _mm256_storeu_ps((float*)&dst_vec[0], a_vec[0]);
  switch ((imm8 & 0x3) >> 0) {
    case 0:
    dst_vec[0] = b_vec[0];
break;
    case 1:
    dst_vec[1] = b_vec[0];
break;
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_insertf128_ps
#define _mm256_insertf128_ps _mm256_insertf128_ps_dbg


/*
 Copy "a" to "dst", then insert 128 bits (composed of 2 packed double-precision (64-bit) floating-point elements) from "b" into "dst" at the location specified by "imm8".
*/
static inline __m256d _mm256_insertf128_pd_dbg(__m256d a, __m128d b, int imm8)
{
  __m256d a_vec[1];
  _mm256_storeu_pd((double*)a_vec, a);
  __m128d b_vec[1];
  _mm_storeu_pd((double*)b_vec, b);
  __m128d dst_vec[2];
  _mm256_storeu_pd((double*)&dst_vec[0], a_vec[0]);
  switch ((imm8 & 0xff) >> 0) {
    case 0:
    dst_vec[0] = b_vec[0];
break;
    case 1:
    dst_vec[1] = b_vec[0];
break;
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_insertf128_pd
#define _mm256_insertf128_pd _mm256_insertf128_pd_dbg


/*
 Copy "a" to "dst", then insert 128 bits from "b" into "dst" at the location specified by "imm8".
*/
static inline __m256i _mm256_insertf128_si256_dbg(__m256i a, __m128i b, int imm8)
{
  __m256i a_vec[1];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  __m128i b_vec[1];
  _mm_storeu_si128((__m128i*)b_vec, b);
  __m128i dst_vec[2];
  _mm256_storeu_si256((__m256i*)&dst_vec[0], a_vec[0]);
  switch ((imm8 & 0x3) >> 0) {
    case 0:
    dst_vec[0] = b_vec[0];
break;
    case 1:
    dst_vec[1] = b_vec[0];
break;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_insertf128_si256
#define _mm256_insertf128_si256 _mm256_insertf128_si256_dbg







/*
 Compute the approximate reciprocal of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 1.5*2^-12.
*/
static inline __m256 _mm256_rcp_ps_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = APPROXIMATE(1.0/a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_rcp_ps
#define _mm256_rcp_ps _mm256_rcp_ps_dbg


/*
 Compute the approximate reciprocal square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst". The maximum relative error for this approximation is less than 1.5*2^-12.
*/
static inline __m256 _mm256_rsqrt_ps_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = APPROXIMATE(1.0 / sqrt(a_vec[j]));
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_rsqrt_ps
#define _mm256_rsqrt_ps _mm256_rsqrt_ps_dbg


/*
 Compute the square root of packed double-precision (64-bit) floating-point elements in "a", and store the results in "dst".
*/
static inline __m256d _mm256_sqrt_pd_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = sqrt(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_sqrt_pd
#define _mm256_sqrt_pd _mm256_sqrt_pd_dbg


/*
 Compute the square root of packed single-precision (32-bit) floating-point elements in "a", and store the results in "dst".
*/
static inline __m256 _mm256_sqrt_ps_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = sqrt(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_sqrt_ps
#define _mm256_sqrt_ps _mm256_sqrt_ps_dbg


/*
 Round the packed double-precision (64-bit) floating-point elements in "a" using the "rounding" parameter, and store the results as packed double-precision floating-point elements in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256d _mm256_round_pd_dbg(__m256d a, int rounding)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ROUND(a_vec[j], rounding);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_round_pd
#define _mm256_round_pd _mm256_round_pd_dbg


/*
 Round the packed single-precision (32-bit) floating-point elements in "a" using the "rounding" parameter, and store the results as packed single-precision floating-point elements in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m256 _mm256_round_ps_dbg(__m256 a, int rounding)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ROUND(a_vec[j], rounding);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_round_ps
#define _mm256_round_ps _mm256_round_ps_dbg


/*
 Set packed double-precision (64-bit) floating-point elements in "dst" with the supplied values.
*/
static inline __m256d _mm256_set_pd_dbg(double e3, double e2, double e1, double e0)
{
  double dst_vec[4];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_set_pd
#define _mm256_set_pd _mm256_set_pd_dbg


/*
 Set packed single-precision (32-bit) floating-point elements in "dst" with the supplied values.
*/
static inline __m256 _mm256_set_ps_dbg(float e7, float e6, float e5, float e4, float e3, float e2, float e1, float e0)
{
  float dst_vec[8];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_set_ps
#define _mm256_set_ps _mm256_set_ps_dbg


/*
 Set packed 8-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m256i _mm256_set_epi8_dbg(char e31, char e30, char e29, char e28, char e27, char e26, char e25, char e24, char e23, char e22, char e21, char e20, char e19, char e18, char e17, char e16, char e15, char e14, char e13, char e12, char e11, char e10, char e9, char e8, char e7, char e6, char e5, char e4, char e3, char e2, char e1, char e0)
{
  int8_t dst_vec[32];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  dst_vec[8] = e8;
  dst_vec[9] = e9;
  dst_vec[10] = e10;
  dst_vec[11] = e11;
  dst_vec[12] = e12;
  dst_vec[13] = e13;
  dst_vec[14] = e14;
  dst_vec[15] = e15;
  dst_vec[16] = e16;
  dst_vec[17] = e17;
  dst_vec[18] = e18;
  dst_vec[19] = e19;
  dst_vec[20] = e20;
  dst_vec[21] = e21;
  dst_vec[22] = e22;
  dst_vec[23] = e23;
  dst_vec[24] = e24;
  dst_vec[25] = e25;
  dst_vec[26] = e26;
  dst_vec[27] = e27;
  dst_vec[28] = e28;
  dst_vec[29] = e29;
  dst_vec[30] = e30;
  dst_vec[31] = e31;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_set_epi8
#define _mm256_set_epi8 _mm256_set_epi8_dbg


/*
 Set packed 16-bit integers in "dst" with the supplied values.
*/
static inline __m256i _mm256_set_epi16_dbg(short e15, short e14, short e13, short e12, short e11, short e10, short e9, short e8, short e7, short e6, short e5, short e4, short e3, short e2, short e1, short e0)
{
  int16_t dst_vec[16];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  dst_vec[8] = e8;
  dst_vec[9] = e9;
  dst_vec[10] = e10;
  dst_vec[11] = e11;
  dst_vec[12] = e12;
  dst_vec[13] = e13;
  dst_vec[14] = e14;
  dst_vec[15] = e15;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_set_epi16
#define _mm256_set_epi16 _mm256_set_epi16_dbg


/*
 Set packed 32-bit integers in "dst" with the supplied values.
*/
static inline __m256i _mm256_set_epi32_dbg(int e7, int e6, int e5, int e4, int e3, int e2, int e1, int e0)
{
  int32_t dst_vec[8];
  dst_vec[0] = e0;
  dst_vec[1] = e1;
  dst_vec[2] = e2;
  dst_vec[3] = e3;
  dst_vec[4] = e4;
  dst_vec[5] = e5;
  dst_vec[6] = e6;
  dst_vec[7] = e7;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_set_epi32
#define _mm256_set_epi32 _mm256_set_epi32_dbg



/*
 Set packed double-precision (64-bit) floating-point elements in "dst" with the supplied values in reverse order.
*/
static inline __m256d _mm256_setr_pd_dbg(double e3, double e2, double e1, double e0)
{
  double dst_vec[4];
  dst_vec[0] = e3;
  dst_vec[1] = e2;
  dst_vec[2] = e1;
  dst_vec[3] = e0;
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_setr_pd
#define _mm256_setr_pd _mm256_setr_pd_dbg


/*
 Set packed single-precision (32-bit) floating-point elements in "dst" with the supplied values in reverse order.
*/
static inline __m256 _mm256_setr_ps_dbg(float e7, float e6, float e5, float e4, float e3, float e2, float e1, float e0)
{
  float dst_vec[8];
  dst_vec[0] = e7;
  dst_vec[1] = e6;
  dst_vec[2] = e5;
  dst_vec[3] = e4;
  dst_vec[4] = e3;
  dst_vec[5] = e2;
  dst_vec[6] = e1;
  dst_vec[7] = e0;
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_setr_ps
#define _mm256_setr_ps _mm256_setr_ps_dbg


/*
 Set packed 8-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m256i _mm256_setr_epi8_dbg(char e31, char e30, char e29, char e28, char e27, char e26, char e25, char e24, char e23, char e22, char e21, char e20, char e19, char e18, char e17, char e16, char e15, char e14, char e13, char e12, char e11, char e10, char e9, char e8, char e7, char e6, char e5, char e4, char e3, char e2, char e1, char e0)
{
  int8_t dst_vec[32];
  dst_vec[0] = e31;
  dst_vec[1] = e30;
  dst_vec[2] = e29;
  dst_vec[3] = e28;
  dst_vec[4] = e27;
  dst_vec[5] = e26;
  dst_vec[6] = e25;
  dst_vec[7] = e24;
  dst_vec[8] = e23;
  dst_vec[9] = e22;
  dst_vec[10] = e21;
  dst_vec[11] = e20;
  dst_vec[12] = e19;
  dst_vec[13] = e18;
  dst_vec[14] = e17;
  dst_vec[15] = e16;
  dst_vec[16] = e15;
  dst_vec[17] = e14;
  dst_vec[18] = e13;
  dst_vec[19] = e12;
  dst_vec[20] = e11;
  dst_vec[21] = e10;
  dst_vec[22] = e9;
  dst_vec[23] = e8;
  dst_vec[24] = e7;
  dst_vec[25] = e6;
  dst_vec[26] = e5;
  dst_vec[27] = e4;
  dst_vec[28] = e3;
  dst_vec[29] = e2;
  dst_vec[30] = e1;
  dst_vec[31] = e0;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_setr_epi8
#define _mm256_setr_epi8 _mm256_setr_epi8_dbg


/*
 Set packed 16-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m256i _mm256_setr_epi16_dbg(short e15, short e14, short e13, short e12, short e11, short e10, short e9, short e8, short e7, short e6, short e5, short e4, short e3, short e2, short e1, short e0)
{
  int16_t dst_vec[16];
  dst_vec[0] = e15;
  dst_vec[1] = e14;
  dst_vec[2] = e13;
  dst_vec[3] = e12;
  dst_vec[4] = e11;
  dst_vec[5] = e10;
  dst_vec[6] = e9;
  dst_vec[7] = e8;
  dst_vec[8] = e7;
  dst_vec[9] = e6;
  dst_vec[10] = e5;
  dst_vec[11] = e4;
  dst_vec[12] = e3;
  dst_vec[13] = e2;
  dst_vec[14] = e1;
  dst_vec[15] = e0;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_setr_epi16
#define _mm256_setr_epi16 _mm256_setr_epi16_dbg


/*
 Set packed 32-bit integers in "dst" with the supplied values in reverse order.
*/
static inline __m256i _mm256_setr_epi32_dbg(int e7, int e6, int e5, int e4, int e3, int e2, int e1, int e0)
{
  int32_t dst_vec[8];
  dst_vec[0] = e7;
  dst_vec[1] = e6;
  dst_vec[2] = e5;
  dst_vec[3] = e4;
  dst_vec[4] = e3;
  dst_vec[5] = e2;
  dst_vec[6] = e1;
  dst_vec[7] = e0;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_setr_epi32
#define _mm256_setr_epi32 _mm256_setr_epi32_dbg



/*
 Broadcast double-precision (64-bit) floating-point value "a" to all elements of "dst".
*/
static inline __m256d _mm256_set1_pd_dbg(double a)
{
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = a;
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_set1_pd
#define _mm256_set1_pd _mm256_set1_pd_dbg


/*
 Broadcast single-precision (32-bit) floating-point value "a" to all elements of "dst".
*/
static inline __m256 _mm256_set1_ps_dbg(float a)
{
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a;
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_set1_ps
#define _mm256_set1_ps _mm256_set1_ps_dbg


/*
 Broadcast 8-bit integer "a" to all elements of "dst". This intrinsic may generate the "vpbroadcastb".
*/
static inline __m256i _mm256_set1_epi8_dbg(char a)
{
  int8_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    dst_vec[j] = a;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_set1_epi8
#define _mm256_set1_epi8 _mm256_set1_epi8_dbg


/*
 Broadcast 16-bit integer "a" to all all elements of "dst". This intrinsic may generate the "vpbroadcastw".
*/
static inline __m256i _mm256_set1_epi16_dbg(short a)
{
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_set1_epi16
#define _mm256_set1_epi16 _mm256_set1_epi16_dbg


/*
 Broadcast 32-bit integer "a" to all elements of "dst". This intrinsic may generate the "vpbroadcastd".
*/
static inline __m256i _mm256_set1_epi32_dbg(int a)
{
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = a;
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_set1_epi32
#define _mm256_set1_epi32 _mm256_set1_epi32_dbg



/*
 Round the packed single-precision (32-bit) floating-point elements in "a" down to an integer value, and store the results as packed single-precision floating-point elements in "dst".
*/
static inline __m256 _mm256_floor_ps_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = FLOOR(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_floor_ps
#define _mm256_floor_ps _mm256_floor_ps_dbg


/*
 Round the packed single-precision (32-bit) floating-point elements in "a" up to an integer value, and store the results as packed single-precision floating-point elements in "dst".
*/
static inline __m256 _mm256_ceil_ps_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = CEIL(a_vec[j]);
  }
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_ceil_ps
#define _mm256_ceil_ps _mm256_ceil_ps_dbg


/*
 Round the packed double-precision (64-bit) floating-point elements in "a" down to an integer value, and store the results as packed double-precision floating-point elements in "dst".
*/
static inline __m256d _mm256_floor_pd_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = FLOOR(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_floor_pd
#define _mm256_floor_pd _mm256_floor_pd_dbg


/*
 Round the packed double-precision (64-bit) floating-point elements in "a" up to an integer value, and store the results as packed double-precision floating-point elements in "dst".
*/
static inline __m256d _mm256_ceil_pd_dbg(__m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = CEIL(a_vec[j]);
  }
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_ceil_pd
#define _mm256_ceil_pd _mm256_ceil_pd_dbg


/*
 Set packed __m256 vector "dst" with the supplied values.
*/
static inline __m256 _mm256_set_m128_dbg(__m128 hi, __m128 lo)
{
  __m128 hi_vec[1];
  _mm_storeu_ps((float*)hi_vec, hi);
  __m128 lo_vec[1];
  _mm_storeu_ps((float*)lo_vec, lo);
  __m128 dst_vec[2];
  dst_vec[0] = lo_vec[0];
  dst_vec[1] = hi_vec[0];
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_set_m128
#define _mm256_set_m128 _mm256_set_m128_dbg


/*
 Set packed __m256d vector "dst" with the supplied values.
*/
static inline __m256d _mm256_set_m128d_dbg(__m128d hi, __m128d lo)
{
  __m128d hi_vec[1];
  _mm_storeu_pd((double*)hi_vec, hi);
  __m128d lo_vec[1];
  _mm_storeu_pd((double*)lo_vec, lo);
  __m128d dst_vec[2];
  dst_vec[0] = lo_vec[0];
  dst_vec[1] = hi_vec[0];
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_set_m128d
#define _mm256_set_m128d _mm256_set_m128d_dbg


/*
 Set packed __m256i vector "dst" with the supplied values.
*/
static inline __m256i _mm256_set_m128i_dbg(__m128i hi, __m128i lo)
{
  __m128i hi_vec[1];
  _mm_storeu_si128((__m128i*)hi_vec, hi);
  __m128i lo_vec[1];
  _mm_storeu_si128((__m128i*)lo_vec, lo);
  __m128i dst_vec[2];
  dst_vec[0] = lo_vec[0];
  dst_vec[1] = hi_vec[0];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_set_m128i
#define _mm256_set_m128i _mm256_set_m128i_dbg


/*
 Set packed __m256 vector "dst" with the supplied values.
*/
static inline __m256 _mm256_setr_m128_dbg(__m128 lo, __m128 hi)
{
  __m128 lo_vec[1];
  _mm_storeu_ps((float*)lo_vec, lo);
  __m128 hi_vec[1];
  _mm_storeu_ps((float*)hi_vec, hi);
  __m128 dst_vec[2];
  dst_vec[0] = lo_vec[0];
  dst_vec[1] = hi_vec[0];
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_setr_m128
#define _mm256_setr_m128 _mm256_setr_m128_dbg


/*
 Set packed __m256d vector "dst" with the supplied values.
*/
static inline __m256d _mm256_setr_m128d_dbg(__m128d lo, __m128d hi)
{
  __m128d lo_vec[1];
  _mm_storeu_pd((double*)lo_vec, lo);
  __m128d hi_vec[1];
  _mm_storeu_pd((double*)hi_vec, hi);
  __m128d dst_vec[2];
  dst_vec[0] = lo_vec[0];
  dst_vec[1] = hi_vec[0];
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_setr_m128d
#define _mm256_setr_m128d _mm256_setr_m128d_dbg


/*
 Set packed __m256i vector "dst" with the supplied values.
*/
static inline __m256i _mm256_setr_m128i_dbg(__m128i lo, __m128i hi)
{
  __m128i lo_vec[1];
  _mm_storeu_si128((__m128i*)lo_vec, lo);
  __m128i hi_vec[1];
  _mm_storeu_si128((__m128i*)hi_vec, hi);
  __m128i dst_vec[2];
  dst_vec[0] = lo_vec[0];
  dst_vec[1] = hi_vec[0];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_setr_m128i
#define _mm256_setr_m128i _mm256_setr_m128i_dbg


/*
 Copy the lower single-precision (32-bit) floating-point element of "a" to "dst".
*/
static inline float _mm256_cvtss_f32_dbg(__m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst;
  dst = a_vec[0];
return dst;
}

#undef _mm256_cvtss_f32
#define _mm256_cvtss_f32 _mm256_cvtss_f32_dbg

/*
 Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using control mask "imm8", and store the results in "dst".
*/
static inline __m128d _mm_blend_pd_dbg(__m128d a, __m128d b, const int imm8)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (imm8 & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_blend_pd
#define _mm_blend_pd _mm_blend_pd_dbg


/*
 Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using control mask "imm8", and store the results in "dst".
*/
static inline __m128 _mm_blend_ps_dbg(__m128 a, __m128 b, const int imm8)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (imm8 & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_blend_ps
#define _mm_blend_ps _mm_blend_ps_dbg


/*
 Blend packed double-precision (64-bit) floating-point elements from "a" and "b" using "mask", and store the results in "dst".
*/
static inline __m128d _mm_blendv_pd_dbg(__m128d a, __m128d b, __m128d mask)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double mask_vec[2];
  _mm_storeu_pd((double*)mask_vec, mask);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    if (mask_vec[j]) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_blendv_pd
#define _mm_blendv_pd _mm_blendv_pd_dbg


/*
 Blend packed single-precision (32-bit) floating-point elements from "a" and "b" using "mask", and store the results in "dst".
*/
static inline __m128 _mm_blendv_ps_dbg(__m128 a, __m128 b, __m128 mask)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  int32_t mask_vec[4];
  _mm_storeu_ps((float*)mask_vec, mask);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (mask_vec[j]) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_blendv_ps
#define _mm_blendv_ps _mm_blendv_ps_dbg


/*
 Blend packed 8-bit integers from "a" and "b" using "mask", and store the results in "dst".
*/
static inline __m128i _mm_blendv_epi8_dbg(__m128i a, __m128i b, __m128i mask)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t mask_vec[16];
  _mm_storeu_si128((__m128i*)mask_vec, mask);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (mask_vec[j]) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_blendv_epi8
#define _mm_blendv_epi8 _mm_blendv_epi8_dbg


/*
 Blend packed 16-bit integers from "a" and "b" using control mask "imm8", and store the results in "dst".
*/
static inline __m128i _mm_blend_epi16_dbg(__m128i a, __m128i b, const int imm8)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (imm8 & ((1 << j) & 0xffffffff)) {
      dst_vec[j] = b_vec[j];
    } else {
      dst_vec[j] = a_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_blend_epi16
#define _mm_blend_epi16 _mm_blend_epi16_dbg


/*
 Conditionally multiply the packed double-precision (64-bit) floating-point elements in "a" and "b" using the high 4 bits in "imm8", sum the four products, and conditionally store the sum in "dst" using the low 4 bits of "imm8".
*/
/*
static inline __m128d _mm_dp_pd_dbg(__m128d a, __m128d b, const int imm8)
{
  __m128d a_vec[1];
  _mm_storeu_pd((double*)a_vec, a);
  __m128d b_vec[1];
  _mm_storeu_pd((double*)b_vec, b);
  __m128d dst_vec[1];
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_dp_pd
#define _mm_dp_pd _mm_dp_pd_dbg
*/

/*
 Conditionally multiply the packed single-precision (32-bit) floating-point elements in "a" and "b" using the high 4 bits in "imm8", sum the four products, and conditionally store the sum in "dst" using the low 4 bits of "imm8".

static inline __m128 _mm_dp_ps_dbg(__m128 a, __m128 b, const int imm8)
{
  __m128 a_vec[1];
  _mm_storeu_ps((float*)a_vec, a);
  __m128 b_vec[1];
  _mm_storeu_ps((float*)b_vec, b);
  __m128 dst_vec[1];
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_dp_ps
#define _mm_dp_ps _mm_dp_ps_dbg
*/

/*
 Copy "a" to "dst", and insert the lower 8-bit integer from "i" into "dst" at the location specified by "imm8". 
*/
static inline __m128i _mm_insert_epi8_dbg(__m128i a, int i, const int imm8)
{
  __m128 a_vec[1];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  _mm_storeu_ps((float*)&dst_vec[0], a_vec[0]);
  int sel = imm8 & 0xf;
  dst_vec[sel] = i;
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_insert_epi8
#define _mm_insert_epi8 _mm_insert_epi8_dbg


/*
 Copy "a" to "dst", and insert the 32-bit integer "i" into "dst" at the location specified by "imm8". 
*/
static inline __m128i _mm_insert_epi32_dbg(__m128i a, int i, const int imm8)
{
  __m128 a_vec[1];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  _mm_storeu_ps((float*)&dst_vec[0], a_vec[0]);
  int sel = (imm8 & 0x3);
  dst_vec[sel] = i;
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_insert_epi32
#define _mm_insert_epi32 _mm_insert_epi32_dbg



/*
 Compare packed 8-bit integers in "a" and "b", and store packed maximum values in "dst". 
*/
static inline __m128i _mm_max_epi8_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_max_epi8
#define _mm_max_epi8 _mm_max_epi8_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m128i _mm_max_epi32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_max_epi32
#define _mm_max_epi32 _mm_max_epi32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m128i _mm_max_epu32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_max_epu32
#define _mm_max_epu32 _mm_max_epu32_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed maximum values in "dst".
*/
static inline __m128i _mm_max_epu16_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] > b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_max_epu16
#define _mm_max_epu16 _mm_max_epu16_dbg


/*
 Compare packed 8-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m128i _mm_min_epi8_dbg(__m128i a, __m128i b)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t b_vec[16];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int8_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_min_epi8
#define _mm_min_epi8 _mm_min_epi8_dbg


/*
 Compare packed 32-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m128i _mm_min_epi32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_min_epi32
#define _mm_min_epi32 _mm_min_epi32_dbg


/*
 Compare packed unsigned 32-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m128i _mm_min_epu32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_min_epu32
#define _mm_min_epu32 _mm_min_epu32_dbg


/*
 Compare packed unsigned 16-bit integers in "a" and "b", and store packed minimum values in "dst".
*/
static inline __m128i _mm_min_epu16_dbg(__m128i a, __m128i b)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t b_vec[8];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (a_vec[j] < b_vec[j]) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = b_vec[j];
    }
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_min_epu16
#define _mm_min_epu16 _mm_min_epu16_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for equality, and store the results in "dst".
*/
static inline __m128i _mm_cmpeq_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ( a_vec[j] == b_vec[j] ) ? 0xFFFFFFFFFFFFFFFFULL : 0;
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cmpeq_epi64
#define _mm_cmpeq_epi64 _mm_cmpeq_epi64_dbg

/*
 Sign extend packed 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi8_epi16_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi8_epi16
#define _mm_cvtepi8_epi16 _mm_cvtepi8_epi16_dbg


/*
 Sign extend packed 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi8_epi32_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi8_epi32
#define _mm_cvtepi8_epi32 _mm_cvtepi8_epi32_dbg

/*
 Sign extend packed 8-bit integers in the low 8 bytes of "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi8_epi64_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi8_epi64
#define _mm_cvtepi8_epi64 _mm_cvtepi8_epi64_dbg

/*
 Sign extend packed 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi16_epi32_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi16_epi32
#define _mm_cvtepi16_epi32 _mm_cvtepi16_epi32_dbg

/*
 Sign extend packed 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi16_epi64_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi16_epi64
#define _mm_cvtepi16_epi64 _mm_cvtepi16_epi64_dbg


/*
 Sign extend packed 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepi32_epi64_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = SignExtend(a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepi32_epi64
#define _mm_cvtepi32_epi64 _mm_cvtepi32_epi64_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 16-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepu8_epi16_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepu8_epi16
#define _mm_cvtepu8_epi16 _mm_cvtepu8_epi16_dbg


/*
 Zero extend packed unsigned 8-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepu8_epi32_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepu8_epi32
#define _mm_cvtepu8_epi32 _mm_cvtepu8_epi32_dbg

/*
 Zero extend packed unsigned 8-bit integers in the low 8 byte sof "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepu8_epi64_dbg(__m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ZeroExtend((uint8_t)a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepu8_epi64
#define _mm_cvtepu8_epi64 _mm_cvtepu8_epi64_dbg

/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 32-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepu16_epi32_dbg(__m128i a)
{
  uint16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepu16_epi32
#define _mm_cvtepu16_epi32 _mm_cvtepu16_epi32_dbg


/*
 Zero extend packed unsigned 16-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepu16_epi64_dbg(__m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ZeroExtend((uint16_t)a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepu16_epi64
#define _mm_cvtepu16_epi64 _mm_cvtepu16_epi64_dbg

/*
 Zero extend packed unsigned 32-bit integers in "a" to packed 64-bit integers, and store the results in "dst".
*/
static inline __m128i _mm_cvtepu32_epi64_dbg(__m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ZeroExtend((uint32_t)a_vec[j]);
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cvtepu32_epi64
#define _mm_cvtepu32_epi64 _mm_cvtepu32_epi64_dbg

/*
 Multiply the low 32-bit integers from each packed 64-bit element in "a" and "b", and store the signed 64-bit results in "dst". 
*/
static inline __m128i _mm_mul_epi32_dbg(__m128i a, __m128i b)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = a_vec[j*2] * b_vec[j*2];
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mul_epi32
#define _mm_mul_epi32 _mm_mul_epi32_dbg

/*
 Multiply the packed 32-bit integers in "a" and "b", producing intermediate 64-bit integers, and store the low 32 bits of the intermediate integers in "dst". 
*/
static inline __m128i _mm_mullo_epi32_dbg(__m128i a, __m128i b)
{
  int64_t tmp;
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t b_vec[4];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int32_t dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    tmp = a_vec[j] * b_vec[j];
    dst_vec[j] = (tmp & 0xffffffff) >> 0;
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mullo_epi32
#define _mm_mullo_epi32 _mm_mullo_epi32_dbg

/*
 Round the packed double-precision (64-bit) floating-point elements in "a" using the "rounding" parameter, and store the results as packed double-precision floating-point elements in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_round_pd_dbg(__m128d a, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ROUND(a_vec[j], rounding);
  }
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_round_pd
#define _mm_round_pd _mm_round_pd_dbg


/*
 Round the packed double-precision (64-bit) floating-point elements in "a" down to an integer value, and store the results as packed double-precision floating-point elements in "dst".
*/
static inline __m128d _mm_floor_pd_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = FLOOR(a_vec[j]);
  }
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_floor_pd
#define _mm_floor_pd _mm_floor_pd_dbg


/*
 Round the packed double-precision (64-bit) floating-point elements in "a" up to an integer value, and store the results as packed double-precision floating-point elements in "dst".
*/
static inline __m128d _mm_ceil_pd_dbg(__m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = CEIL(a_vec[j]);
  }
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_ceil_pd
#define _mm_ceil_pd _mm_ceil_pd_dbg


/*
 Round the packed single-precision (32-bit) floating-point elements in "a" using the "rounding" parameter, and store the results as packed single-precision floating-point elements in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_round_ps_dbg(__m128 a, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = ROUND(a_vec[j], rounding);
  }
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_round_ps
#define _mm_round_ps _mm_round_ps_dbg


/*
 Round the packed single-precision (32-bit) floating-point elements in "a" down to an integer value, and store the results as packed single-precision floating-point elements in "dst".
*/
static inline __m128 _mm_floor_ps_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = FLOOR(a_vec[j]);
  }
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_floor_ps
#define _mm_floor_ps _mm_floor_ps_dbg


/*
 Round the packed single-precision (32-bit) floating-point elements in "a" up to an integer value, and store the results as packed single-precision floating-point elements in "dst".
*/
static inline __m128 _mm_ceil_ps_dbg(__m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  for (int j = 0; j <= 3; j++) {
    dst_vec[j] = CEIL(a_vec[j]);
  }
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_ceil_ps
#define _mm_ceil_ps _mm_ceil_ps_dbg


/*
 Round the lower double-precision (64-bit) floating-point element in "b" using the "rounding" parameter, store the result as a double-precision floating-point element in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128d _mm_round_sd_dbg(__m128d a, __m128d b, int rounding)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = ROUND(b_vec[0], rounding);
  dst_vec[1] = a_vec[1];
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_round_sd
#define _mm_round_sd _mm_round_sd_dbg


/*
 Round the lower double-precision (64-bit) floating-point element in "b" down to an integer value, store the result as a double-precision floating-point element in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_floor_sd_dbg(__m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = FLOOR(b_vec[0]);
  dst_vec[1] = a_vec[1];
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_floor_sd
#define _mm_floor_sd _mm_floor_sd_dbg


/*
 Round the lower double-precision (64-bit) floating-point element in "b" up to an integer value, store the result as a double-precision floating-point element in the lower element of "dst", and copy the upper element from "a" to the upper element of "dst".
*/
static inline __m128d _mm_ceil_sd_dbg(__m128d a, __m128d b)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double b_vec[2];
  _mm_storeu_pd((double*)b_vec, b);
  double dst_vec[2];
  dst_vec[0] = CEIL(b_vec[0]);
  dst_vec[1] = a_vec[1];
return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_ceil_sd
#define _mm_ceil_sd _mm_ceil_sd_dbg


/*
 Round the lower single-precision (32-bit) floating-point element in "b" using the "rounding" parameter, store the result as a single-precision floating-point element in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
	
*/
static inline __m128 _mm_round_ss_dbg(__m128 a, __m128 b, int rounding)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = ROUND(b_vec[0], rounding);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_round_ss
#define _mm_round_ss _mm_round_ss_dbg


/*
 Round the lower single-precision (32-bit) floating-point element in "b" down to an integer value, store the result as a single-precision floating-point element in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_floor_ss_dbg(__m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = FLOOR(b_vec[0]);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_floor_ss
#define _mm_floor_ss _mm_floor_ss_dbg


/*
 Round the lower single-precision (32-bit) floating-point element in "b" up to an integer value, store the result as a single-precision floating-point element in the lower element of "dst", and copy the upper 3 packed elements from "a" to the upper elements of "dst".
*/
static inline __m128 _mm_ceil_ss_dbg(__m128 a, __m128 b)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float b_vec[4];
  _mm_storeu_ps((float*)b_vec, b);
  float dst_vec[4];
  dst_vec[0] = CEIL(b_vec[0]);
  dst_vec[1] = a_vec[1];
  dst_vec[2] = a_vec[2];
  dst_vec[3] = a_vec[3];
return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_ceil_ss
#define _mm_ceil_ss _mm_ceil_ss_dbg

/*
 Compare packed 64-bit integers in "a" and "b" for greater-than, and store the results in "dst".
*/
static inline __m128i _mm_cmpgt_epi64_dbg(__m128i a, __m128i b)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t b_vec[2];
  _mm_storeu_si128((__m128i*)b_vec, b);
  int64_t dst_vec[2];
  for (int j = 0; j <= 1; j++) {
    dst_vec[j] = ( a_vec[j] > b_vec[j] ) ? 0xFFFFFFFFFFFFFFFFULL : 0;
  }
return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_cmpgt_epi64
#define _mm_cmpgt_epi64 _mm_cmpgt_epi64_dbg

/*
 Shuffle 8-bit integers in "a" within 128-bit lanes according to shuffle control mask in the corresponding 8-bit element of "b", and store the results in "dst".
*/
static inline __m256i _mm256_shuffle_epi8_dbg(__m256i a, __m256i b)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t b_vec[32];
  _mm256_storeu_si256((__m256i*)b_vec, b);
  int8_t dst_vec[32];
  int index;
  for (int j = 0; j <= 15; j++) {
    if ((b_vec[j] & 0x80) >> 7) {
      dst_vec[j] = 0;
    } else {
      index = b_vec[j] & 0xf;
      dst_vec[j] = a_vec[index];
    }
    if ((b_vec[j + 16] & 0x80) >> 7) {
      dst_vec[j + 16] = 0;
    } else {
      index = b_vec[j + 16] & 0xf;
      dst_vec[j + 16] = a_vec[index + 16];
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_shuffle_epi8
#define _mm256_shuffle_epi8 _mm256_shuffle_epi8_dbg

/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
*/
static inline __m512 _mm512_add_ps_dbg(__m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_add_ps
#define _mm512_add_ps _mm512_add_ps_dbg

/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_add_round_ps_dbg(__m512 a, __m512 b, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = a_vec[j] + b_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_add_round_ps
#define _mm512_add_round_ps _mm512_add_round_ps_dbg


/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_mask_add_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_add_ps
#define _mm512_mask_add_ps _mm512_mask_add_ps_dbg

/*
 Add packed single-precision (32-bit) floating-point elements in "a" and "b", and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_mask_add_round_ps_dbg(__m512 src, __mmask16 k, __m512 a, __m512 b, int rounding)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j] + b_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_add_round_ps
#define _mm512_mask_add_round_ps _mm512_mask_add_round_ps_dbg

/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst".

*/
static inline __m512 _mm512_fmadd_ps_dbg(__m512 a, __m512 b, __m512 c)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_fmadd_ps
#define _mm512_fmadd_ps _mm512_fmadd_ps_dbg


/*
 Multiply packed single-precision (32-bit) floating-point elements in "a" and "b", add the intermediate result to packed elements in "c", and store the results in "dst".
	(_MM_FROUND_TO_NEAREST_INT |_MM_FROUND_NO_EXC) // round to nearest, and suppress exceptions
        (_MM_FROUND_TO_NEG_INF |_MM_FROUND_NO_EXC)     // round down, and suppress exceptions
        (_MM_FROUND_TO_POS_INF |_MM_FROUND_NO_EXC)     // round up, and suppress exceptions
        (_MM_FROUND_TO_ZERO |_MM_FROUND_NO_EXC)        // truncate, and suppress exceptions
        _MM_FROUND_CUR_DIRECTION // use MXCSR.RC; see _MM_SET_ROUNDING_MODE
*/
static inline __m512 _mm512_fmadd_round_ps_dbg(__m512 a, __m512 b, __m512 c, int rounding)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  float c_vec[16];
  _mm512_storeu_ps((void*)c_vec, c);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    dst_vec[j] = (a_vec[j] * b_vec[j]) + c_vec[j];
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_fmadd_round_ps
#define _mm512_fmadd_round_ps _mm512_fmadd_round_ps_dbg

/*
 Reduce the packed 32-bit integers in "a" by maximum. Returns the maximum of all elements in "a".
*/
static inline int _mm512_reduce_max_epi32_dbg(__m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  long int max = LONG_MIN;
  for (int j = 0; j <= 15; j++) {
    max = MAX(max, a_vec[j]);
  }
return max;
}

#undef _mm512_reduce_max_epi32
#define _mm512_reduce_max_epi32 _mm512_reduce_max_epi32_dbg

/*
 Reduce the packed single-precision (32-bit) floating-point elements in "a" by maximum. Returns the maximum of all elements in "a".
*/
static inline float _mm512_reduce_max_ps_dbg(__m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float max = FLT_MIN;
  for (int j = 0; j <= 15; j++) {
    max = MAX(max, a_vec[j]);
  }
return max;
}

#undef _mm512_reduce_max_ps
#define _mm512_reduce_max_ps _mm512_reduce_max_ps_dbg

/*
 Shift packed 16-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in "dst".
*/
static inline __m256i _mm256_srai_epi16_dbg(__m256i a, int imm8)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if ((imm8 & 0xff) > 15) {
      dst_vec[j] = (a_vec[j] < 0) ? 0xFFFF : 0;
    } else {
      dst_vec[j] = (a_vec[j] >> (imm8 & 0xff)) | ((a_vec[j] < 0) ? (0xFFFF << (16 - (imm8 & 0xff))) : 0);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_srai_epi16
#define _mm256_srai_epi16 _mm256_srai_epi16_dbg

/*
 Shift packed 32-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in "dst".
*/
static inline __m256i _mm256_srai_epi32_dbg(__m256i a, int imm8)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if ((imm8 & 0xff) > 31) {
      dst_vec[j] = (a_vec[j] < 0) ? 0xFFFFFFFFL : 0;
    } else {
      dst_vec[j] = (a_vec[j] >> (imm8 & 0xff)) | ((a_vec[j] < 0) ? (0xFFFFFFFFL << (32 - (imm8 & 0xff))) : 0);
    }
  }
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_srai_epi32
#define _mm256_srai_epi32 _mm256_srai_epi32_dbg

static inline __m512i _mm512_sra_epi16_dbg(__m512i a, __m128i count)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t count_vec[2];
  _mm_storeu_si128((__m128i*)count_vec, count);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if (count_vec[0] > 15) {
      dst_vec[j] = (a_vec[j] < 0) ? 0xFFFF : 0;
    } else {
      dst_vec[j] = (a_vec[j] >> count_vec[0]) | ((a_vec[j] < 0) ? (0xFFFF << (16 - count_vec[0])) : 0);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_sra_epi16
#define _mm512_sra_epi16 _mm512_sra_epi16_dbg

static inline __m512i _mm512_srai_epi16_dbg(__m512i a, int imm8)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  for (int j = 0; j <= 31; j++) {
    if ((imm8 & 0xff) > 15) {
      dst_vec[j] = (a_vec[j] < 0) ? 0xFFFF : 0;
    } else {
      dst_vec[j] = (a_vec[j] >> (imm8 & 0xff)) | ((a_vec[j] < 0) ? (0xFFFF << (16 - (imm8 & 0xff))) : 0);
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_srai_epi16
#define _mm512_srai_epi16 _mm512_srai_epi16_dbg

/*
 Shift packed 32-bit integers in "a" right by "imm8" while shifting in sign bits, and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_srai_epi32_dbg (__mmask16 k, __m512i a, unsigned int imm8)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      if ((imm8 & 0xff) > 31) {
        dst_vec[j] = (a_vec[j] < 0) ? 0xFFFFFFFFL : 0;
      } else {
        dst_vec[j] = (a_vec[j] >> (imm8 & 0xff)) | ((a_vec[j] < 0) ? (0xFFFFFFFFL << (32 - (imm8 & 0xff))) : 0);
      }
    } else {
        dst_vec[j] = 0;
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_srai_epi32
#define _mm512_maskz_srai_epi32 _mm512_maskz_srai_epi32_dbg

/*
 Contiguously store the active 16-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m512i _mm512_maskz_compress_epi16_dbg(__mmask32 k, __m512i a)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 31; j++)
    dst_vec[j] = 0;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_compress_epi16
#define _mm512_maskz_compress_epi16 _mm512_maskz_compress_epi16_dbg


/*
 Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m512i _mm512_mask_compress_epi16_dbg(__m512i src, __mmask32 k, __m512i a)
{
  int16_t src_vec[32];
  _mm512_storeu_si512((void*)src_vec, src);
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 31; j++)
    dst_vec[j] = src_vec[j];
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_compress_epi16
#define _mm512_mask_compress_epi16 _mm512_mask_compress_epi16_dbg


/*
 Contiguously store the active 16-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m256i _mm256_maskz_compress_epi16_dbg(__mmask16 k, __m256i a)
{
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = 0;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_compress_epi16
#define _mm256_maskz_compress_epi16 _mm256_maskz_compress_epi16_dbg


/*
 Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m256i _mm256_mask_compress_epi16_dbg(__m256i src, __mmask16 k, __m256i a)
{
  int16_t src_vec[16];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int16_t a_vec[16];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int16_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = src_vec[j];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_compress_epi16
#define _mm256_mask_compress_epi16 _mm256_mask_compress_epi16_dbg


/*
 Contiguously store the active 16-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m128i _mm_maskz_compress_epi16_dbg(__mmask8 k, __m128i a)
{
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_compress_epi16
#define _mm_maskz_compress_epi16 _mm_maskz_compress_epi16_dbg


/*
 Contiguously store the active 16-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m128i _mm_mask_compress_epi16_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int16_t src_vec[8];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int16_t a_vec[8];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int16_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = src_vec[j];
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_compress_epi16
#define _mm_mask_compress_epi16 _mm_mask_compress_epi16_dbg


/*
 Contiguously store the active 8-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m512i _mm512_maskz_compress_epi8_dbg(__mmask64 k, __m512i a)
{
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  int m = 0;
  for (int j = 0; j <= 63; j++) {
    if (k & ((1 << j) & 0xffffffffffffffffUL)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 63; j++)
    dst_vec[j] = 0;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_compress_epi8
#define _mm512_maskz_compress_epi8 _mm512_maskz_compress_epi8_dbg


/*
 Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m512i _mm512_mask_compress_epi8_dbg(__m512i src, __mmask64 k, __m512i a)
{
  int8_t src_vec[64];
  _mm512_storeu_si512((void*)src_vec, src);
  int8_t a_vec[64];
  _mm512_storeu_si512((void*)a_vec, a);
  int8_t dst_vec[64];
  int m = 0;
  for (int j = 0; j <= 63; j++) {
    if (k & ((1 << j) & 0xffffffffffffffffUL)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 63; j++)
    dst_vec[j] = src_vec[j];
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_compress_epi8
#define _mm512_mask_compress_epi8 _mm512_mask_compress_epi8_dbg

/*
 Contiguously store the active 8-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m256i _mm256_maskz_compress_epi8_dbg(__mmask32 k, __m256i a)
{
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 31; j++)
    dst_vec[j] = 0;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_compress_epi8
#define _mm256_maskz_compress_epi8 _mm256_maskz_compress_epi8_dbg

/*
 Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m256i _mm256_mask_compress_epi8_dbg(__m256i src, __mmask32 k, __m256i a)
{
  int8_t src_vec[32];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int8_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int8_t dst_vec[32];
  int m = 0;
  for (int j = 0; j <= 31; j++) {
    if (k & ((1 << j) & 0xffffffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 31; j++)
    dst_vec[j] = src_vec[j];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_compress_epi8
#define _mm256_mask_compress_epi8 _mm256_mask_compress_epi8_dbg


/*
 Contiguously store the active 8-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m128i _mm_maskz_compress_epi8_dbg(__mmask16 k, __m128i a)
{
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_compress_epi8
#define _mm_maskz_compress_epi8 _mm_maskz_compress_epi8_dbg


/*
 Contiguously store the active 8-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m128i _mm_mask_compress_epi8_dbg(__m128i src, __mmask16 k, __m128i a)
{
  int8_t src_vec[16];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int8_t a_vec[16];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int8_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = src_vec[j];
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_compress_epi8
#define _mm_mask_compress_epi8 _mm_mask_compress_epi8_dbg


/*
 Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m512d _mm512_mask_compress_pd_dbg(__m512d src, __mmask8 k, __m512d a)
{
  double src_vec[7];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[7];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[7];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = src_vec[j];
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_compress_pd
#define _mm512_mask_compress_pd _mm512_mask_compress_pd_dbg


/*
 Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m512d _mm512_maskz_compress_pd_dbg(__mmask8 k, __m512d a)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = 0;
  return _mm512_loadu_pd((void*)dst_vec);;
}

#undef _mm512_maskz_compress_pd
#define _mm512_maskz_compress_pd _mm512_maskz_compress_pd_dbg


/*
 Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m512 _mm512_mask_compress_ps_dbg(__m512 src, __mmask16 k, __m512 a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = src_vec[j];
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_compress_ps
#define _mm512_mask_compress_ps _mm512_mask_compress_ps_dbg


/*
 Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m512 _mm512_maskz_compress_ps_dbg(__mmask16 k, __m512 a)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = 0;
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_maskz_compress_ps
#define _mm512_maskz_compress_ps _mm512_maskz_compress_ps_dbg


/*
 Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m512i _mm512_mask_compress_epi32_dbg(__m512i src, __mmask16 k, __m512i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = src_vec[j];
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_compress_epi32
#define _mm512_mask_compress_epi32 _mm512_mask_compress_epi32_dbg


/*
 Contiguously store the active 32-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m512i _mm512_maskz_compress_epi32_dbg(__mmask16 k, __m512i a)
{
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  int m = 0;
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 15; j++)
    dst_vec[j] = 0;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_compress_epi32
#define _mm512_maskz_compress_epi32 _mm512_maskz_compress_epi32_dbg


/*
 Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m512i _mm512_mask_compress_epi64_dbg(__m512i src, __mmask8 k, __m512i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = src_vec[j];
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_compress_epi64
#define _mm512_mask_compress_epi64 _mm512_mask_compress_epi64_dbg


/*
 Contiguously store the active 64-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m512i _mm512_maskz_compress_epi64_dbg(__mmask8 k, __m512i a)
{
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = 0;
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_maskz_compress_epi64
#define _mm512_maskz_compress_epi64 _mm512_maskz_compress_epi64_dbg


/*
 Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m256d _mm256_mask_compress_pd_dbg(__m256d src, __mmask8 k, __m256d a)
{
  double src_vec[4];
  _mm256_storeu_pd((double*)src_vec, src);
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = src_vec[j];
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_mask_compress_pd
#define _mm256_mask_compress_pd _mm256_mask_compress_pd_dbg


/*
 Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m256d _mm256_maskz_compress_pd_dbg(__mmask8 k, __m256d a)
{
  double a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  double dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = 0;
  return _mm256_loadu_pd((void*)dst_vec);
}

#undef _mm256_maskz_compress_pd
#define _mm256_maskz_compress_pd _mm256_maskz_compress_pd_dbg


/*
 Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m128d _mm_mask_compress_pd_dbg(__m128d src, __mmask8 k, __m128d a)
{
  double src_vec[2];
  _mm_storeu_pd((double*)src_vec, src);
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 1; j++)
    dst_vec[j] = src_vec[j];
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_mask_compress_pd
#define _mm_mask_compress_pd _mm_mask_compress_pd_dbg


/*
 Contiguously store the active double-precision (64-bit) floating-point elements in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m128d _mm_maskz_compress_pd_dbg(__mmask8 k, __m128d a)
{
  double a_vec[2];
  _mm_storeu_pd((double*)a_vec, a);
  double dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 1; j++)
    dst_vec[j] = 0;
  return _mm_loadu_pd((double*)dst_vec);
}

#undef _mm_maskz_compress_pd
#define _mm_maskz_compress_pd _mm_maskz_compress_pd_dbg


/*
 Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m256 _mm256_mask_compress_ps_dbg(__m256 src, __mmask8 k, __m256 a)
{
  float src_vec[8];
  _mm256_storeu_ps((float*)src_vec, src);
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = src_vec[j];
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_mask_compress_ps
#define _mm256_mask_compress_ps _mm256_mask_compress_ps_dbg


/*
 Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m256 _mm256_maskz_compress_ps_dbg(__mmask8 k, __m256 a)
{
  float a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  float dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = 0;
  return _mm256_loadu_ps((void*)dst_vec);
}

#undef _mm256_maskz_compress_ps
#define _mm256_maskz_compress_ps _mm256_maskz_compress_ps_dbg


/*
 Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m128 _mm_mask_compress_ps_dbg(__m128 src, __mmask8 k, __m128 a)
{
  float src_vec[4];
  _mm_storeu_ps((float*)src_vec, src);
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = src_vec[j];
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_mask_compress_ps
#define _mm_mask_compress_ps _mm_mask_compress_ps_dbg


/*
 Contiguously store the active single-precision (32-bit) floating-point elements in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m128 _mm_maskz_compress_ps_dbg(__mmask8 k, __m128 a)
{
  float a_vec[4];
  _mm_storeu_ps((float*)a_vec, a);
  float dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = 0;
  return _mm_loadu_ps((float*)dst_vec);
}

#undef _mm_maskz_compress_ps
#define _mm_maskz_compress_ps _mm_maskz_compress_ps_dbg


/*
 Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m256i _mm256_mask_compress_epi32_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int32_t src_vec[8];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = src_vec[j];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_compress_epi32
#define _mm256_mask_compress_epi32 _mm256_mask_compress_epi32_dbg


/*
 Contiguously store the active 32-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m256i _mm256_maskz_compress_epi32_dbg(__mmask8 k, __m256i a)
{
  int32_t a_vec[8];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int32_t dst_vec[8];
  int m = 0;
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 7; j++)
    dst_vec[j] = 0;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_compress_epi32
#define _mm256_maskz_compress_epi32 _mm256_maskz_compress_epi32_dbg


/*
 Contiguously store the active 32-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m128i _mm_mask_compress_epi32_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int32_t src_vec[4];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = src_vec[j];
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_compress_epi32
#define _mm_mask_compress_epi32 _mm_mask_compress_epi32_dbg


/*
 Contiguously store the active 32-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m128i _mm_maskz_compress_epi32_dbg(__mmask8 k, __m128i a)
{
  int32_t a_vec[4];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int32_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_compress_epi32
#define _mm_maskz_compress_epi32 _mm_maskz_compress_epi32_dbg


/*
 Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m256i _mm256_mask_compress_epi64_dbg(__m256i src, __mmask8 k, __m256i a)
{
  int64_t src_vec[4];
  _mm256_storeu_si256((__m256i*)src_vec, src);
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = src_vec[j];
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_mask_compress_epi64
#define _mm256_mask_compress_epi64 _mm256_mask_compress_epi64_dbg


/*
 Contiguously store the active 64-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m256i _mm256_maskz_compress_epi64_dbg(__mmask8 k, __m256i a)
{
  int64_t a_vec[4];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int64_t dst_vec[4];
  int m = 0;
  for (int j = 0; j <= 3; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 3; j++)
    dst_vec[j] = 0;
  return _mm256_loadu_si256((void*)dst_vec);
}

#undef _mm256_maskz_compress_epi64
#define _mm256_maskz_compress_epi64 _mm256_maskz_compress_epi64_dbg


/*
 Contiguously store the active 64-bit integers in "a" (those with their respective bit set in writemask "k") to "dst", and pass through the remaining elements from "src".
*/
static inline __m128i _mm_mask_compress_epi64_dbg(__m128i src, __mmask8 k, __m128i a)
{
  int64_t src_vec[2];
  _mm_storeu_si128((__m128i*)src_vec, src);
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 1; j++)
    dst_vec[j] = src_vec[j];
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_mask_compress_epi64
#define _mm_mask_compress_epi64 _mm_mask_compress_epi64_dbg


/*
 Contiguously store the active 64-bit integers in "a" (those with their respective bit set in zeromask "k") to "dst", and set the remaining elements to zero.
*/
static inline __m128i _mm_maskz_compress_epi64_dbg(__mmask8 k, __m128i a)
{
  int64_t a_vec[2];
  _mm_storeu_si128((__m128i*)a_vec, a);
  int64_t dst_vec[2];
  int m = 0;
  for (int j = 0; j <= 1; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[m] = a_vec[j];
      m = m + 1;
    }
  }
  for (int j = m; j <= 1; j++)
    dst_vec[j] = 0;
  return _mm_loadu_si128((__m128i*)dst_vec);
}

#undef _mm_maskz_compress_epi64
#define _mm_maskz_compress_epi64 _mm_maskz_compress_epi64_dbg

static inline __m256 _mm256_castpd_ps_dbg(__m256d a)
{
  return (__m256)a;
}
#undef _mm256_castpd_ps
#define _mm256_castpd_ps _mm256_castpd_ps_dbg
static inline __m256d _mm256_castps_pd_dbg(__m256 a)
{
  return (__m256d)a;
}
#undef _mm256_castps_pd
#define _mm256_castps_pd _mm256_castps_pd_dbg
static inline __m256i _mm256_castps_si256_dbg(__m256 a)
{
  return (__m256i)a;
}
#undef _mm256_castps_si256
#define _mm256_castps_si256 _mm256_castps_si256_dbg
static inline __m256i _mm256_castpd_si256_dbg(__m256d a)
{
  return (__m256i)a;
}
#undef _mm256_castpd_si256
#define _mm256_castpd_si256 _mm256_castpd_si256_dbg
static inline __m256 _mm256_castsi256_ps_dbg(__m256i a)
{
  return (__m256)a;
}
#undef _mm256_castsi256_ps
#define _mm256_castsi256_ps _mm256_castsi256_ps_dbg
static inline __m256d _mm256_castsi256_pd_dbg(__m256i a)
{
  return (__m256d)a;
}
#undef _mm256_castsi256_pd
#define _mm256_castsi256_pd _mm256_castsi256_pd_dbg
static inline __m128 _mm256_castps256_ps128_dbg(__m256 a)
{
  __m128 b[2];
  _mm256_storeu_ps((float*)b, a);
  return b[0];
}
#undef _mm256_castps256_ps128
#define _mm256_castps256_ps128 _mm256_castps256_ps128_dbg
static inline __m128d _mm256_castpd256_pd128_dbg(__m256d a)
{
  __m128d b[2];
  _mm256_storeu_pd((double*)b, a);
  return b[0];
}
#undef _mm256_castpd256_pd128
#define _mm256_castpd256_pd128 _mm256_castpd256_pd128_dbg
static inline __m128i _mm256_castsi256_si128_dbg(__m256i a)
{
  __m128i b[2];
  _mm256_storeu_si256((__m256i*)b, a);
  return b[0];
}
#undef _mm256_castsi256_si128
#define _mm256_castsi256_si128 _mm256_castsi256_si128_dbg
static inline __m256 _mm256_castps128_ps256_dbg(__m128 a)
{
  __m128 b[2];
  b[0] = a;
  return _mm256_loadu_ps((float*)b);
}
#undef _mm256_castps128_ps256
#define _mm256_castps128_ps256 _mm256_castps128_ps256_dbg
static inline __m256d _mm256_castpd128_pd256_dbg(__m128d a)
{
  __m128d b[2];
  b[0] = a;
  return _mm256_loadu_pd((double*)b);
}
#undef _mm256_castpd128_pd256
#define _mm256_castpd128_pd256 _mm256_castpd128_pd256_dbg
static inline __m256i _mm256_castsi128_si256_dbg(__m128i a)
{
  __m128i b[2];
  b[0] = a;
  return _mm256_loadu_si256((__m256i*)b);
}
#undef _mm256_castsi128_si256
#define _mm256_castsi128_si256 _mm256_castsi128_si256_dbg
static inline __m512d _mm512_castpd256_pd512_dbg(__m256d a)
{
  __m256d b[2];
  b[0] = a;
  return _mm512_loadu_pd((double*)b);
}
#undef _mm512_castpd256_pd512
#define _mm512_castpd256_pd512 _mm512_castpd256_pd512_dbg
static inline __m128d _mm512_castpd512_pd128_dbg(__m512d a)
{
  __m128d b[4];
  _mm512_storeu_pd((__m512d*)b, a);
  return b[0];
}
#undef _mm512_castpd512_pd128
#define _mm512_castpd512_pd128 _mm512_castpd512_pd128_dbg
static inline __m128 _mm512_castps512_ps128_dbg(__m512 a)
{
  __m128 b[4];
  _mm512_storeu_ps((__m512*)b, a);
  return b[0];
}
#undef _mm512_castps512_ps128
#define _mm512_castps512_ps128 _mm512_castps512_ps128_dbg
static inline __m256d _mm512_castpd512_pd256_dbg(__m512d a)
{
  __m256d b[2];
  _mm512_storeu_pd((__m512d*)b, a);
  return b[0];
}
#undef _mm512_castpd512_pd256
#define _mm512_castpd512_pd256 _mm512_castpd512_pd256_dbg
static inline __m512 _mm512_castpd_ps_dbg(__m512d a)
{
  return (__m512)a;
}
#undef _mm512_castpd_ps
#define _mm512_castpd_ps _mm512_castpd_ps_dbg
static inline __m512i _mm512_castpd_si512_dbg(__m512d a)
{
  return (__m512i)a;
}
#undef _mm512_castpd_si512
#define _mm512_castpd_si512 _mm512_castpd_si512_dbg
static inline __m512 _mm512_castps128_ps512_dbg(__m128 a)
{
  __m128 b[4];
  b[0] = a;
  return _mm512_loadu_ps(b);
}
#undef _mm512_castps128_ps512
#define _mm512_castps128_ps512 _mm512_castps128_ps512_dbg
static inline __m512 _mm512_castps256_ps512_dbg(__m256 a)
{
  __m256 b[2];
  b[0] = a;
  return _mm512_loadu_ps(b);
}
#undef _mm512_castps256_ps512
#define _mm512_castps256_ps512 _mm512_castps256_ps512_dbg
static inline __m256 _mm512_castps512_ps256_dbg(__m512 a)
{
  __m256 b[2];
  _mm512_storeu_ps((__m512*)b, a);
  return b[0];
}
#undef _mm512_castps512_ps256
#define _mm512_castps512_ps256 _mm512_castps512_ps256_dbg
static inline __m512d _mm512_castps_pd_dbg(__m512 a)
{
  return (__m512d)a;
}
#undef _mm512_castps_pd
#define _mm512_castps_pd _mm512_castps_pd_dbg
static inline __m512i _mm512_castps_si512_dbg(__m512 a)
{
  return (__m512i)a;
}
#undef _mm512_castps_si512
#define _mm512_castps_si512 _mm512_castps_si512_dbg
static inline __m512i _mm512_castsi128_si512_dbg(__m128i a)
{
  __m128i b[4];
  b[0] = a;
  return _mm512_loadu_si512(b);
}
#undef _mm512_castsi128_si512
#define _mm512_castsi128_si512 _mm512_castsi128_si512_dbg
static inline __m512i _mm512_castsi256_si512_dbg(__m256i a)
{
  __m256i b[2];
  b[0] = a;
  return _mm512_loadu_si512(b);
}
#undef _mm512_castsi256_si512
#define _mm512_castsi256_si512 _mm512_castsi256_si512_dbg
static inline __m512d _mm512_castsi512_pd_dbg(__m512i a)
{
  return (__m512d)a;
}
#undef _mm512_castsi512_pd
#define _mm512_castsi512_pd _mm512_castsi512_pd_dbg
static inline __m512 _mm512_castsi512_ps_dbg(__m512i a)
{
  return (__m512)a;
}
#undef _mm512_castsi512_ps
#define _mm512_castsi512_ps _mm512_castsi512_ps_dbg
static inline __m128i _mm512_castsi512_si128_dbg(__m512i a)
{
  __m128i b[4];
  _mm512_storeu_si512((__m512i*)b, a);
  return b[0];
}
#undef _mm512_castsi512_si128
#define _mm512_castsi512_si128 _mm512_castsi512_si128_dbg
static inline __m256i _mm512_castsi512_si256_dbg(__m512i a)
{
  __m256i b[2];
  _mm512_storeu_si512((__m512i*)b, a);
  return b[0];
}
#undef _mm512_castsi512_si256
#define _mm512_castsi512_si256 _mm512_castsi512_si256_dbg

/*
 Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit set in mask "k"), and store the results in "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_maskz_expandloadu_epi32_dbg(__mmask16 k, void const* mem_addr)
{
  return _mm512_maskz_expand_epi32(k, _mm512_loadu_si512(mem_addr));
}

#undef _mm512_maskz_expandloadu_epi32
#define _mm512_maskz_expandloadu_epi32 _mm512_maskz_expandloadu_epi32_dbg

/*
 Load contiguous active 32-bit integers from unaligned memory at "mem_addr" (those with their respective bit set in mask "k"), and store the results in "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_expandloadu_epi32_dbg(__m512i src, __mmask16 k, void const* mem_addr)
{
  return _mm512_mask_expand_epi32(src, k, _mm512_loadu_si512(mem_addr));
}

#undef _mm512_mask_expandloadu_epi32
#define _mm512_mask_expandloadu_epi32 _mm512_mask_expandloadu_epi32_dbg

/*
 Move packed double-precision (64-bit) floating-point elements from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512d _mm512_mask_mov_pd_dbg(__m512d src, __mmask8 k, __m512d a)
{
  double src_vec[8];
  _mm512_storeu_pd((void*)src_vec, src);
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_pd((void*)dst_vec);
}

#undef _mm512_mask_mov_pd
#define _mm512_mask_mov_pd _mm512_mask_mov_pd_dbg

/*
 Move packed single-precision (32-bit) floating-point elements from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512 _mm512_mask_mov_ps_dbg(__m512 src, __mmask16 k, __m512 a)
{
  float src_vec[16];
  _mm512_storeu_ps((void*)src_vec, src);
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_ps((void*)dst_vec);
}

#undef _mm512_mask_mov_ps
#define _mm512_mask_mov_ps _mm512_mask_mov_ps_dbg

/*
 Move packed 32-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_mov_epi32_dbg(__m512i src, __mmask16 k, __m512i a)
{
  int32_t src_vec[16];
  _mm512_storeu_si512((void*)src_vec, src);
  int32_t a_vec[16];
  _mm512_storeu_si512((void*)a_vec, a);
  int32_t dst_vec[16];
  for (int j = 0; j <= 15; j++) {
    if (k & ((1 << j) & 0xffff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mov_epi32
#define _mm512_mask_mov_epi32 _mm512_mask_mov_epi32_dbg

/*
 Move packed 64-bit integers from "a" to "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set).
*/
static inline __m512i _mm512_mask_mov_epi64_dbg(__m512i src, __mmask8 k, __m512i a)
{
  int64_t src_vec[8];
  _mm512_storeu_si512((void*)src_vec, src);
  int64_t a_vec[8];
  _mm512_storeu_si512((void*)a_vec, a);
  int64_t dst_vec[8];
  for (int j = 0; j <= 7; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = src_vec[j];
    }
  }
  return _mm512_loadu_si512((void*)dst_vec);
}

#undef _mm512_mask_mov_epi64
#define _mm512_mask_mov_epi64 _mm512_mask_mov_epi64_dbg

/*
Create mask from the most significant bit of each 8-bit element in "a", and store the result in "dst".
*/
static inline int _mm256_movemask_epi8_dbg(__m256i a)
{
  uint32_t a_vec[32];
  _mm256_storeu_si256((__m256i*)a_vec, a);
  int dst;
  for (int j = 0; j <= 31; j++) {
    dst |= (a_vec[j] & 0x80) ? (1 << j) : 0;
  }
return dst;
}

#undef _mm256_movemask_epi8
#define _mm256_movemask_epi8 _mm256_movemask_epi8_dbg

/*
 Set each bit of mask "dst" based on the most significant bit of the corresponding packed double-precision (64-bit) floating-point element in "a".
*/
static inline int _mm256_movemask_pd_dbg(__m256d a)
{
  uint64_t a_vec[4];
  _mm256_storeu_pd((double*)a_vec, a);
  int dst;
  for (int j = 0; j <= 3; j++) {
    dst |= (a_vec[j] & 0x8000000000000000ULL) ? (1 << j) : 0;
  }
  return dst;
}

#undef _mm256_movemask_pd
#define _mm256_movemask_pd _mm256_movemask_pd_dbg

/*
 Set each bit of mask "dst" based on the most significant bit of the corresponding packed single-precision (32-bit) floating-point element in "a".
*/
static inline int _mm256_movemask_ps_dbg(__m256 a)
{
  int32_t a_vec[8];
  _mm256_storeu_ps((float*)a_vec, a);
  int dst;
  for (int j = 0; j <= 7; j++) {
    dst |= (a_vec[j] & 0x80000000ULL) ? (1 << j) : 0;
  }
  return dst;
}

#undef _mm256_movemask_ps
#define _mm256_movemask_ps _mm256_movemask_ps_dbg

/*
 Return vector of type __m512i with undefined elements.
*/
static inline __m512i _mm512_undefined_epi32_dbg()
{
  __m512i undefined; /*copy garbage from stack*/
  return undefined;
}
#define _mm512_undefined_epi32 _mm512_undefined_epi32_dbg
        
/*
 Load packed single-precision (32-bit) floating-point elements from memory into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). "mem_addr" does not need to be aligned on any particular boundary.
*/
static inline __m512 _mm512_maskz_loadu_ps_dbg (__mmask16 k, void const* mem_addr)
{
  return _mm512_maskz_mov_ps(k, _mm512_loadu_ps(mem_addr));
}
#undef _mm512_maskz_loadu_ps
#define _mm512_maskz_loadu_ps _mm512_maskz_loadu_ps_dbg

/*
 Load packed 16-bit integers from memory into "dst" using zeromask "k" (elements are zeroed out when the corresponding mask bit is not set). "mem_addr" does not need to be aligned on any particular boundary..
*/
static inline __m128i _mm_maskz_loadu_epi16_dbg (__mmask8 k, void const* mem_addr)
{
  int16_t *a_vec = (int16_t*)mem_addr;
  int16_t dst_vec[8];
  for (int j = 0; j < 8; j++) {
    if (k & ((1 << j) & 0xff)) {
      dst_vec[j] = a_vec[j];
    } else {
      dst_vec[j] = 0;
    }
  }
  return _mm_loadu_si128((__m128i*)dst_vec);
}
#undef _mm_maskz_loadu_epi16
#define _mm_maskz_loadu_epi16 _mm_maskz_loadu_epi16_dbg

/*
 Load packed 16-bit integers from memory into "dst" using writemask "k" (elements are copied from "src" when the corresponding mask bit is not set). "mem_addr" does not need to be aligned on any particular boundary.
*/
static inline __m256i _mm256_mask_loadu_epi16_dbg (__m256i src, __mmask16 k, void const* mem_addr)
{
  return _mm256_mask_mov_epi16(src, k, _mm256_loadu_epi16(mem_addr));
}
#undef _mm256_mask_loadu_epi16
#define _mm256_mask_loadu_epi16 _mm256_mask_loadu_epi16_dbg

/*
 Unpack and interleave 16-bit integers from the high half of each 128-bit lane in "a" and "b", and store the results in "dst".
*/
static inline __m512i _mm512_unpackhi_epi16_dbg(__m512i a, __m512i b)
{
  int16_t a_vec[32];
  _mm512_storeu_si512((void*)a_vec, a);
  int16_t b_vec[32];
  _mm512_storeu_si512((void*)b_vec, b);
  int16_t dst_vec[32];

  int counter = 4;
  for (int j = 0; j <= 30 ; j += 2) {
    dst_vec[j] = a_vec[counter];
    dst_vec[j + 1] = b_vec[counter];
    counter++;
    if (counter % 8 == 0) counter += 4;
  }

  return _mm512_loadu_epi16((void*)dst_vec);
}

#undef _mm512_unpackhi_epi16
#define _mm512_unpackhi_epi16 _mm512_unpackhi_epi16_dbg

/*
 Compare packed single-precision (32-bit) floating-point elements in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask16 _mm512_cmpeq_ps_mask_dbg(__m512 a, __m512 b)
{
  float a_vec[16];
  _mm512_storeu_ps((void*)a_vec, a);
  float b_vec[16];
  _mm512_storeu_ps((void*)b_vec, b);
  __mmask16 k = 0;
  for (int j = 0; j <= 15; j++) {
    k |= ((a_vec[j] == b_vec[j]) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_ps_mask
#define _mm512_cmpeq_ps_mask _mm512_cmpeq_ps_mask_dbg

/*
 Compare packed double-precision (64-bit) floating-point elements in "a" and "b" for equality, and store the results in mask vector "k".
*/
static inline __mmask8 _mm512_cmpeq_pd_mask_dbg(__m512d a, __m512d b)
{
  double a_vec[8];
  _mm512_storeu_pd((void*)a_vec, a);
  double b_vec[8];
  _mm512_storeu_pd((void*)b_vec, b);
  __mmask8 k = 0;
  for (int j = 0; j <= 7; j++) {
    k |= ((a_vec[j] == b_vec[j]) ? 1 : 0) << j;
  }
  return k;
}

#undef _mm512_cmpeq_pd_mask
#define _mm512_cmpeq_pd_mask _mm512_cmpeq_pd_mask_dbg

#undef MIN
#undef MAX
#undef NEG
#undef SELECT2
#undef SELECT4

#ifdef __cplusplus
}
#endif
#endif
